{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.87 ms, sys: 0 ns, total: 2.87 ms\n",
      "Wall time: 2.41 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#To hide warnings export PYTHONWARNINGS=\"ignore\"\n",
    "#Imports{\n",
    "\n",
    "import os\n",
    "from os.path import dirname\n",
    "from os.path import join\n",
    "os.environ['KERAS_BACKEND'] = 'theano'\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' #Cha\n",
    "\n",
    "\n",
    "import sys\n",
    "import warnings\n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    \n",
    "import cPickle as pickle\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from scipy.misc import imread\n",
    "from scipy.misc import imresize\n",
    "from scipy.io import loadmat\n",
    "from scipy.interpolate import spline\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout, Flatten, MaxPooling2D, Input, ZeroPadding2D,merge,Lambda\n",
    "from keras.layers.convolutional import Convolution2D\n",
    "from keras.models import Model\n",
    "from keras.utils.layer_utils import convert_all_kernels_in_model\n",
    "from keras.optimizers import SGD\n",
    "from keras import backend as K\n",
    "from keras.engine import Layer\n",
    "from keras.layers.core import Lambda\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.layers.core import  Lambda\n",
    "from keras.regularizers import l2\n",
    "\n",
    "#}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code snippet needed to read activation values from each layer of the pre-trained artificial neural networks\n",
    "def get_activations(model, layer, X_batch):\n",
    "    #keras.backend.function(inputs, outputs, updates=None)\n",
    "    get_activations = K.function([model.layers[0].input, K.learning_phase()], [model.layers[layer].output,])\n",
    "    #The learning phase flag is a bool tensor (0 = test, 1 = train)\n",
    "    activations = get_activations([X_batch,0])\n",
    "    return activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to pre-process the input image to ensure uniform size and color\n",
    "def preprocess_image_batch(image_paths, img_size=None, crop_size=None, color_mode='rgb', out=None):\n",
    "    \"\"\"\n",
    "    Consistent preprocessing of images batches\n",
    "    :param image_paths: iterable: images to process\n",
    "    :param crop_size: tuple: crop images if specified\n",
    "    :param img_size: tuple: resize images if specified\n",
    "    :param color_mode: Use rgb or change to bgr mode based on type of model you want to use\n",
    "    :param out: append output to this iterable if specified\n",
    "    \"\"\"\n",
    "    img_list = []\n",
    "\n",
    "    for im_path in image_paths:\n",
    "        img = imread(im_path, mode='RGB')\n",
    "        #print im_path\n",
    "        #print img.shape\n",
    "        if img_size:\n",
    "            img = imresize(img, img_size)\n",
    "\n",
    "        img = img.astype('float32')\n",
    "        # We normalize the colors (in RGB space) with the empirical means on the training set\n",
    "        img[:, :, 0] -= 123.68\n",
    "        img[:, :, 1] -= 116.779\n",
    "        img[:, :, 2] -= 103.939\n",
    "        # We permute the colors to get them in the BGR order\n",
    "        if color_mode == 'bgr':\n",
    "            img[:, :, [0, 1, 2]] = img[:, :, [2, 1, 0]]\n",
    "        img = img.transpose((2, 0, 1))\n",
    "\n",
    "        if crop_size:\n",
    "            img = img[:, (img_size[0] - crop_size[0]) // 2:(img_size[0] + crop_size[0]) // 2\n",
    "            , (img_size[1] - crop_size[1]) // 2:(img_size[1] + crop_size[1]) // 2]\n",
    "\n",
    "        img_list.append(img)\n",
    "\n",
    "    try:\n",
    "        img_batch = np.stack(img_list, axis=0)\n",
    "    except:\n",
    "        print im_path\n",
    "        raise ValueError('when img_size and crop_size are None, images'\n",
    "                ' in image_paths must have the same shapes.')\n",
    "\n",
    "    if out is not None and hasattr(out, 'append'):\n",
    "        out.append(img_batch)\n",
    "    else:\n",
    "        return img_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Helper function to normalization across channels\n",
    "K.set_image_dim_ordering('th')\n",
    "def crosschannelnormalization(alpha=1e-4, k=2, beta=0.75, n=5, **kwargs):\n",
    "    \"\"\"\n",
    "    This is the function used for cross channel normalization in the original\n",
    "    Alexnet\n",
    "    \"\"\"\n",
    "    def f(X):\n",
    "        if K.image_dim_ordering()=='tf':\n",
    "            b, r, c, ch = X.get_shape()\n",
    "        else:\n",
    "            b, ch, r, c = X.shape\n",
    "\n",
    "        half = n // 2\n",
    "        square = K.square(X)\n",
    "        scale = k\n",
    "        if K.image_dim_ordering() == 'th':\n",
    "            extra_channels = K.spatial_2d_padding(K.permute_dimensions(square, (0, 2, 3, 1)), ((0,0),(half,half)))\n",
    "            extra_channels = K.permute_dimensions(extra_channels, (0, 3, 1, 2))\n",
    "            for i in range(n):\n",
    "                scale += alpha * extra_channels[:, i:i+ch, :, :]\n",
    "        if K.image_dim_ordering() == 'tf':\n",
    "            extra_channels = K.spatial_2d_padding(K.permute_dimensions(square, (0, 3, 1, 2)), (half, 0))\n",
    "            extra_channels = K.permute_dimensions(extra_channels, (0, 2, 3, 1))\n",
    "            for i in range(n):\n",
    "                scale += alpha * extra_channels[:, :, :, i:i+int(ch)]\n",
    "        scale = scale ** beta\n",
    "        return X / scale\n",
    "\n",
    "\n",
    "    return Lambda(f, output_shape=lambda input_shape: input_shape, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Helper Function to split tensor\n",
    "def splittensor(axis=1, ratio_split=1, id_split=0, **kwargs):\n",
    "    def f(X):\n",
    "        div = K.shape(X)[axis] // ratio_split\n",
    "\n",
    "        if axis == 0:\n",
    "            output = X[id_split*div:(id_split+1)*div, :, :, :]\n",
    "        elif axis == 1:\n",
    "            output = X[:, id_split*div:(id_split+1)*div, :, :]\n",
    "        elif axis == 2:\n",
    "            output = X[:, :, id_split*div:(id_split+1)*div, :]\n",
    "        elif axis == 3:\n",
    "            output = X[:, :, :, id_split*div:(id_split+1)*div]\n",
    "        else:\n",
    "            raise ValueError(\"This axis is not possible\")\n",
    "        return output\n",
    "\n",
    "    def g(input_shape):\n",
    "        output_shape = list(input_shape)\n",
    "        output_shape[axis] = output_shape[axis] // ratio_split\n",
    "        return tuple(output_shape)\n",
    "\n",
    "\n",
    "    return Lambda(f, output_shape=lambda input_shape: g(input_shape), **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Alexnet layer architecture class\n",
    "def AlexNet(img_shape=(3, 227, 227), n_classes=1000, l2_reg=0.,weights_path=None, lambda_mask=None):\n",
    "\n",
    "    dim_ordering = K.image_dim_ordering()\n",
    "    print dim_ordering\n",
    "    if dim_ordering == 'th':\n",
    "        batch_index = 0\n",
    "        channel_index = 1\n",
    "        row_index = 2\n",
    "        col_index = 3\n",
    "    if dim_ordering == 'tf':\n",
    "        batch_index = 0\n",
    "        channel_index = 3\n",
    "        row_index = 1\n",
    "        col_index = 2\n",
    "        \n",
    "    \n",
    "    inputs = Input(img_shape)\n",
    "\n",
    "    conv_1 = Convolution2D(96, 11, 11, subsample=(4, 4), activation='relu',\n",
    "                           name='conv_1', W_regularizer=l2(l2_reg))(inputs)\n",
    "\n",
    "    if lambda_mask is not None:\n",
    "        conv_1_mask  = np.reshape(lambda_mask[0:290400], (96,55,55))\n",
    "    else:\n",
    "        conv_1_mask = np.ones(shape=((96, 55, 55)))\n",
    "    \n",
    "    conv_1_mask  = K.variable(conv_1_mask)\n",
    "    conv_1_lambda = Lambda(lambda x: x * conv_1_mask)(conv_1)\n",
    "\n",
    "    conv_2 = MaxPooling2D((3, 3), strides=(2, 2))(conv_1_lambda)\n",
    "    conv_2 = crosschannelnormalization(name=\"convpool_1\")(conv_2)\n",
    "    conv_2 = ZeroPadding2D((2, 2))(conv_2)\n",
    "    conv_2 = merge([\n",
    "        Convolution2D(128, 5, 5, activation=\"relu\", name='conv_2_'+str(i+1),\n",
    "                      W_regularizer=l2(l2_reg))(\n",
    "            splittensor(axis=channel_index, ratio_split=2, id_split=i)(conv_2)\n",
    "        ) for i in range(2)], mode='concat', concat_axis=channel_index, name=\"conv_2\")\n",
    "\n",
    "    if lambda_mask is not None:\n",
    "        conv_2_mask  = np.reshape(lambda_mask[290400:477024],(256, 27, 27) )\n",
    "    else:\n",
    "        conv_2_mask = np.ones(shape=((256, 27, 27)))\n",
    "        \n",
    "    conv_2_mask = K.variable(conv_2_mask)\n",
    "    conv_2_lambda = Lambda(lambda x: x * conv_2_mask)(conv_2)\n",
    "\n",
    "    conv_3 = MaxPooling2D((3, 3), strides=(2, 2))(conv_2_lambda)\n",
    "    conv_3 = crosschannelnormalization()(conv_3)\n",
    "    conv_3 = ZeroPadding2D((1, 1))(conv_3)\n",
    "    conv_3 = Convolution2D(384, 3, 3, activation='relu', name='conv_3',\n",
    "                           W_regularizer=l2(l2_reg))(conv_3)\n",
    "\n",
    "    if lambda_mask is not None:\n",
    "        conv_3_mask  = np.reshape(lambda_mask[477024:541920],(384, 13, 13))\n",
    "    else:\n",
    "        conv_3_mask = np.ones(shape=((384, 13, 13)))\n",
    "    \n",
    "    conv_3_mask = K.variable(conv_3_mask)\n",
    "    conv_3_lambda = Lambda(lambda x: x * conv_3_mask)(conv_3)\n",
    "\n",
    "    conv_4 = ZeroPadding2D((1, 1))(conv_3_lambda)\n",
    "    conv_4 = merge([\n",
    "        Convolution2D(192, 3, 3, activation=\"relu\", name='conv_4_'+str(i+1),\n",
    "                      W_regularizer=l2(l2_reg))(\n",
    "            splittensor(axis=channel_index, ratio_split=2, id_split=i)(conv_4)\n",
    "        ) for i in range(2)], mode='concat', concat_axis=channel_index, name=\"conv_4\")\n",
    "\n",
    "    if lambda_mask is not None:\n",
    "        conv_4_mask  = np.reshape(lambda_mask[541920:606816],(384, 13, 13))\n",
    "    else:\n",
    "        conv_4_mask = np.ones(shape=((384, 13, 13)))\n",
    "        \n",
    "    conv_4_mask = K.variable(conv_4_mask)\n",
    "    conv_4_lambda = Lambda(lambda x: x * conv_4_mask)(conv_4)\n",
    "\n",
    "    conv_5 = ZeroPadding2D((1, 1))(conv_4_lambda)\n",
    "    conv_5 = merge([\n",
    "        Convolution2D(128, 3, 3, activation=\"relu\", name='conv_5_'+str(i+1),\n",
    "                      W_regularizer=l2(l2_reg))(\n",
    "            splittensor(axis=channel_index, ratio_split=2, id_split=i)(conv_5)\n",
    "        ) for i in range(2)], mode='concat', concat_axis=channel_index, name=\"conv_5\")\n",
    "\n",
    "    if lambda_mask is not None:\n",
    "        conv_5_mask  = np.reshape(lambda_mask[606816:650080],(256, 13, 13))\n",
    "    else:\n",
    "        conv_5_mask = np.ones(shape=((256, 13, 13)))\n",
    "    \n",
    "    conv_5_mask = K.variable(conv_5_mask)\n",
    "    conv_5_lambda = Lambda(lambda x: x * conv_5_mask)(conv_5)\n",
    "\n",
    "    dense_1 = MaxPooling2D((3, 3), strides=(2, 2), name=\"convpool_5\")(conv_5_lambda)\n",
    "\n",
    "    dense_1 = Flatten(name=\"flatten\")(dense_1)\n",
    "    dense_1 = Dense(4096, activation='relu', name='dense_1',\n",
    "                    W_regularizer=l2(l2_reg))(dense_1)\n",
    "\n",
    "    if lambda_mask is not None:\n",
    "        dense_1_mask  = np.reshape(lambda_mask[650080:654176],(4096,))\n",
    "    else:\n",
    "        dense_1_mask = np.ones(shape=((4096,)))\n",
    "    \n",
    "    \n",
    "    dense_1_mask = K.variable(dense_1_mask)\n",
    "    dense_1_lambda = Lambda(lambda x: x * dense_1_mask)(dense_1)\n",
    "\n",
    "    dense_2 = Dropout(0.5)(dense_1_lambda)\n",
    "    dense_2 = Dense(4096, activation='relu', name='dense_2',\n",
    "                    W_regularizer=l2(l2_reg))(dense_2)\n",
    "\n",
    "    if lambda_mask is not None:\n",
    "        dense_2_mask  = np.reshape(lambda_mask[654176:658272],(4096,))\n",
    "    else:\n",
    "        dense_2_mask = np.ones(shape=((4096,)))\n",
    "    \n",
    "    dense_2_mask = K.variable(dense_2_mask)\n",
    "    dense_2_lambda = Lambda(lambda x: x * dense_2_mask)(dense_2)\n",
    "\n",
    "    dense_3 = Dropout(0.5)(dense_2_lambda)\n",
    "    if n_classes == 1000:\n",
    "        dense_3 = Dense(n_classes, name='dense_3',\n",
    "                        W_regularizer=l2(l2_reg))(dense_3)\n",
    "    else:\n",
    "        # We change the name so when loading the weights_file from a\n",
    "        # Imagenet pretrained model does not crash\n",
    "        dense_3 = Dense(n_classes, name='dense_3_new',\n",
    "                        W_regularizer=l2(l2_reg))(dense_3)\n",
    "\n",
    "\n",
    "    prediction = Activation(\"softmax\", name=\"softmax\")(dense_3)\n",
    "\n",
    "    model = Model(input=inputs, output=prediction)\n",
    "    if weights_path:\n",
    "        model.load_weights(weights_path)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the details of all the 1000 classes and the function to conver the synset id to words{\n",
    "meta_clsloc_file = '../../data/meta_clsloc.mat'\n",
    "synsets = loadmat(meta_clsloc_file)['synsets'][0]\n",
    "synsets_imagenet_sorted = sorted([(int(s[0]), str(s[1][0])) for s in synsets[:1000]],key=lambda v: v[1])\n",
    "corr = {}\n",
    "for j in range(1000):\n",
    "    corr[synsets_imagenet_sorted[j][0]] = j\n",
    "\n",
    "corr_inv = {}\n",
    "for j in range(1, 1001):\n",
    "    corr_inv[corr[j]] = j\n",
    "\n",
    "def id_to_words(id_):\n",
    "    return synsets[corr_inv[id_] - 1][2][0]\n",
    "\n",
    "def pprint_output(out, n_max_synsets=10):\n",
    "    wids = []\n",
    "    best_ids = out.argsort()[::-1][:10]\n",
    "    for u in best_ids:\n",
    "        wids.append(str(synsets[corr_inv[u] - 1][1][0]))\n",
    "    #print('%.2f' % round(100 * out[u], 2) + ' : ' + id_to_words(u)+' '+ str(synsets[corr_inv[u] - 1][1][0]))\n",
    "    return wids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code snippet to load the ground truth labels to measure the performance{\n",
    "truth = {}\n",
    "with open('../../data/ILSVRC2014_clsloc_validation_ground_truth.txt') as f:\n",
    "    line_num = 1\n",
    "    for line in f.readlines():\n",
    "        ind_ = int(line)\n",
    "        temp  = None\n",
    "        for i in synsets_imagenet_sorted:\n",
    "            if i[0] == ind_:\n",
    "                temp = i\n",
    "        #print ind_,temp\n",
    "        if temp != None:\n",
    "            truth[line_num] = temp\n",
    "        else:\n",
    "            print '##########', ind_\n",
    "            pass\n",
    "        line_num += 1\n",
    "#}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to predict the top 5 accuracy\n",
    "def top5accuracy(true, predicted):\n",
    "    assert len(true) == len(predicted)\n",
    "    result = []\n",
    "    flag  = 0\n",
    "    for i in range(len(true)):\n",
    "        flag  = 0\n",
    "        temp = true[i]\n",
    "        for j in predicted[i]:\n",
    "            if j == temp:\n",
    "                flag = 1\n",
    "                break\n",
    "        if flag == 1:\n",
    "            result.append(1)\n",
    "        else:\n",
    "            result.append(0)\n",
    "    counter = 0.\n",
    "    for i in result:\n",
    "        if i == 1:\n",
    "            counter += 1.\n",
    "    error = 1.0 - counter/float(len(result))\n",
    "    #print len(np.where(np.asarray(result) == 1)[0])\n",
    "    return len(np.where(np.asarray(result) == 1)[0]), error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#Testing on test data{\n",
    "data_path = '../../data/pkl/'\n",
    "classes = ['animate','inanimate']\n",
    "\n",
    "result = {}\n",
    "\n",
    "with open(data_path+classes[0]+'_train.pkl') as f:\n",
    "    X_fold = pickle.load(f)\n",
    "with open(data_path+classes[1]+'_train.pkl') as f:\n",
    "    y_fold = pickle.load(f)\n",
    "\n",
    "X = np.column_stack((X_fold,y_fold))\n",
    "kmeans = MiniBatchKMeans(n_clusters=65827,\n",
    "                         random_state=0,\n",
    "                         batch_size=6,\n",
    "                         max_iter=10).fit(X)\n",
    "kmeans.cluster_centers_\n",
    "pred_kmeans = kmeans.predict(X)\n",
    "X_new = kmeans.cluster_centers_\n",
    "\n",
    "#DO CLUSTERING AND GET CLUSTERS\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import SpectralClustering\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.cluster import Birch\n",
    "from sklearn.cluster import GaussianMixture\n",
    "\n",
    "j = 1 #Set this value from scree plot!\n",
    "\n",
    "print j\n",
    "clf = KMeans(n_clusters=j, random_state=143)\n",
    "#clf= SpectralClustering(n_clusters=j, assign_labels=\"discretize\",random_state=143)\n",
    "#clf =  AgglomerativeClustering(n_clusters=j, linkage='ward')\n",
    "#clf = Birch(branching_factor=50, n_clusters=j, threshold=0.5,compute_labels=True)\n",
    "#clf = GaussianMixture(n_components=j, covariance_type='full',random_state=143)\n",
    "\n",
    "y_pred = clf.fit_predict(X_new)\n",
    "#print clf.labels_\n",
    "Z = clf.predict(X)\n",
    "\n",
    "for label in set(clf.labels_):\n",
    "    print('Cluster: ',j,'Label: ', label)\n",
    "\n",
    "    #Lesioning and measuring performance\n",
    "    pred = clf.predict(X_new)\n",
    "    loc = np.where(pred==label)\n",
    "    loc_temp = kmeans.predict(X_new[loc[0]])\n",
    "    loc_new =[]\n",
    "    for entry in set(loc_temp):\n",
    "        temp = np.where(pred_kmeans==entry)[0]\n",
    "        loc_new.extend(temp)\n",
    "\n",
    "    lambda_mask = np.ones(shape=((658272,)))\n",
    "    lambda_mask[loc_new] = 0.\n",
    "\n",
    "    #plt.scatter(X[:,0],X[:,1], c=y_pred) \n",
    "\n",
    "    sgd = SGD(lr=0.1, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "    model = AlexNet(weights_path=\"../../data/weights/alexnet_weights.h5\",lambda_mask=lambda_mask)\n",
    "    model.compile(optimizer=sgd, loss='mse')\n",
    "\n",
    "    flag = 0\n",
    "    dprime = 0.\n",
    "    for p in classes:\n",
    "        im_valid_test = []\n",
    "        image_list_valid = '../../data/pkl/'+p+'_image_list_test.txt'\n",
    "        with open(image_list_valid,'rb') as f:\n",
    "            for line in f.readlines():\n",
    "                im_valid_test.append(line.strip('\\n'))\n",
    "        im_temp = preprocess_image_batch(im_valid_test,img_size=(256,256), crop_size=(227,227), color_mode=\"rgb\")\n",
    "        out = model.predict(im_temp,batch_size=64)\n",
    "        \n",
    "        true_valid_wids = []\n",
    "        for i in im_valid_test:\n",
    "                temp1 = i.split('/')[4]\n",
    "                temp = temp1.split('.')[0].split('_')[2]\n",
    "                true_valid_wids.append(truth[int(temp)][1])\n",
    "\n",
    "        predicted_valid_wids = []\n",
    "        for i in range(len(im_valid_test)):\n",
    "            #print im_list[i], pprint_output(out[i]), true_wids[i]\n",
    "            predicted_valid_wids.append(pprint_output(out[i]))\n",
    "\n",
    "        count, error  = top5accuracy(true_valid_wids, predicted_valid_wids)\n",
    "\n",
    "        print str(p)+' '+str(count)+' '+str(len(im_valid_test))+' '+str(error)+' '+str(1-error)\n",
    "\n",
    "        if flag == 0:\n",
    "            dprime = error\n",
    "            flag = 1\n",
    "        else:\n",
    "            dprime -= error\n",
    "\n",
    "    result[label] = dprime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#Version 1.1 - Reading pkl files from step 0 and clustering it{\n",
    "data_path = '../../data/pkl/'\n",
    "classes = ['animate','inanimate']\n",
    "\n",
    "\n",
    "result= {}\n",
    "fold = 1\n",
    "\n",
    "with open(data_path+classes[0]+'_train.pkl') as f:\n",
    "    X_fold = pickle.load(f)\n",
    "with open(data_path+classes[1]+'_train.pkl') as f:\n",
    "    y_fold = pickle.load(f)\n",
    "\n",
    "\n",
    "X = np.column_stack((X_fold,y_fold))\n",
    "kmeans = MiniBatchKMeans(n_clusters=65827,\n",
    "                         random_state=0,\n",
    "                         batch_size=6,\n",
    "                         max_iter=10).fit(X)\n",
    "kmeans.cluster_centers_\n",
    "pred_kmeans = kmeans.predict(X)\n",
    "X_new = kmeans.cluster_centers_\n",
    "\n",
    "\n",
    "#DO CLUSTERING AND GET CLUSTERS\n",
    "\n",
    "from sklearn.cluster import AffinityPropagation\n",
    "\n",
    "method ='AffProp'\n",
    "print(method)\n",
    "\n",
    "clf = AffinityPropagation()\n",
    "y_pred = clf.fit_predict(X_new)\n",
    "#print clf.cluster_centers_\n",
    "\n",
    "for label in set(clf.labels_):\n",
    "    print('Cluster: ',j,'Label: ', label)\n",
    "\n",
    "    #Lesioning and measuring performance\n",
    "    pred = clf.predict(X_new)\n",
    "    loc = np.where(pred==label)\n",
    "    loc_temp = kmeans.predict(X_new[loc[0]])\n",
    "    loc_new =[]\n",
    "    for entry in set(loc_temp):\n",
    "        temp = np.where(pred_kmeans==entry)[0]\n",
    "        loc_new.extend(temp)\n",
    "\n",
    "    lambda_mask = np.ones(shape=((658272,)))\n",
    "    lambda_mask[loc_new] = 0.\n",
    "\n",
    "    #plt.scatter(X[:,0],X[:,1], c=y_pred) \n",
    "\n",
    "    sgd = SGD(lr=0.1, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "    model = AlexNet(weights_path=\"../../data/weights/alexnet_weights.h5\",lambda_mask=lambda_mask)\n",
    "    model.compile(optimizer=sgd, loss='mse')\n",
    "\n",
    "    flag = 0\n",
    "    dprime = 0.\n",
    "    for p in classes:\n",
    "        im_valid_test = []\n",
    "        image_list_valid = '../../data/pkl/'+p+'_image_list_test.txt'\n",
    "        with open(image_list_valid,'rb') as f:\n",
    "            for line in f.readlines():\n",
    "                im_valid_test.append(line.strip('\\n'))\n",
    "        im_temp = preprocess_image_batch(im_valid_test,img_size=(256,256), crop_size=(227,227), color_mode=\"rgb\")\n",
    "        out = model.predict(im_temp,batch_size=64)\n",
    "\n",
    "        true_valid_wids = []\n",
    "        for i in im_valid_test:\n",
    "                temp1 = i.split('/')[4]\n",
    "                temp = temp1.split('.')[0].split('_')[2]\n",
    "                true_valid_wids.append(truth[int(temp)][1])\n",
    "\n",
    "        predicted_valid_wids = []\n",
    "        for i in range(len(im_valid_test)):\n",
    "            #print im_list[i], pprint_output(out[i]), true_wids[i]\n",
    "            predicted_valid_wids.append(pprint_output(out[i]))\n",
    "\n",
    "        count, error  = top5accuracy(true_valid_wids, predicted_valid_wids)\n",
    "\n",
    "        print str(p)+' '+str(fold)+' '+str(count)+' '+str(len(im_valid_test))+' '+str(error)\n",
    "\n",
    "        if flag == 0:\n",
    "            dprime = error\n",
    "            flag = 1\n",
    "        else:\n",
    "            dprime -= error\n",
    "\n",
    "    result[label] = dprime\n",
    "\n",
    "#}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.values(),X.shape,Z.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_temp = []\n",
    "for item in y_pred:\n",
    "    z_temp.append(result[item])\n",
    "loc_z = kmeans.predict(X_new)\n",
    "z = np.ones(shape=((658272,)))\n",
    "for i in range(len(loc_z)):\n",
    "    temp = np.where(pred_kmeans==loc_z[i])[0]\n",
    "    z[temp] = z_temp[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = X[:,0]\n",
    "y = X[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Density Plot for Animate/Inanimate\n",
    "\n",
    "print(x.shape,y.shape,z.shape)\n",
    "fig, ax = plt.subplots()\n",
    "cs = ax.scatter(x, y, c=z, s=10,cmap=plt.cm.RdYlGn)\n",
    "cbar = fig.colorbar(cs)\n",
    "lims = [\n",
    "    np.min([ax.get_xlim(), ax.get_ylim()]),  # min of both axes\n",
    "    np.max([ax.get_xlim(), ax.get_ylim()]),  # max of both axes\n",
    "]\n",
    "ax.plot(lims, lims, 'k-', alpha=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print result.values().index(max(result.values())), result.values().index(min(result.values()))\n",
    "ana = int(result.values().index(max(result.values())))\n",
    "ina = int(result.values().index(min(result.values())))\n",
    "print (result[ana], -1*(result[ina])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#D' histogram for animate and inanimate double bars for different clustering techniques\n",
    "\n",
    "labels = ['kNN','Spectral']\n",
    "animate_means = [0.78,0.8]\n",
    "inanimate_means = [0.88, 0.73]\n",
    "\n",
    "x = np.arange(len(labels))  # the label locations\n",
    "width = 0.35  # the width of the bars\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "rects1 = ax.bar(x - width/2, animate_means, width, label='Animate')\n",
    "rects2 = ax.bar(x + width/2, inanimate_means, width, label='Inanimate')\n",
    "\n",
    "# Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "ax.set_ylabel('Classification Performance')\n",
    "ax.set_title('Performance by Class')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(labels)\n",
    "ax.legend()\n",
    "plt.ylim([0.,1.])\n",
    "\n",
    "def autolabel(rects):\n",
    "    \"\"\"Attach a text label above each bar in *rects*, displaying its height.\"\"\"\n",
    "    for rect in rects:\n",
    "        height = rect.get_height()\n",
    "        ax.annotate('{}'.format(height),\n",
    "                    xy=(rect.get_x() + rect.get_width() / 2, height),\n",
    "                    xytext=(0, 3),  # 3 points vertical offset\n",
    "                    textcoords=\"offset points\",\n",
    "                    ha='center', va='bottom')\n",
    "\n",
    "\n",
    "autolabel(rects1)\n",
    "autolabel(rects2)\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "lims = [\n",
    "    np.min([ax.get_xlim(), ax.get_ylim()]),  # min of both axes\n",
    "    np.max([ax.get_xlim(), ax.get_ylim()]),  # max of both axes\n",
    "]\n",
    "ax.plot(lims, lims, 'k-', alpha=0.75)\n",
    "ax.scatter(X[:,0],X[:,1]) \n",
    "plt.xlim([-2.2,250])\n",
    "plt.ylim([-2.2,300])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "label_loc = np.where(pred==7)[0]\n",
    "Z = X[label_loc]\n",
    "plt.scatter(Z[:,0],Z[:,1]) \n",
    "plt.xlim([-2.2,250])\n",
    "plt.ylim([-2.2,300])\n",
    "lims = [\n",
    "    np.min([ax.get_xlim(), ax.get_ylim()]),  # min of both axes\n",
    "    np.max([ax.get_xlim(), ax.get_ylim()]),  # max of both axes\n",
    "]\n",
    "\n",
    "# now plot both limits against eachother\n",
    "ax.plot(lims, lims, 'k-', alpha=0.75, zorder=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "label_loc = np.where(pred==ana)[0]\n",
    "Z = X[label_loc]\n",
    "plt.scatter(Z[:,0],Z[:,1]) \n",
    "plt.xlim([-2.2,250])\n",
    "plt.ylim([-2.2,300])\n",
    "lims = [\n",
    "    np.min([ax.get_xlim(), ax.get_ylim()]),  # min of both axes\n",
    "    np.max([ax.get_xlim(), ax.get_ylim()]),  # max of both axes\n",
    "]\n",
    "\n",
    "# now plot both limits against eachother\n",
    "ax.plot(lims, lims, 'k-', alpha=0.75, zorder=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "label_loc = np.where(pred==ina)[0]\n",
    "Z = X[label_loc]\n",
    "plt.scatter(Z[:,0],Z[:,1]) \n",
    "plt.xlim([-2.2,250])\n",
    "plt.ylim([-2.2,300])\n",
    "lims = [\n",
    "    np.min([ax.get_xlim(), ax.get_ylim()]),  # min of both axes\n",
    "    np.max([ax.get_xlim(), ax.get_ylim()]),  # max of both axes\n",
    "]\n",
    "\n",
    "# now plot both limits against eachother\n",
    "ax.plot(lims, lims, 'k-', alpha=0.75, zorder=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_loc = np.where(pred==7)[0]\n",
    "X = np.arange(8)\n",
    "Y = np.zeros((8,))\n",
    "\n",
    "for i in label_loc:\n",
    "    if i in range(0,290400): \n",
    "        Y[0] += 1\n",
    "    elif i in range(290400,477024):\n",
    "        Y[1] += 1\n",
    "    elif i in range(477024,541920):\n",
    "        Y[2] += 1\n",
    "    elif i in range(541920,606816):\n",
    "        Y[3] += 1\n",
    "    elif i in range(606816,650080):\n",
    "        Y[4] += 1\n",
    "    elif i in range(650080,654176):\n",
    "        Y[5] += 1\n",
    "    elif i in range(654176,658272):\n",
    "        Y[6] += 1\n",
    "    elif i in range(658272,659272):\n",
    "        Y[7] += 1\n",
    "    else:\n",
    "        print i\n",
    "Y[0] = float(Y[0]) /290400\n",
    "Y[1] = float(Y[0]) / 196624\n",
    "Y[2] = float(Y[2]) / 64896\n",
    "Y[3] = float(Y[3]) / 64896\n",
    "Y[4] = float(Y[4]) /43264\n",
    "Y[5] = float(Y[5]) /4096\n",
    "Y[6] = float(Y[6]) /4096\n",
    "Y[7] = float(Y[7]) /1000\n",
    "plt.bar(X,Y)\n",
    "plt.xticks(X, ('conv_1', 'conv_2', 'conv_3', 'conv_4','conv_5','fc1','fc2','softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "X = np.arange(8)\n",
    "Y[0] = float(Y[0]) /290400\n",
    "Y[1] = float(Y[0]) / 196624\n",
    "Y[2] = float(Y[2]) / 64896\n",
    "Y[3] = float(Y[3]) / 64896\n",
    "Y[4] = float(Y[4]) /43264\n",
    "Y[5] = float(Y[5]) /4096\n",
    "Y[6] = float(Y[6]) /4096\n",
    "Y[7] = float(Y[7]) /1000\n",
    "plt.bar(X,Y)\n",
    "plt.xticks(X, ('conv_1', 'conv_2', 'conv_3', 'conv_4','conv_5','fc1','fc2','softmax'))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_loc = np.where(pred==ana)[0]\n",
    "X = np.arange(8)\n",
    "Y = np.zeros((8,))\n",
    "\n",
    "for i in label_loc:\n",
    "    if i in range(0,290400): \n",
    "        Y[0] += 1\n",
    "    elif i in range(290400,477024):\n",
    "        Y[1] += 1\n",
    "    elif i in range(477024,541920):\n",
    "        Y[2] += 1\n",
    "    elif i in range(541920,606816):\n",
    "        Y[3] += 1\n",
    "    elif i in range(606816,650080):\n",
    "        Y[4] += 1\n",
    "    elif i in range(650080,654176):\n",
    "        Y[5] += 1\n",
    "    elif i in range(654176,658272):\n",
    "        Y[6] += 1\n",
    "    elif i in range(658272,659272):\n",
    "        Y[7] += 1\n",
    "    else:\n",
    "        print i\n",
    "Y[0] = float(Y[0]) /290400\n",
    "Y[1] = float(Y[0]) / 196624\n",
    "Y[2] = float(Y[2]) / 64896\n",
    "Y[3] = float(Y[3]) / 64896\n",
    "Y[4] = float(Y[4]) /43264\n",
    "Y[5] = float(Y[5]) /4096\n",
    "Y[6] = float(Y[6]) /4096\n",
    "Y[7] = float(Y[7]) /1000\n",
    "plt.ylim([0,1.])\n",
    "plt.bar(X,Y)\n",
    "plt.xticks(X, ('conv_1', 'conv_2', 'conv_3', 'conv_4','conv_5','fc1','fc2','softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_loc = np.where(pred==ina)[0]\n",
    "X = np.arange(8)\n",
    "Y = np.zeros((8,))\n",
    "\n",
    "for i in label_loc:\n",
    "    if i in range(0,290400): \n",
    "        Y[0] += 1\n",
    "    elif i in range(290400,477024):\n",
    "        Y[1] += 1\n",
    "    elif i in range(477024,541920):\n",
    "        Y[2] += 1\n",
    "    elif i in range(541920,606816):\n",
    "        Y[3] += 1\n",
    "    elif i in range(606816,650080):\n",
    "        Y[4] += 1\n",
    "    elif i in range(650080,654176):\n",
    "        Y[5] += 1\n",
    "    elif i in range(654176,658272):\n",
    "        Y[6] += 1\n",
    "    elif i in range(658272,659272):\n",
    "        Y[7] += 1\n",
    "    else:\n",
    "        print i\n",
    "Y[0] = float(Y[0]) /290400\n",
    "Y[1] = float(Y[0]) / 196624\n",
    "Y[2] = float(Y[2]) / 64896\n",
    "Y[3] = float(Y[3]) / 64896\n",
    "Y[4] = float(Y[4]) /43264\n",
    "Y[5] = float(Y[5]) /4096\n",
    "Y[6] = float(Y[6]) /4096\n",
    "Y[7] = float(Y[7]) /1000\n",
    "plt.ylim([0,1.])\n",
    "plt.bar(X,Y)\n",
    "plt.xticks(X, ('conv_1', 'conv_2', 'conv_3', 'conv_4','conv_5','fc1','fc2','softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
