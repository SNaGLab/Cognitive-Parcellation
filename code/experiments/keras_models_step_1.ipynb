{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#To hide warnings export PYTHONWARNINGS=\"ignore\"\n",
    "#Imports{\n",
    "\n",
    "import os\n",
    "from os.path import dirname\n",
    "from os.path import join\n",
    "os.environ['KERAS_BACKEND'] = 'tensorflow'\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' #Cha\n",
    "\n",
    "\n",
    "import sys\n",
    "import warnings\n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    \n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from imageio import imread\n",
    "from skimage.transform import resize\n",
    "from scipy.io import loadmat\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "import PIL.Image\n",
    "#}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confirm TensorFlow sees the GPU\n",
    "from tensorflow.python.client import device_lib\n",
    "assert 'GPU' in str(device_lib.list_local_devices())\n",
    "\n",
    "# confirm Keras sees the GPU (for TensorFlow 1.X + Keras)\n",
    "from keras import backend\n",
    "assert len(backend.tensorflow_backend._get_available_gpus()) > 0\n",
    "\n",
    "# confirm PyTorch sees the GPU\n",
    "from torch import cuda\n",
    "assert cuda.is_available()\n",
    "assert cuda.device_count() > 0\n",
    "print(cuda.get_device_name(cuda.current_device()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model architecture\n",
    "<img src=\"../../data/misc/vgg16.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code snippet needed to read activation values from each layer of the pre-trained artificial neural networks\n",
    "def get_activations(model, layer, X_batch):\n",
    "    #keras.backend.function(inputs, outputs, updates=None)\n",
    "    get_activations = keras.backend.function([model.layers[0].input, keras.backend.learning_phase()], [model.layers[layer].output,])\n",
    "    #The learning phase flag is a bool tensor (0 = test, 1 = train)\n",
    "    activations = get_activations([X_batch,0])\n",
    "    return activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to pre-process the input image to ensure uniform size and color\n",
    "def preprocess_image_batch(image_paths, img_size=None, crop_size=None, color_mode='rgb', out=None):\n",
    "    \"\"\"\n",
    "    Consistent preprocessing of images batches\n",
    "    :param image_paths: iterable: images to process\n",
    "    :param crop_size: tuple: crop images if specified\n",
    "    :param img_size: tuple: resize images if specified\n",
    "    :param color_mode: Use rgb or change to bgr mode based on type of model you want to use\n",
    "    :param out: append output to this iterable if specified\n",
    "    \"\"\"\n",
    "    img_list = []\n",
    "\n",
    "    for im_path in image_paths:\n",
    "        '''\n",
    "        img = imread(im_path,as_gray=False, pilmode=\"RGB\")\n",
    "        #print im_path\n",
    "        #print img.shape\n",
    "        if img_size:\n",
    "            img = resize(img, img_size)\n",
    "\n",
    "        img = img.astype('float32')\n",
    "        # We normalize the colors (in RGB space) with the empirical means on the training set\n",
    "        img[:, :, 0] -= 123.68\n",
    "        img[:, :, 1] -= 116.779\n",
    "        img[:, :, 2] -= 103.939\n",
    "        # We permute the colors to get them in the BGR order\n",
    "        if color_mode == 'bgr':\n",
    "            img[:, :, [0, 1, 2]] = img[:, :, [2, 1, 0]]\n",
    "        img = img.transpose((2, 0, 1))\n",
    "\n",
    "        if crop_size:\n",
    "            img = img[:, (img_size[0] - crop_size[0]) // 2:(img_size[0] + crop_size[0]) // 2\n",
    "            , (img_size[1] - crop_size[1]) // 2:(img_size[1] + crop_size[1]) // 2]\n",
    "\n",
    "        img_list.append(img)\n",
    "        '''\n",
    "        size = 224\n",
    "        ret = PIL.Image.open(im_path)\n",
    "        ret = ret.resize((size, size))\n",
    "        ret = np.asarray(ret, dtype=np.uint8).astype(np.float32)\n",
    "        if ret.ndim == 2:\n",
    "            ret.resize((size, size, 1))\n",
    "            ret = np.repeat(ret, 3, axis=-1)\n",
    "        ret = ret.transpose((2, 0, 1))\n",
    "        ret = np.flip(ret,0)\n",
    "        x = preprocess_input(ret)\n",
    "        img_list.append(x)\n",
    "        \n",
    "\n",
    "    try:\n",
    "        img_batch = np.stack(img_list, axis=0)\n",
    "    except:\n",
    "        print(im_path)\n",
    "        raise ValueError('when img_size and crop_size are None, images'\n",
    "                ' in image_paths must have the same shapes.')\n",
    "\n",
    "    if out is not None and hasattr(out, 'append'):\n",
    "        out.append(img_batch)\n",
    "    else:\n",
    "        return img_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to predict the top 5 accuracy\n",
    "def top5accuracy(true, predicted):\n",
    "    assert len(true) == len(predicted)\n",
    "    result = []\n",
    "    flag  = 0\n",
    "    for i in range(len(true)):\n",
    "        flag  = 0\n",
    "        temp = true[i]\n",
    "        for j in predicted[i][0:5]:\n",
    "            if j == temp:\n",
    "                flag = 1\n",
    "                break\n",
    "        if flag == 1:\n",
    "            result.append(1)\n",
    "        else:\n",
    "            result.append(0)\n",
    "    counter = 0.\n",
    "    for i in result:\n",
    "        if i == 1:\n",
    "            counter += 1.\n",
    "    error = 1.0 - counter/float(len(result))\n",
    "    #print len(np.where(np.asarray(result) == 1)[0])\n",
    "    return len(np.where(np.asarray(result) == 1)[0]), error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the details of all the 1000 classes and the function to conver the synset id to words{\n",
    "meta_clsloc_file = '../../data/meta_clsloc.mat'\n",
    "synsets = loadmat(meta_clsloc_file)['synsets'][0]\n",
    "synsets_imagenet_sorted = sorted([(int(s[0]), str(s[1][0])) for s in synsets[:1000]],key=lambda v: v[1])\n",
    "corr = {}\n",
    "for j in range(1000):\n",
    "    corr[synsets_imagenet_sorted[j][0]] = j\n",
    "\n",
    "corr_inv = {}\n",
    "for j in range(1, 1001):\n",
    "    corr_inv[corr[j]] = j\n",
    "\n",
    "def id_to_words(id_):\n",
    "    return synsets[corr_inv[id_] - 1][2][0]\n",
    "\n",
    "def pprint_output(out, n_max_synsets=10):\n",
    "    wids = []\n",
    "    best_ids = out.argsort()[::-1][:10]\n",
    "    for u in best_ids:\n",
    "        wids.append(str(synsets[corr_inv[u] - 1][1][0]))\n",
    "    #print('%.2f' % round(100 * out[u], 2) + ' : ' + id_to_words(u)+' '+ str(synsets[corr_inv[u] - 1][1][0]))\n",
    "    return wids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code snippet to load the ground truth labels to measure the performance{\n",
    "truth = {}\n",
    "with open('../../data/ILSVRC2014_clsloc_validation_ground_truth.txt') as f:\n",
    "    line_num = 1\n",
    "    for line in f.readlines():\n",
    "        ind_ = int(line)\n",
    "        temp  = None\n",
    "        for i in synsets_imagenet_sorted:\n",
    "            if i[0] == ind_:\n",
    "                temp = i\n",
    "        #print ind_,temp\n",
    "        if temp != None:\n",
    "            truth[line_num] = temp\n",
    "        else:\n",
    "            print('##########', ind_)\n",
    "            pass\n",
    "        line_num += 1\n",
    "#}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "from keras import layers\n",
    "from keras import models\n",
    "import keras.utils as keras_utils\n",
    "from keras.layers import Lambda\n",
    "\n",
    "from  keras.applications import imagenet_utils\n",
    "from  keras.applications.imagenet_utils import decode_predictions\n",
    "from  keras_applications.imagenet_utils import _obtain_input_shape\n",
    "\n",
    "preprocess_input = imagenet_utils.preprocess_input\n",
    "\n",
    "WEIGHTS_PATH = ('https://github.com/fchollet/deep-learning-models/'\n",
    "                'releases/download/v0.1/'\n",
    "                'vgg16_weights_tf_dim_ordering_tf_kernels.h5')\n",
    "WEIGHTS_PATH_NO_TOP = ('https://github.com/fchollet/deep-learning-models/'\n",
    "                       'releases/download/v0.1/'\n",
    "                       'vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5')\n",
    "\n",
    "\n",
    "def VGG16(include_top=True,\n",
    "          weights='imagenet',\n",
    "          input_tensor=None,\n",
    "          input_shape=None,\n",
    "          pooling=None,\n",
    "          classes=1000,\n",
    "          lambda_mask=None,\n",
    "          **kwargs):\n",
    "    \"\"\"Instantiates the VGG16 architecture.\n",
    "    Optionally loads weights pre-trained on ImageNet.\n",
    "    Note that the data format convention used by the model is\n",
    "    the one specified in your Keras config at `~/.keras/keras.json`.\n",
    "    # Arguments\n",
    "        include_top: whether to include the 3 fully-connected\n",
    "            layers at the top of the network.\n",
    "        weights: one of `None` (random initialization),\n",
    "              'imagenet' (pre-training on ImageNet),\n",
    "              or the path to the weights file to be loaded.\n",
    "        input_tensor: optional Keras tensor\n",
    "            (i.e. output of `layers.Input()`)\n",
    "            to use as image input for the model.\n",
    "        input_shape: optional shape tuple, only to be specified\n",
    "            if `include_top` is False (otherwise the input shape\n",
    "            has to be `(224, 224, 3)`\n",
    "            (with `channels_last` data format)\n",
    "            or `(3, 224, 224)` (with `channels_first` data format).\n",
    "            It should have exactly 3 input channels,\n",
    "            and width and height should be no smaller than 32.\n",
    "            E.g. `(200, 200, 3)` would be one valid value.\n",
    "        pooling: Optional pooling mode for feature extraction\n",
    "            when `include_top` is `False`.\n",
    "            - `None` means that the output of the model will be\n",
    "                the 4D tensor output of the\n",
    "                last convolutional block.\n",
    "            - `avg` means that global average pooling\n",
    "                will be applied to the output of the\n",
    "                last convolutional block, and thus\n",
    "                the output of the model will be a 2D tensor.\n",
    "            - `max` means that global max pooling will\n",
    "                be applied.\n",
    "        classes: optional number of classes to classify images\n",
    "            into, only to be specified if `include_top` is True, and\n",
    "            if no `weights` argument is specified.\n",
    "    # Returns\n",
    "        A Keras model instance.\n",
    "    # Raises\n",
    "        ValueError: in case of invalid argument for `weights`,\n",
    "            or invalid input shape.\n",
    "    \"\"\"\n",
    "    backend= keras.backend\n",
    "    layers = keras.layers\n",
    "    models = keras.models\n",
    "    keras_utils = keras.utils\n",
    "\n",
    "    if not (weights in {'imagenet', None} or os.path.exists(weights)):\n",
    "        raise ValueError('The `weights` argument should be either '\n",
    "                         '`None` (random initialization), `imagenet` '\n",
    "                         '(pre-training on ImageNet), '\n",
    "                         'or the path to the weights file to be loaded.')\n",
    "\n",
    "    if weights == 'imagenet' and include_top and classes != 1000:\n",
    "        raise ValueError('If using `weights` as `\"imagenet\"` with `include_top`'\n",
    "                         ' as true, `classes` should be 1000')\n",
    "    # Determine proper input shape\n",
    "    input_shape = _obtain_input_shape(input_shape,\n",
    "                                      default_size=224,\n",
    "                                      min_size=32,\n",
    "                                      data_format=backend.image_data_format(),\n",
    "                                      require_flatten=include_top,\n",
    "                                      weights=weights)\n",
    "\n",
    "    if input_tensor is None:\n",
    "        img_input = layers.Input(shape=input_shape)\n",
    "    else:\n",
    "        if not backend.is_keras_tensor(input_tensor):\n",
    "            img_input = layers.Input(tensor=input_tensor, shape=input_shape)\n",
    "        else:\n",
    "            img_input = input_tensor\n",
    "    # Block 1\n",
    "    x = layers.Conv2D(64, (3, 3),\n",
    "                      activation='relu',\n",
    "                      padding='same',\n",
    "                      name='block1_conv1')(img_input)\n",
    "    \n",
    "    if lambda_mask is not None:\n",
    "        block_1_conv_1_mask  = np.reshape(lambda_mask[0:3211264], (64, 224, 224))\n",
    "    else:\n",
    "        block_1_conv_1_mask = np.ones(shape=((64, 224, 224)))\n",
    "    \n",
    "    block_1_conv_1_mask  = backend.variable(block_1_conv_1_mask)\n",
    "    block_1_conv_1_lambda = Lambda(lambda x: x * block_1_conv_1_mask)(x)\n",
    "    \n",
    "    x = layers.Conv2D(64, (3, 3),\n",
    "                      activation='relu',\n",
    "                      padding='same',\n",
    "                      name='block1_conv2')(block_1_conv_1_lambda)\n",
    "    \n",
    "    if lambda_mask is not None:\n",
    "        block_1_conv_2_mask  = np.reshape(lambda_mask[3211264:6422528], (64, 224, 224))\n",
    "    else:\n",
    "        block_1_conv_2_mask = np.ones(shape=((64, 224, 224)))\n",
    "    \n",
    "    block_1_conv_2_mask  = backend.variable(block_1_conv_2_mask)\n",
    "    block_1_conv_2_lambda = Lambda(lambda x: x * block_1_conv_2_mask)(x)\n",
    "    \n",
    "    \n",
    "    x = layers.MaxPooling2D((2, 2), strides=(2, 2), name='block1_pool')(block_1_conv_2_lambda)\n",
    "\n",
    "    # Block 2\n",
    "    x = layers.Conv2D(128, (3, 3),\n",
    "                      activation='relu',\n",
    "                      padding='same',\n",
    "                      name='block2_conv1')(x)\n",
    "    \n",
    "    if lambda_mask is not None:\n",
    "        block_2_conv_1_mask  = np.reshape(lambda_mask[6422528:8028160], (128, 112, 112))\n",
    "    else:\n",
    "        block_2_conv_1_mask = np.ones(shape=((128, 112, 112)))\n",
    "    \n",
    "    block_2_conv_1_mask  = backend.variable(block_2_conv_1_mask)\n",
    "    block_2_conv_1_lambda = Lambda(lambda x: x * block_2_conv_1_mask)(x)\n",
    "    \n",
    "    x = layers.Conv2D(128, (3, 3),\n",
    "                      activation='relu',\n",
    "                      padding='same',\n",
    "                      name='block2_conv2')(block_2_conv_1_lambda)\n",
    "    \n",
    "    \n",
    "    if lambda_mask is not None:\n",
    "        block_2_conv_2_mask  = np.reshape(lambda_mask[8028160:9633792], (128, 112, 112))\n",
    "    else:\n",
    "        block_2_conv_2_mask = np.ones(shape=((128, 112, 112)))\n",
    "    \n",
    "    block_2_conv_2_mask  = backend.variable(block_2_conv_2_mask)\n",
    "    block_2_conv_2_lambda = Lambda(lambda x: x * block_2_conv_2_mask)(x)\n",
    "    \n",
    "    x = layers.MaxPooling2D((2, 2), strides=(2, 2), name='block2_pool')(block_2_conv_2_lambda)\n",
    "\n",
    "    # Block 3\n",
    "    x = layers.Conv2D(256, (3, 3),\n",
    "                      activation='relu',\n",
    "                      padding='same',\n",
    "                      name='block3_conv1')(x)\n",
    "    \n",
    "    if lambda_mask is not None:\n",
    "        block_3_conv_1_mask  = np.reshape(lambda_mask[9633792:10436608], (256, 56, 56))\n",
    "    else:\n",
    "        block_3_conv_1_mask = np.ones(shape=((256, 56, 56)))\n",
    "    \n",
    "    block_3_conv_1_mask  = backend.variable(block_3_conv_1_mask)\n",
    "    block_3_conv_1_lambda = Lambda(lambda x: x * block_3_conv_1_mask)(x)\n",
    "    \n",
    "    \n",
    "    x = layers.Conv2D(256, (3, 3),\n",
    "                      activation='relu',\n",
    "                      padding='same',\n",
    "                      name='block3_conv2')(block_3_conv_1_lambda)\n",
    "    \n",
    "    if lambda_mask is not None:\n",
    "        block_3_conv_2_mask  = np.reshape(lambda_mask[10436608:11239424], (256, 56, 56))\n",
    "    else:\n",
    "        block_3_conv_2_mask = np.ones(shape=((256, 56, 56)))\n",
    "    \n",
    "    block_3_conv_2_mask  = backend.variable(block_3_conv_2_mask)\n",
    "    block_3_conv_2_lambda = Lambda(lambda x: x * block_3_conv_2_mask)(x)\n",
    "    \n",
    "    x = layers.Conv2D(256, (3, 3),\n",
    "                      activation='relu',\n",
    "                      padding='same',\n",
    "                      name='block3_conv3')( block_3_conv_2_lambda)\n",
    "    \n",
    "    if lambda_mask is not None:\n",
    "        block_3_conv_3_mask  = np.reshape(lambda_mask[11239424:12042240], (256, 56, 56))\n",
    "    else:\n",
    "        block_3_conv_3_mask = np.ones(shape=((256, 56, 56)))\n",
    "    \n",
    "    block_3_conv_3_mask  = backend.variable(block_3_conv_3_mask)\n",
    "    block_3_conv_3_lambda = Lambda(lambda x: x * block_3_conv_3_mask)(x)\n",
    "    \n",
    "    x = layers.MaxPooling2D((2, 2), strides=(2, 2), name='block3_pool')(block_3_conv_3_lambda)\n",
    "\n",
    "    # Block 4\n",
    "    x = layers.Conv2D(512, (3, 3),\n",
    "                      activation='relu',\n",
    "                      padding='same',\n",
    "                      name='block4_conv1')(x)\n",
    "    \n",
    "    if lambda_mask is not None:\n",
    "        block_4_conv_1_mask  = np.reshape(lambda_mask[12042240:12443648], (512, 28, 28))\n",
    "    else:\n",
    "        block_4_conv_1_mask = np.ones(shape=((512, 28, 28)))\n",
    "    \n",
    "    block_4_conv_1_mask  = backend.variable(block_4_conv_1_mask)\n",
    "    block_4_conv_1_lambda = Lambda(lambda x: x * block_4_conv_1_mask)(x)\n",
    "    \n",
    "    \n",
    "    x = layers.Conv2D(512, (3, 3),\n",
    "                      activation='relu',\n",
    "                      padding='same',\n",
    "                      name='block4_conv2')(block_4_conv_1_lambda)\n",
    "    \n",
    "    if lambda_mask is not None:\n",
    "        block_4_conv_2_mask  = np.reshape(lambda_mask[12443648:12845056], (512, 28, 28))\n",
    "    else:\n",
    "        block_4_conv_2_mask = np.ones(shape=((512, 28, 28)))\n",
    "    \n",
    "    block_4_conv_2_mask  = backend.variable(block_4_conv_2_mask)\n",
    "    block_4_conv_2_lambda = Lambda(lambda x: x * block_4_conv_2_mask)(x)\n",
    "    \n",
    "    \n",
    "    x = layers.Conv2D(512, (3, 3),\n",
    "                      activation='relu',\n",
    "                      padding='same',\n",
    "                      name='block4_conv3')(block_4_conv_2_lambda)\n",
    "    \n",
    "    \n",
    "    if lambda_mask is not None:\n",
    "        block_4_conv_3_mask  = np.reshape(lambda_mask[12845056:13246464], (512, 28, 28))\n",
    "    else:\n",
    "        block_4_conv_3_mask = np.ones(shape=((512, 28, 28)))\n",
    "    \n",
    "    block_4_conv_3_mask  = backend.variable(block_4_conv_3_mask)\n",
    "    block_4_conv_3_lambda = Lambda(lambda x: x * block_4_conv_3_mask)(x)\n",
    "    \n",
    "    \n",
    "    x = layers.MaxPooling2D((2, 2), strides=(2, 2), name='block4_pool')( block_4_conv_3_lambda)\n",
    "\n",
    "    # Block 5\n",
    "    x = layers.Conv2D(512, (3, 3),\n",
    "                      activation='relu',\n",
    "                      padding='same',\n",
    "                      name='block5_conv1')(x)\n",
    "    \n",
    "    if lambda_mask is not None:\n",
    "        block_5_conv_1_mask  = np.reshape(lambda_mask[13246464:13346816], (512, 14, 14))\n",
    "    else:\n",
    "        block_5_conv_1_mask = np.ones(shape=((512, 14, 14)))\n",
    "    \n",
    "    block_5_conv_1_mask  = backend.variable(block_5_conv_1_mask)\n",
    "    block_5_conv_1_lambda = Lambda(lambda x: x * block_5_conv_1_mask)(x)\n",
    "    \n",
    "    x = layers.Conv2D(512, (3, 3),\n",
    "                      activation='relu',\n",
    "                      padding='same',\n",
    "                      name='block5_conv2')(block_5_conv_1_lambda)\n",
    "    \n",
    "    if lambda_mask is not None:\n",
    "        block_5_conv_2_mask  = np.reshape(lambda_mask[13346816:13447168], (512, 14, 14))\n",
    "    else:\n",
    "        block_5_conv_2_mask = np.ones(shape=((512, 14, 14)))\n",
    "    \n",
    "    block_5_conv_2_mask  = backend.variable(block_5_conv_2_mask)\n",
    "    block_5_conv_2_lambda = Lambda(lambda x: x * block_5_conv_2_mask)(x)\n",
    "    \n",
    "    x = layers.Conv2D(512, (3, 3),\n",
    "                      activation='relu',\n",
    "                      padding='same',\n",
    "                      name='block5_conv3')(block_5_conv_2_lambda)\n",
    "    \n",
    "    if lambda_mask is not None:\n",
    "        block_5_conv_3_mask  = np.reshape(lambda_mask[13447168:13547520], (512, 14, 14))\n",
    "    else:\n",
    "        block_5_conv_3_mask = np.ones(shape=((512, 14, 14)))\n",
    "    \n",
    "    block_5_conv_3_mask  = backend.variable(block_5_conv_3_mask)\n",
    "    block_5_conv_3_lambda = Lambda(lambda x: x * block_5_conv_3_mask)(x)\n",
    "    \n",
    "    x = layers.MaxPooling2D((2, 2), strides=(2, 2), name='block5_pool')( block_5_conv_3_lambda)\n",
    "\n",
    "    if include_top:\n",
    "        # Classification block\n",
    "        x = layers.Flatten(name='flatten')(x)\n",
    "        x = layers.Dense(4096, activation='relu', name='fc1')(x)\n",
    "        \n",
    "        if lambda_mask is not None:\n",
    "            block_fc1_mask  = np.reshape(lambda_mask[13547520:13551616], (4096,))\n",
    "        else:\n",
    "            block_fc1_mask = np.ones(shape=((4096,)))\n",
    "        block_fc1_mask  = backend.variable(block_fc1_mask)\n",
    "        block_fc1_lambda = Lambda(lambda x: x * block_fc1_mask)(x)\n",
    "    \n",
    "        x = layers.Dense(4096, activation='relu', name='fc2')(block_fc1_lambda)\n",
    "        \n",
    "        if lambda_mask is not None:\n",
    "            block_fc2_mask  = np.reshape(lambda_mask[13551616:13555712], (4096,))\n",
    "        else:\n",
    "            block_fc2_mask = np.ones(shape=((4096,)))\n",
    "        block_fc2_mask  = backend.variable(block_fc2_mask)\n",
    "        block_fc2_lambda = Lambda(lambda x: x * block_fc2_mask)(x)\n",
    "        \n",
    "        x = layers.Dense(classes, activation='softmax', name='predictions')(block_fc2_lambda)\n",
    "    else:\n",
    "        if pooling == 'avg':\n",
    "            x = layers.GlobalAveragePooling2D()(x)\n",
    "        elif pooling == 'max':\n",
    "            x = layers.GlobalMaxPooling2D()(x)\n",
    "\n",
    "    # Ensure that the model takes into account\n",
    "    # any potential predecessors of `input_tensor`.\n",
    "    if input_tensor is not None:\n",
    "        inputs = keras_utils.get_source_inputs(input_tensor)\n",
    "    else:\n",
    "        inputs = img_input\n",
    "    # Create model.\n",
    "    model = models.Model(inputs, x, name='vgg16')\n",
    "\n",
    "    # Load weights.\n",
    "    if weights == 'imagenet':\n",
    "        if include_top:\n",
    "            weights_path = keras_utils.get_file(\n",
    "                'vgg16_weights_tf_dim_ordering_tf_kernels.h5',\n",
    "                WEIGHTS_PATH,\n",
    "                cache_subdir='models',\n",
    "                file_hash='64373286793e3c8b2b4e3219cbf3544b')\n",
    "        else:\n",
    "            weights_path = keras_utils.get_file(\n",
    "                'vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5',\n",
    "                WEIGHTS_PATH_NO_TOP,\n",
    "                cache_subdir='models',\n",
    "                file_hash='6d6bbae143d832006294945121d1f1fc')\n",
    "        model.load_weights(weights_path)\n",
    "        if backend.backend() == 'theano':\n",
    "            keras_utils.convert_all_kernels_in_model(model)\n",
    "    elif weights is not None:\n",
    "        model.load_weights(weights)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0413 18:11:24.831305 140298365019904 deprecation_wrapper.py:119] From /home/abhijit/.conda/envs/tf_gpu/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0413 18:11:24.986248 140298365019904 deprecation_wrapper.py:119] From /home/abhijit/.conda/envs/tf_gpu/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 3, 224, 224)       0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 64, 224, 224)      1792      \n",
      "_________________________________________________________________\n",
      "lambda_1 (Lambda)            (None, 64, 224, 224)      0         \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 64, 224, 224)      36928     \n",
      "_________________________________________________________________\n",
      "lambda_2 (Lambda)            (None, 64, 224, 224)      0         \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 64, 112, 112)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 128, 112, 112)     73856     \n",
      "_________________________________________________________________\n",
      "lambda_3 (Lambda)            (None, 128, 112, 112)     0         \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 128, 112, 112)     147584    \n",
      "_________________________________________________________________\n",
      "lambda_4 (Lambda)            (None, 128, 112, 112)     0         \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 128, 56, 56)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 256, 56, 56)       295168    \n",
      "_________________________________________________________________\n",
      "lambda_5 (Lambda)            (None, 256, 56, 56)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 256, 56, 56)       590080    \n",
      "_________________________________________________________________\n",
      "lambda_6 (Lambda)            (None, 256, 56, 56)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 256, 56, 56)       590080    \n",
      "_________________________________________________________________\n",
      "lambda_7 (Lambda)            (None, 256, 56, 56)       0         \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 256, 28, 28)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 512, 28, 28)       1180160   \n",
      "_________________________________________________________________\n",
      "lambda_8 (Lambda)            (None, 512, 28, 28)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 512, 28, 28)       2359808   \n",
      "_________________________________________________________________\n",
      "lambda_9 (Lambda)            (None, 512, 28, 28)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 512, 28, 28)       2359808   \n",
      "_________________________________________________________________\n",
      "lambda_10 (Lambda)           (None, 512, 28, 28)       0         \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 512, 14, 14)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 512, 14, 14)       2359808   \n",
      "_________________________________________________________________\n",
      "lambda_11 (Lambda)           (None, 512, 14, 14)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 512, 14, 14)       2359808   \n",
      "_________________________________________________________________\n",
      "lambda_12 (Lambda)           (None, 512, 14, 14)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 512, 14, 14)       2359808   \n",
      "_________________________________________________________________\n",
      "lambda_13 (Lambda)           (None, 512, 14, 14)       0         \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 512, 7, 7)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "fc1 (Dense)                  (None, 4096)              102764544 \n",
      "_________________________________________________________________\n",
      "lambda_14 (Lambda)           (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "fc2 (Dense)                  (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "lambda_15 (Lambda)           (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "predictions (Dense)          (None, 1000)              4097000   \n",
      "=================================================================\n",
      "Total params: 138,357,544\n",
      "Trainable params: 138,357,544\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_name = 'VGG16'\n",
    "model = VGG16(include_top=True, weights='imagenet', input_tensor=None, input_shape=None, pooling=None, classes=1000)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "K.image_data_format()\n",
    "tf.keras.backend.clear_session()\n",
    "gc.collect()\n",
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#Test Cell 1\n",
    "\n",
    "data_path = '../../data/pkl/'\n",
    "classes = ['animate','inanimate']\n",
    "fold = 1\n",
    "\n",
    "with open(data_path+classes[0]+'_train_'+model_name+'.pkl','rb') as f:\n",
    "        X_fold = pickle.load(f)\n",
    "with open(data_path+classes[1]+'_train_'+model_name+'.pkl','rb') as f:\n",
    "        y_fold = pickle.load(f)\n",
    "    \n",
    "X = np.column_stack((X_fold,y_fold))\n",
    "X = np.float32(X)\n",
    "\n",
    "\n",
    "kmeans = MiniBatchKMeans(n_clusters=67783, #200x reduction in points\n",
    "                         max_iter=10).fit(X)\n",
    "\n",
    "pred_kmeans = kmeans.predict(X)\n",
    "\n",
    "X_new = kmeans.cluster_centers_\n",
    "\n",
    "with open('../../data/pkl/kmeans_first_train_'+model_name+'.pkl', 'wb') as handle:\n",
    "    pickle.dump([X_new,pred_kmeans,kmeans], handle, protocol=pickle.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%time\n",
    "#Version 1 - Reading pkl files from step 0 and clustering it{\n",
    "data_path = '../../data/pkl/'\n",
    "classes = ['animate','inanimate']\n",
    "\n",
    "\n",
    "result= {}\n",
    "\n",
    "k = 4 #Total Number of folds\n",
    "fold = 1\n",
    "\n",
    "for i in range(k):\n",
    "    \n",
    "    print('Perfoming Fold: ', fold)\n",
    "    clf_result = {}\n",
    "    \n",
    "    if os.path.exists('../../data/pkl/kmeans_first_'+str(fold)+'_'+model_name+'.pkl'):\n",
    "        with open('../../data/pkl/kmeans_first_'+str(fold)+'_'+model_name+'.pkl',\"rb\") as f:\n",
    "            X_new,pred_kmeans,kmeans = pickle.load(f)\n",
    "    else:   \n",
    "        with open(data_path+classes[0]+'_fold_'+str(fold)+'_train_'+model_name+'.pkl','rb') as f:\n",
    "            X_fold = pickle.load(f)\n",
    "        with open(data_path+classes[1]+'_fold_'+str(fold)+'_train_'+model_name+'.pkl','rb') as f:\n",
    "            y_fold = pickle.load(f)\n",
    "\n",
    "        X = np.column_stack((X_fold,y_fold))\n",
    "        kmeans = MiniBatchKMeans(n_clusters=67783,\n",
    "                                 max_iter=10).fit(X)\n",
    "        #print kmeans.cluster_centers_\n",
    "        pred_kmeans = kmeans.predict(X)\n",
    "        X_new = kmeans.cluster_centers_\n",
    "\n",
    "        with open('../../data/pkl/kmeans_first_'+str(fold)+'_'+model_name+'.pkl', 'wb') as handle:\n",
    "            pickle.dump([X_new,pred_kmeans,kmeans], handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    #DO CLUSTERING AND GET CLUSTERS\n",
    "    \n",
    "    from sklearn.cluster import KMeans\n",
    "\n",
    "    from sklearn.mixture import GaussianMixture\n",
    "    \n",
    "    method ='GMM'\n",
    "    print(method)\n",
    "    for j in range(1,31,1):\n",
    " \n",
    "        clf_result[j] = {}\n",
    "\n",
    "        clf = KMeans(n_clusters=j)    \n",
    "        clf = GaussianMixture(n_components=j, covariance_type='full')\n",
    "        y_pred = clf.fit_predict(X_new)\n",
    "        #print clf.cluster_centers_\n",
    "\n",
    "        for label in set(y_pred):\n",
    "            print('Cluster: ',j,'Label: ', label)\n",
    "            \n",
    "            #Lesioning and measuring performance\n",
    "            pred = y_pred.copy()\n",
    "            loc = np.where(pred==label)\n",
    "            loc_temp = kmeans.predict(X_new[loc[0]])\n",
    "            loc_new =[]\n",
    "            for entry in set(loc_temp):\n",
    "                temp = np.where(pred_kmeans==entry)[0]\n",
    "                loc_new.extend(temp)\n",
    "\n",
    "            lambda_mask = np.ones(shape=((13555712,)))\n",
    "            lambda_mask[loc_new] = 0.\n",
    "\n",
    "            #plt.scatter(X[:,0],X[:,1], c=y_pred) \n",
    "            #Change Model\n",
    "            model = VGG16(include_top=True, weights='imagenet', input_tensor=None, input_shape=None, pooling=None, classes=1000,lambda_mask=lambda_mask)\n",
    "\n",
    "            flag = 0\n",
    "            dprime = 0.\n",
    "            for p in classes:\n",
    "                im_valid_test = []\n",
    "                image_list_valid = '../../data/pkl/'+p+'_image_list_valid_fold_'+str(fold)+'.txt'\n",
    "                with open(image_list_valid,'r') as f:\n",
    "                    for line in f.readlines():\n",
    "                        im_valid_test.append(line.strip('\\n'))\n",
    "                im_temp = preprocess_image_batch(im_valid_test,img_size=(256,256), crop_size=(224,224), color_mode=\"rgb\")\n",
    "                out = model.predict(im_temp,batch_size=64)\n",
    "\n",
    "                true_valid_wids = []\n",
    "                for i in im_valid_test:\n",
    "                        temp1 = i.split('/')[4]\n",
    "                        temp = temp1.split('.')[0].split('_')[2]\n",
    "                        true_valid_wids.append(truth[int(temp)][1])\n",
    "\n",
    "                predicted_valid_wids = []\n",
    "                for i in range(len(im_valid_test)):\n",
    "                    #print im_list[i], pprint_output(out[i]), true_wids[i]\n",
    "                    predicted_valid_wids.append(pprint_output(out[i]))\n",
    "\n",
    "                count, error  = top5accuracy(true_valid_wids, predicted_valid_wids)\n",
    "\n",
    "                print(str(p)+' '+str(fold)+' '+str(count)+' '+str(len(im_valid_test))+' '+str(error))\n",
    "                \n",
    "                if flag == 0:\n",
    "                    dprime = error\n",
    "                    flag = 1\n",
    "                else:\n",
    "                    dprime -= error\n",
    "                    \n",
    "            tf.keras.backend.clear_session()\n",
    "            gc.collect()\n",
    "            del model\n",
    "            clf_result[j][label] = dprime\n",
    "    \n",
    "    with open('../../data/pkl/'+str(method)+'_30_scree_fold_'+str(fold)+'_'+model_name+'.pkl', 'wb') as handle:\n",
    "        pickle.dump(clf_result, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "    result[fold] = clf_result\n",
    "    fold += 1\n",
    "#}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading the pickle files\n",
    "method ='GMM'\n",
    "\n",
    "k = 4\n",
    "result ={}\n",
    "for i in range(1,k+1,1):\n",
    "    name = '../../data/pkl/'+str(method)+'_30_scree_fold_'+str(i)+'_VGG16.pkl'   #CHANGE\n",
    "    with open(name,\"rb\") as f:\n",
    "        result[i] = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: {0: 0.0},\n",
       " 2: {0: 0.0, 1: 0.1282051282051282},\n",
       " 3: {0: 0.0, 1: 0.0, 2: 0.15384615384615385},\n",
       " 4: {0: 0.0, 1: 0.0, 2: 0.33333333333333337, 3: -0.05128205128205121},\n",
       " 5: {0: 0.0,\n",
       "  1: 0.23076923076923073,\n",
       "  2: -0.10256410256410253,\n",
       "  3: 0.10256410256410253,\n",
       "  4: 0.0},\n",
       " 6: {0: 0.0,\n",
       "  1: -0.41025641025641024,\n",
       "  2: -0.10256410256410253,\n",
       "  3: 0.0,\n",
       "  4: 0.2564102564102564,\n",
       "  5: 0.46153846153846156},\n",
       " 7: {0: 0.0,\n",
       "  1: 0.23076923076923084,\n",
       "  2: 0.3846153846153846,\n",
       "  3: 0.0,\n",
       "  4: -0.5384615384615384,\n",
       "  5: 0.0,\n",
       "  6: -0.05128205128205121},\n",
       " 8: {0: 0.5641025641025641,\n",
       "  1: 0.0,\n",
       "  2: 0.6153846153846153,\n",
       "  3: 0.15384615384615397,\n",
       "  4: -0.05128205128205121,\n",
       "  5: 0.0,\n",
       "  6: -0.46153846153846156,\n",
       "  7: -0.10256410256410253},\n",
       " 9: {0: 0.0,\n",
       "  1: -0.46153846153846156,\n",
       "  2: 0.23076923076923084,\n",
       "  3: -0.02564102564102566,\n",
       "  4: -0.05128205128205121,\n",
       "  5: 0.7435897435897436,\n",
       "  6: 0.5897435897435898,\n",
       "  7: -0.3846153846153847,\n",
       "  8: 0.0},\n",
       " 10: {0: -0.4871794871794872,\n",
       "  1: 0.0,\n",
       "  2: 0.20512820512820507,\n",
       "  3: 0.7435897435897436,\n",
       "  4: 0.6153846153846154,\n",
       "  5: -0.07692307692307687,\n",
       "  6: -0.07692307692307687,\n",
       "  7: -0.3846153846153847,\n",
       "  8: 0.0,\n",
       "  9: -0.05128205128205121},\n",
       " 11: {0: 0.02564102564102566,\n",
       "  1: 0.6153846153846154,\n",
       "  2: 0.0,\n",
       "  3: -0.07692307692307687,\n",
       "  4: -0.5128205128205128,\n",
       "  5: -0.07692307692307687,\n",
       "  6: -0.23076923076923084,\n",
       "  7: 0.02564102564102555,\n",
       "  8: 0.6666666666666666,\n",
       "  9: -0.07692307692307687,\n",
       "  10: -0.6666666666666666},\n",
       " 12: {0: 0.0,\n",
       "  1: -0.23076923076923073,\n",
       "  2: 0.6923076923076923,\n",
       "  3: -0.07692307692307687,\n",
       "  4: -0.07692307692307687,\n",
       "  5: 0.15384615384615374,\n",
       "  6: -0.3589743589743589,\n",
       "  7: 0.05128205128205121,\n",
       "  8: -0.07692307692307687,\n",
       "  9: -0.6410256410256411,\n",
       "  10: 0.0,\n",
       "  11: 0.4358974358974359},\n",
       " 13: {0: 0.05128205128205132,\n",
       "  1: 0.0,\n",
       "  2: 0.5128205128205128,\n",
       "  3: -0.02564102564102566,\n",
       "  4: -0.05128205128205121,\n",
       "  5: -0.4871794871794872,\n",
       "  6: -0.6923076923076923,\n",
       "  7: -0.10256410256410253,\n",
       "  8: 0.02564102564102566,\n",
       "  9: 0.5384615384615384,\n",
       "  10: -0.20512820512820518,\n",
       "  11: 0.23076923076923084,\n",
       "  12: 0.15384615384615385},\n",
       " 14: {0: 0.7692307692307693,\n",
       "  1: 0.1282051282051282,\n",
       "  2: 0.0,\n",
       "  3: 0.07692307692307687,\n",
       "  4: -0.05128205128205121,\n",
       "  5: 0.1794871794871794,\n",
       "  6: 0.3076923076923077,\n",
       "  7: -0.20512820512820507,\n",
       "  8: -0.3076923076923077,\n",
       "  9: -0.07692307692307687,\n",
       "  10: 0.4871794871794871,\n",
       "  11: 0.05128205128205121,\n",
       "  12: -0.1282051282051282,\n",
       "  13: -0.4871794871794871},\n",
       " 15: {0: 0.02564102564102566,\n",
       "  1: 0.15384615384615385,\n",
       "  2: 0.0,\n",
       "  3: 0.0,\n",
       "  4: -0.05128205128205121,\n",
       "  5: -0.1794871794871794,\n",
       "  6: 0.5128205128205128,\n",
       "  7: 0.05128205128205121,\n",
       "  8: 0.0,\n",
       "  9: -0.5128205128205129,\n",
       "  10: 0.0,\n",
       "  11: 0.3076923076923076,\n",
       "  12: 0.46153846153846145,\n",
       "  13: -0.10256410256410253,\n",
       "  14: -0.6666666666666667},\n",
       " 16: {0: 0.17948717948717952,\n",
       "  1: 0.05128205128205132,\n",
       "  2: 0.0,\n",
       "  3: -0.07692307692307687,\n",
       "  4: 0.05128205128205132,\n",
       "  5: -0.6410256410256411,\n",
       "  6: 0.4871794871794871,\n",
       "  7: 0.07692307692307687,\n",
       "  8: 0.4871794871794872,\n",
       "  9: -0.10256410256410253,\n",
       "  10: -0.2564102564102564,\n",
       "  11: -0.07692307692307687,\n",
       "  12: -0.1282051282051282,\n",
       "  13: 0.07692307692307687,\n",
       "  14: -0.46153846153846156,\n",
       "  15: 0.2564102564102564},\n",
       " 17: {0: -0.10256410256410253,\n",
       "  1: 0.0,\n",
       "  2: 0.33333333333333337,\n",
       "  3: 0.0,\n",
       "  4: -0.05128205128205121,\n",
       "  5: -0.23076923076923073,\n",
       "  6: 0.02564102564102555,\n",
       "  7: 0.1282051282051282,\n",
       "  8: 0.3076923076923076,\n",
       "  9: 0.0,\n",
       "  10: 0.05128205128205132,\n",
       "  11: 0.46153846153846156,\n",
       "  12: 0.05128205128205132,\n",
       "  13: -0.46153846153846156,\n",
       "  14: -0.10256410256410253,\n",
       "  15: 0.05128205128205121,\n",
       "  16: -0.41025641025641024},\n",
       " 18: {0: 0.0,\n",
       "  1: 0.0,\n",
       "  2: -0.28205128205128205,\n",
       "  3: -0.02564102564102566,\n",
       "  4: 0.15384615384615385,\n",
       "  5: -0.07692307692307687,\n",
       "  6: 0.7435897435897436,\n",
       "  7: -0.1282051282051282,\n",
       "  8: 0.33333333333333337,\n",
       "  9: -0.6153846153846154,\n",
       "  10: -0.02564102564102566,\n",
       "  11: -0.10256410256410264,\n",
       "  12: -0.15384615384615385,\n",
       "  13: 0.1794871794871794,\n",
       "  14: 0.3846153846153847,\n",
       "  15: -0.07692307692307687,\n",
       "  16: 0.0,\n",
       "  17: -0.5128205128205128},\n",
       " 19: {0: -0.435897435897436,\n",
       "  1: 0.02564102564102566,\n",
       "  2: 0.07692307692307687,\n",
       "  3: -0.07692307692307687,\n",
       "  4: 0.07692307692307698,\n",
       "  5: -0.17948717948717952,\n",
       "  6: -0.46153846153846156,\n",
       "  7: -0.10256410256410253,\n",
       "  8: -0.23076923076923073,\n",
       "  9: 0.0,\n",
       "  10: 0.3076923076923077,\n",
       "  11: 0.28205128205128205,\n",
       "  12: -0.07692307692307687,\n",
       "  13: 0.46153846153846156,\n",
       "  14: -0.07692307692307698,\n",
       "  15: 0.1282051282051282,\n",
       "  16: -0.07692307692307687,\n",
       "  17: -0.02564102564102566,\n",
       "  18: 0.02564102564102566},\n",
       " 20: {0: 0.0,\n",
       "  1: -0.20512820512820507,\n",
       "  2: 0.3076923076923077,\n",
       "  3: -0.07692307692307687,\n",
       "  4: -0.02564102564102566,\n",
       "  5: 0.07692307692307698,\n",
       "  6: 0.1282051282051282,\n",
       "  7: -0.10256410256410253,\n",
       "  8: 0.15384615384615374,\n",
       "  9: -0.02564102564102566,\n",
       "  10: 0.28205128205128205,\n",
       "  11: -0.15384615384615385,\n",
       "  12: 0.7692307692307693,\n",
       "  13: -0.05128205128205132,\n",
       "  14: -0.46153846153846156,\n",
       "  15: 0.3076923076923076,\n",
       "  16: -0.07692307692307687,\n",
       "  17: -0.5641025641025641,\n",
       "  18: -0.20512820512820507,\n",
       "  19: -0.10256410256410253},\n",
       " 21: {0: 0.20512820512820507,\n",
       "  1: -0.6923076923076923,\n",
       "  2: -0.07692307692307687,\n",
       "  3: -0.4871794871794872,\n",
       "  4: -0.15384615384615385,\n",
       "  5: -0.07692307692307687,\n",
       "  6: 0.0,\n",
       "  7: 0.28205128205128205,\n",
       "  8: -0.02564102564102555,\n",
       "  9: -0.05128205128205121,\n",
       "  10: -0.05128205128205121,\n",
       "  11: -0.07692307692307687,\n",
       "  12: -0.23076923076923073,\n",
       "  13: -0.17948717948717952,\n",
       "  14: 0.02564102564102555,\n",
       "  15: 0.7435897435897436,\n",
       "  16: 0.07692307692307687,\n",
       "  17: 0.23076923076923073,\n",
       "  18: 0.15384615384615374,\n",
       "  19: -0.02564102564102566,\n",
       "  20: -0.15384615384615385},\n",
       " 22: {0: 0.02564102564102566,\n",
       "  1: -0.5128205128205128,\n",
       "  2: -0.1282051282051282,\n",
       "  3: 0.0,\n",
       "  4: -0.07692307692307687,\n",
       "  5: 0.02564102564102566,\n",
       "  6: 0.02564102564102566,\n",
       "  7: -0.10256410256410253,\n",
       "  8: -0.6923076923076923,\n",
       "  9: -0.15384615384615385,\n",
       "  10: -0.07692307692307698,\n",
       "  11: 0.3076923076923077,\n",
       "  12: -0.07692307692307687,\n",
       "  13: -0.02564102564102555,\n",
       "  14: 0.6923076923076923,\n",
       "  15: -0.07692307692307687,\n",
       "  16: -0.23076923076923073,\n",
       "  17: 0.0,\n",
       "  18: 0.1794871794871794,\n",
       "  19: 0.0,\n",
       "  20: 0.20512820512820507,\n",
       "  21: 0.3076923076923077},\n",
       " 23: {0: -0.6153846153846154,\n",
       "  1: 0.05128205128205132,\n",
       "  2: 0.0,\n",
       "  3: -0.07692307692307687,\n",
       "  4: -0.10256410256410253,\n",
       "  5: -0.20512820512820507,\n",
       "  6: 0.15384615384615374,\n",
       "  7: -0.02564102564102566,\n",
       "  8: -0.10256410256410253,\n",
       "  9: 0.3846153846153846,\n",
       "  10: -0.07692307692307687,\n",
       "  11: 0.15384615384615385,\n",
       "  12: 0.02564102564102555,\n",
       "  13: 0.1282051282051282,\n",
       "  14: 0.05128205128205121,\n",
       "  15: -0.1794871794871794,\n",
       "  16: -0.02564102564102555,\n",
       "  17: -0.3589743589743589,\n",
       "  18: -0.07692307692307687,\n",
       "  19: 0.23076923076923073,\n",
       "  20: -0.10256410256410253,\n",
       "  21: -0.10256410256410253,\n",
       "  22: 0.02564102564102566},\n",
       " 24: {0: 0.07692307692307698,\n",
       "  1: 0.07692307692307687,\n",
       "  2: -0.07692307692307687,\n",
       "  3: -0.07692307692307687,\n",
       "  4: -0.23076923076923073,\n",
       "  5: 0.23076923076923073,\n",
       "  6: -0.05128205128205132,\n",
       "  7: -0.15384615384615385,\n",
       "  8: 0.0,\n",
       "  9: -0.07692307692307687,\n",
       "  10: -0.10256410256410253,\n",
       "  11: 0.3076923076923077,\n",
       "  12: -0.10256410256410253,\n",
       "  13: 0.05128205128205121,\n",
       "  14: -0.05128205128205121,\n",
       "  15: -0.02564102564102566,\n",
       "  16: 0.1794871794871794,\n",
       "  17: -0.5384615384615384,\n",
       "  18: 0.28205128205128205,\n",
       "  19: -0.05128205128205132,\n",
       "  20: -0.07692307692307687,\n",
       "  21: -0.05128205128205121,\n",
       "  22: 0.33333333333333337,\n",
       "  23: -0.3076923076923076},\n",
       " 25: {0: 0.1794871794871794,\n",
       "  1: -0.02564102564102555,\n",
       "  2: 0.0,\n",
       "  3: -0.07692307692307687,\n",
       "  4: 0.17948717948717952,\n",
       "  5: 0.3076923076923076,\n",
       "  6: -0.33333333333333326,\n",
       "  7: 0.0,\n",
       "  8: 0.02564102564102555,\n",
       "  9: 0.0,\n",
       "  10: -0.20512820512820507,\n",
       "  11: -0.10256410256410253,\n",
       "  12: -0.15384615384615385,\n",
       "  13: -0.02564102564102566,\n",
       "  14: -0.07692307692307687,\n",
       "  15: -0.07692307692307687,\n",
       "  16: 0.1282051282051282,\n",
       "  17: -0.07692307692307687,\n",
       "  18: 0.1282051282051282,\n",
       "  19: 0.641025641025641,\n",
       "  20: -0.10256410256410253,\n",
       "  21: 0.02564102564102555,\n",
       "  22: -0.1282051282051282,\n",
       "  23: -0.20512820512820507,\n",
       "  24: -0.4871794871794871},\n",
       " 26: {0: -0.05128205128205132,\n",
       "  1: 0.05128205128205132,\n",
       "  2: 0.0,\n",
       "  3: 0.1282051282051282,\n",
       "  4: -0.07692307692307687,\n",
       "  5: -0.02564102564102566,\n",
       "  6: 0.1282051282051282,\n",
       "  7: -0.10256410256410253,\n",
       "  8: -0.20512820512820507,\n",
       "  9: -0.07692307692307687,\n",
       "  10: -0.23076923076923073,\n",
       "  11: -0.1282051282051282,\n",
       "  12: -0.02564102564102555,\n",
       "  13: -0.4871794871794872,\n",
       "  14: -0.1282051282051282,\n",
       "  15: 0.02564102564102566,\n",
       "  16: 0.10256410256410264,\n",
       "  17: 0.15384615384615374,\n",
       "  18: -0.10256410256410253,\n",
       "  19: 0.1282051282051282,\n",
       "  20: 0.3076923076923077,\n",
       "  21: 0.0,\n",
       "  22: 0.0,\n",
       "  23: -0.20512820512820507,\n",
       "  24: -0.05128205128205121,\n",
       "  25: -0.02564102564102566},\n",
       " 27: {0: 0.07692307692307698,\n",
       "  1: -0.23076923076923073,\n",
       "  2: -0.02564102564102555,\n",
       "  3: 0.15384615384615385,\n",
       "  4: -0.07692307692307687,\n",
       "  5: 0.0,\n",
       "  6: -0.10256410256410253,\n",
       "  7: -0.07692307692307687,\n",
       "  8: 0.2564102564102564,\n",
       "  9: -0.07692307692307687,\n",
       "  10: 0.3076923076923076,\n",
       "  11: -0.07692307692307687,\n",
       "  12: -0.07692307692307687,\n",
       "  13: -0.07692307692307698,\n",
       "  14: -0.07692307692307687,\n",
       "  15: 0.07692307692307698,\n",
       "  16: 0.15384615384615385,\n",
       "  17: -0.05128205128205121,\n",
       "  18: -0.15384615384615385,\n",
       "  19: 0.05128205128205132,\n",
       "  20: -0.4871794871794872,\n",
       "  21: -0.05128205128205132,\n",
       "  22: -0.07692307692307687,\n",
       "  23: 0.15384615384615374,\n",
       "  24: -0.10256410256410253,\n",
       "  25: -0.28205128205128205,\n",
       "  26: -0.02564102564102566},\n",
       " 28: {0: -0.1794871794871794,\n",
       "  1: 0.05128205128205121,\n",
       "  2: -0.02564102564102566,\n",
       "  3: 0.15384615384615374,\n",
       "  4: -0.07692307692307687,\n",
       "  5: 0.02564102564102566,\n",
       "  6: 0.1282051282051282,\n",
       "  7: 0.10256410256410264,\n",
       "  8: -0.10256410256410253,\n",
       "  9: -0.07692307692307687,\n",
       "  10: 0.02564102564102566,\n",
       "  11: 0.0,\n",
       "  12: -0.07692307692307687,\n",
       "  13: -0.07692307692307687,\n",
       "  14: -0.07692307692307687,\n",
       "  15: -0.4358974358974359,\n",
       "  16: -0.02564102564102566,\n",
       "  17: -0.20512820512820507,\n",
       "  18: -0.10256410256410253,\n",
       "  19: -0.02564102564102555,\n",
       "  20: -0.1794871794871794,\n",
       "  21: -0.20512820512820507,\n",
       "  22: 0.0,\n",
       "  23: 0.07692307692307698,\n",
       "  24: -0.10256410256410253,\n",
       "  25: 0.0,\n",
       "  26: 0.15384615384615385,\n",
       "  27: 0.28205128205128205},\n",
       " 29: {0: -0.3589743589743589,\n",
       "  1: 0.0,\n",
       "  2: 0.15384615384615385,\n",
       "  3: -0.07692307692307687,\n",
       "  4: 0.0,\n",
       "  5: -0.07692307692307687,\n",
       "  6: -0.1794871794871794,\n",
       "  7: 0.0,\n",
       "  8: -0.1282051282051282,\n",
       "  9: -0.07692307692307687,\n",
       "  10: -0.07692307692307698,\n",
       "  11: -0.02564102564102555,\n",
       "  12: 0.1282051282051282,\n",
       "  13: -0.02564102564102555,\n",
       "  14: -0.07692307692307687,\n",
       "  15: 0.3076923076923076,\n",
       "  16: -0.05128205128205121,\n",
       "  17: -0.20512820512820507,\n",
       "  18: 0.02564102564102566,\n",
       "  19: -0.02564102564102566,\n",
       "  20: 0.15384615384615374,\n",
       "  21: -0.10256410256410253,\n",
       "  22: 0.02564102564102566,\n",
       "  23: -0.02564102564102566,\n",
       "  24: 0.07692307692307687,\n",
       "  25: 0.05128205128205121,\n",
       "  26: -0.10256410256410253,\n",
       "  27: 0.0,\n",
       "  28: -0.20512820512820507},\n",
       " 30: {0: 0.0,\n",
       "  1: 0.0,\n",
       "  2: 0.15384615384615385,\n",
       "  3: -0.07692307692307687,\n",
       "  4: -0.10256410256410253,\n",
       "  5: 0.05128205128205132,\n",
       "  6: -0.07692307692307687,\n",
       "  7: -0.10256410256410253,\n",
       "  8: -0.05128205128205132,\n",
       "  9: 0.1282051282051282,\n",
       "  10: 0.1794871794871794,\n",
       "  11: 0.5897435897435898,\n",
       "  12: -0.02564102564102555,\n",
       "  13: 0.1282051282051282,\n",
       "  14: 0.0,\n",
       "  15: -0.07692307692307687,\n",
       "  16: 0.15384615384615385,\n",
       "  17: -0.07692307692307698,\n",
       "  18: -0.05128205128205121,\n",
       "  19: -0.10256410256410253,\n",
       "  20: -0.05128205128205121,\n",
       "  21: -0.1794871794871794,\n",
       "  22: -0.1794871794871794,\n",
       "  23: 0.0,\n",
       "  24: 0.05128205128205121,\n",
       "  25: -0.1282051282051282,\n",
       "  26: -0.07692307692307687,\n",
       "  27: -0.15384615384615385,\n",
       "  28: -0.5128205128205128,\n",
       "  29: -0.2564102564102564}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../../results/GMM_results_fold_1.eps'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-8e03750a8fe7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Performance Impact(Animate vs Inanimate) '\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgca\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxaxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_major_locator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMaxNLocator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprune\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'lower'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msavefig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../../results/'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'_results_fold_'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'.eps'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'eps'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/lesion/lib/python3.7/site-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36msavefig\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    727\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msavefig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    728\u001b[0m     \u001b[0mfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgcf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 729\u001b[0;31m     \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msavefig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    730\u001b[0m     \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw_idle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m# need this if 'transparent=True' to reset colors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    731\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/lesion/lib/python3.7/site-packages/matplotlib/figure.py\u001b[0m in \u001b[0;36msavefig\u001b[0;34m(self, fname, transparent, **kwargs)\u001b[0m\n\u001b[1;32m   2178\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_visible\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframeon\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2180\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_figure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2182\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mframeon\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/lesion/lib/python3.7/site-packages/matplotlib/backend_bases.py\u001b[0m in \u001b[0;36mprint_figure\u001b[0;34m(self, filename, dpi, facecolor, edgecolor, orientation, format, bbox_inches, **kwargs)\u001b[0m\n\u001b[1;32m   2089\u001b[0m                     \u001b[0morientation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morientation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2090\u001b[0m                     \u001b[0mbbox_inches_restore\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_bbox_inches_restore\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2091\u001b[0;31m                     **kwargs)\n\u001b[0m\u001b[1;32m   2092\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2093\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mbbox_inches\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mrestore_bbox\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/lesion/lib/python3.7/site-packages/matplotlib/backends/backend_ps.py\u001b[0m in \u001b[0;36mprint_eps\u001b[0;34m(self, outfile, *args, **kwargs)\u001b[0m\n\u001b[1;32m    840\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    841\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mprint_eps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 842\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_print_ps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'eps'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    843\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    844\u001b[0m     def _print_ps(self, outfile, format, *args,\n",
      "\u001b[0;32m~/anaconda3/envs/lesion/lib/python3.7/site-packages/matplotlib/backends/backend_ps.py\u001b[0m in \u001b[0;36m_print_ps\u001b[0;34m(self, outfile, format, papertype, dpi, facecolor, edgecolor, orientation, *args, **kwargs)\u001b[0m\n\u001b[1;32m    869\u001b[0m             self._print_figure(outfile, format, dpi, facecolor, edgecolor,\n\u001b[1;32m    870\u001b[0m                                \u001b[0morientation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0misLandscape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpapertype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 871\u001b[0;31m                                **kwargs)\n\u001b[0m\u001b[1;32m    872\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    873\u001b[0m     def _print_figure(\n",
      "\u001b[0;32m~/anaconda3/envs/lesion/lib/python3.7/site-packages/matplotlib/backends/backend_ps.py\u001b[0m in \u001b[0;36m_print_figure\u001b[0;34m(self, outfile, format, dpi, facecolor, edgecolor, orientation, isLandscape, papertype, metadata, dryrun, bbox_inches_restore, **kwargs)\u001b[0m\n\u001b[1;32m   1104\u001b[0m                 \u001b[0mprint_figure_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1105\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1106\u001b[0;31m                 \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'latin-1'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfh\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1107\u001b[0m                     \u001b[0mprint_figure_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../../results/GMM_results_fold_1.eps'"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib.ticker import MaxNLocator\n",
    "f = 1\n",
    "clf_result = result[f]\n",
    "\n",
    "\n",
    "fig = plt.figure(1)\n",
    "X = range(1,31,1)\n",
    "#X = range(2,51,1)\n",
    "for cl in X:\n",
    "    i = 0\n",
    "    for item in clf_result[cl].keys():\n",
    "        plt.plot(cl,clf_result[cl][item],'ro')\n",
    "        i += 1\n",
    "        \n",
    "plt.xticks(X)\n",
    "plt.xlabel('Number of cluster(s) k')\n",
    "plt.ylabel(\"Performance Impact\")\n",
    "plt.title('Performance Impact(Animate vs Inanimate) '+ str(f))\n",
    "plt.gca().xaxis.set_major_locator(MaxNLocator(prune='lower'))\n",
    "plt.savefig('../../results/'+str(method)+'_results_fold_'+str(f)+'.eps', format='eps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = 2\n",
    "clf_result = result[f]\n",
    "\n",
    "\n",
    "fig = plt.figure(1)\n",
    "X = range(1,51,1)\n",
    "#X = range(2,51,1)\n",
    "for cl in X:\n",
    "    i = 0\n",
    "    for item in clf_result[cl].keys():\n",
    "        plt.plot(cl,clf_result[cl][item],'ro')\n",
    "        i += 1\n",
    "        \n",
    "plt.xticks(X)\n",
    "plt.xlabel('Number of cluster(s) k')\n",
    "plt.ylabel(\"Performance Impact(Animate vs Inanimate)\")\n",
    "plt.title('Scree Plot for fold '+ str(f))\n",
    "plt.gca().xaxis.set_major_locator(MaxNLocator(prune='lower'))\n",
    "#plt.savefig('../../results/scree/'+str(method)+'_results_fold_'+str(f)+'.png', format='png', dpi=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = 3\n",
    "clf_result = result[f]\n",
    "\n",
    "\n",
    "fig = plt.figure(1)\n",
    "X = range(1,51,1)\n",
    "#X = range(2,51,1)\n",
    "for cl in X:\n",
    "    i = 0\n",
    "    for item in clf_result[cl].keys():\n",
    "        plt.plot(cl,clf_result[cl][item],'ro')\n",
    "        i += 1\n",
    "        \n",
    "plt.xticks(X)\n",
    "plt.xlabel('Number of cluster(s) k')\n",
    "plt.ylabel(\"Performance Impact(Animate vs Inanimate)\")\n",
    "plt.title('Scree Plot for fold '+ str(f))\n",
    "plt.gca().xaxis.set_major_locator(MaxNLocator(prune='lower'))\n",
    "#plt.savefig('../../results/scree/'+str(method)+'_results_fold_'+str(f)+'.png', format='png', dpi=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = 4\n",
    "clf_result = result[f]\n",
    "\n",
    "\n",
    "fig = plt.figure(1)\n",
    "X = range(1,51,1)\n",
    "#X = range(2,51,1)\n",
    "for cl in X:\n",
    "    i = 0\n",
    "    for item in clf_result[cl].keys():\n",
    "        plt.plot(cl,clf_result[cl][item],'ro')\n",
    "        i += 1\n",
    "        \n",
    "plt.xticks(X)\n",
    "plt.xlabel('Number of cluster(s) k')\n",
    "plt.ylabel(\"Performance Impact(Animate vs Inanimate)\")\n",
    "plt.title('Scree Plot for fold '+ str(f))\n",
    "plt.gca().xaxis.set_major_locator(MaxNLocator(prune='lower'))\n",
    "#plt.savefig('../../results/scree/'+str(method)+'_results_fold_'+str(f)+'.png', format='png', dpi=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find MaxAd', MaxId' and its average\n",
    "plt.figure()\n",
    "noc = 4\n",
    "for i in range(1,noc+1,1):\n",
    "    X = []\n",
    "    Y = []\n",
    "    for j in range(2,51,1):\n",
    "        X.append(j)\n",
    "        temp = []\n",
    "        for key, value in result[i][j].items():\n",
    "            temp.append(value)\n",
    "        maxa = max(temp)\n",
    "        maxi = min(temp)\n",
    "        avg = float(maxa - maxi)\n",
    "        Y.append(avg)\n",
    "    #print X,Y\n",
    "    plt.plot(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Smooth average graph\n",
    "from scipy.interpolate import spline\n",
    "noc = 4\n",
    "flag = 0\n",
    "X = range(2,51,1)\n",
    "an_fold =[]\n",
    "ian_fold = []\n",
    "Y = []\n",
    "for i in range(1,noc+1,1):\n",
    "    if i == 2:\n",
    "        flag = 1\n",
    "    for j in range(2,51,1):\n",
    "        temp = []\n",
    "        for key, value in result[i][j].items():\n",
    "            temp.append(value)\n",
    "        maxa = max(temp)\n",
    "        maxi = min(temp)\n",
    "        if flag == 0:\n",
    "            an_fold.append(maxa)\n",
    "            ian_fold.append(maxi)\n",
    "        else:\n",
    "            an_fold[j-2] += maxa\n",
    "            ian_fold[j-2] = maxi\n",
    "\n",
    "for j in range(2,51,1):\n",
    "    maxa = (an_fold[j-2]) / 4.\n",
    "    maxi = (ian_fold[j-2]) /4.\n",
    "    diff = maxa - maxi\n",
    "    Y.append(diff)\n",
    "    \n",
    "x_sm = np.array(X)\n",
    "y_sm = np.array(Y)\n",
    "\n",
    "x_smooth = np.linspace(x_sm.min(), x_sm.max(), 200)\n",
    "y_smooth = spline(X, Y, x_smooth)\n",
    "\n",
    "plt.plot(x_smooth, y_smooth, 'r', linewidth=1)\n",
    "plt.plot(Y.index(max(Y))+1,max(Y),'o')\n",
    "plt.xlabel('Number of cluster(s) k')\n",
    "plt.ylabel(\"Average Performance\")\n",
    "plt.savefig('../../results/scree/'+str(method)+'_results_fold_avg.png', format='png', dpi=200)\n",
    "print(max(Y), Y.index(max(Y)) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
