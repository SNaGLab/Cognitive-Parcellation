{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#To hide warnings export PYTHONWARNINGS=\"ignore\"\n",
    "#Imports{\n",
    "\n",
    "import os\n",
    "from os.path import dirname\n",
    "from os.path import join\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' #Cha\n",
    "\n",
    "\n",
    "import sys\n",
    "import warnings\n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    \n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from imageio import imread\n",
    "from skimage.transform import resize\n",
    "from scipy.io import loadmat\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "#K.set_image_dim_ordering('tf')\n",
    "from keras_applications import imagenet_utils as utils\n",
    "from keras_applications import correct_pad\n",
    "from keras.layers import Lambda\n",
    "\n",
    "import PIL.Image\n",
    "\n",
    "#}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = 'pkl_mobile_4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "global start_index, end_index\n",
    "start_index = end_index = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code snippet needed to read activation values from each layer of the pre-trained artificial neural networks\n",
    "def get_activations(model, layer, X_batch):\n",
    "    #keras.backend.function(inputs, outputs, updates=None)\n",
    "    get_activations = keras.backend.function([model.layers[0].input, keras.backend.learning_phase()], [model.layers[layer].output,])\n",
    "    #The learning phase flag is a bool tensor (0 = test, 1 = train)\n",
    "    activations = get_activations([X_batch,0])\n",
    "    return activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'channels_last'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "backend= keras.backend\n",
    "layers = keras.layers\n",
    "models = keras.models\n",
    "keras_utils = keras.utils\n",
    "keras.backend.image_data_format()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"MobileNet v2 models for Keras.\n",
    "\n",
    "MobileNetV2 is a general architecture and can be used for multiple use cases.\n",
    "Depending on the use case, it can use different input layer size and\n",
    "different width factors. This allows different width models to reduce\n",
    "the number of multiply-adds and thereby\n",
    "reduce inference cost on mobile devices.\n",
    "\n",
    "MobileNetV2 is very similar to the original MobileNet,\n",
    "except that it uses inverted residual blocks with\n",
    "bottlenecking features. It has a drastically lower\n",
    "parameter count than the original MobileNet.\n",
    "MobileNets support any input size greater\n",
    "than 32 x 32, with larger image sizes\n",
    "offering better performance.\n",
    "\n",
    "The number of parameters and number of multiply-adds\n",
    "can be modified by using the `alpha` parameter,\n",
    "which increases/decreases the number of filters in each layer.\n",
    "By altering the image size and `alpha` parameter,\n",
    "all 22 models from the paper can be built, with ImageNet weights provided.\n",
    "\n",
    "The paper demonstrates the performance of MobileNets using `alpha` values of\n",
    "1.0 (also called 100 % MobileNet), 0.35, 0.5, 0.75, 1.0, 1.3, and 1.4\n",
    "\n",
    "For each of these `alpha` values, weights for 5 different input image sizes\n",
    "are provided (224, 192, 160, 128, and 96).\n",
    "\n",
    "\n",
    "The following table describes the performance of\n",
    "MobileNet on various input sizes:\n",
    "------------------------------------------------------------------------\n",
    "MACs stands for Multiply Adds\n",
    "\n",
    " Classification Checkpoint| MACs (M) | Parameters (M)| Top 1 Accuracy| Top 5 Accuracy\n",
    "--------------------------|------------|---------------|---------|----|-------------\n",
    "| [mobilenet_v2_1.4_224]  | 582 | 6.06 |          75.0 | 92.5 |\n",
    "| [mobilenet_v2_1.3_224]  | 509 | 5.34 |          74.4 | 92.1 |\n",
    "| [mobilenet_v2_1.0_224]  | 300 | 3.47 |          71.8 | 91.0 |\n",
    "| [mobilenet_v2_1.0_192]  | 221 | 3.47 |          70.7 | 90.1 |\n",
    "| [mobilenet_v2_1.0_160]  | 154 | 3.47 |          68.8 | 89.0 |\n",
    "| [mobilenet_v2_1.0_128]  | 99  | 3.47 |          65.3 | 86.9 |\n",
    "| [mobilenet_v2_1.0_96]   | 56  | 3.47 |          60.3 | 83.2 |\n",
    "| [mobilenet_v2_0.75_224] | 209 | 2.61 |          69.8 | 89.6 |\n",
    "| [mobilenet_v2_0.75_192] | 153 | 2.61 |          68.7 | 88.9 |\n",
    "| [mobilenet_v2_0.75_160] | 107 | 2.61 |          66.4 | 87.3 |\n",
    "| [mobilenet_v2_0.75_128] | 69  | 2.61 |          63.2 | 85.3 |\n",
    "| [mobilenet_v2_0.75_96]  | 39  | 2.61 |          58.8 | 81.6 |\n",
    "| [mobilenet_v2_0.5_224]  | 97  | 1.95 |          65.4 | 86.4 |\n",
    "| [mobilenet_v2_0.5_192]  | 71  | 1.95 |          63.9 | 85.4 |\n",
    "| [mobilenet_v2_0.5_160]  | 50  | 1.95 |          61.0 | 83.2 |\n",
    "| [mobilenet_v2_0.5_128]  | 32  | 1.95 |          57.7 | 80.8 |\n",
    "| [mobilenet_v2_0.5_96]   | 18  | 1.95 |          51.2 | 75.8 |\n",
    "| [mobilenet_v2_0.35_224] | 59  | 1.66 |          60.3 | 82.9 |\n",
    "| [mobilenet_v2_0.35_192] | 43  | 1.66 |          58.2 | 81.2 |\n",
    "| [mobilenet_v2_0.35_160] | 30  | 1.66 |          55.7 | 79.1 |\n",
    "| [mobilenet_v2_0.35_128] | 20  | 1.66 |          50.8 | 75.0 |\n",
    "| [mobilenet_v2_0.35_96]  | 11  | 1.66 |          45.5 | 70.4 |\n",
    "\n",
    "The weights for all 16 models are obtained and\n",
    "translated from the Tensorflow checkpoints\n",
    "from TensorFlow checkpoints found [here]\n",
    "(https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet/README.md).\n",
    "\n",
    "# Reference\n",
    "\n",
    "This file contains building code for MobileNetV2, based on\n",
    "[MobileNetV2: Inverted Residuals and Linear Bottlenecks]\n",
    "(https://arxiv.org/abs/1801.04381) (CVPR 2018)\n",
    "\n",
    "Tests comparing this model to the existing Tensorflow model can be\n",
    "found at [mobilenet_v2_keras]\n",
    "(https://github.com/JonathanCMitchell/mobilenet_v2_keras)\n",
    "\"\"\"\n",
    "from __future__ import print_function\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "\n",
    "import os\n",
    "import warnings\n",
    "import numpy as np\n",
    "\n",
    "# TODO Change path to v1.1\n",
    "BASE_WEIGHT_PATH = ('https://github.com/JonathanCMitchell/mobilenet_v2_keras/'\n",
    "                    'releases/download/v1.1/')\n",
    "\n",
    "backend= keras.backend\n",
    "layers = keras.layers\n",
    "models = keras.models\n",
    "keras_utils = keras.utils\n",
    "\n",
    "\n",
    "def preprocess_input(x, **kwargs):\n",
    "    \"\"\"Preprocesses a numpy array encoding a batch of images.\n",
    "\n",
    "    # Arguments\n",
    "        x: a 4D numpy array consists of RGB values within [0, 255].\n",
    "\n",
    "    # Returns\n",
    "        Preprocessed array.\n",
    "    \"\"\"\n",
    "    return utils.preprocess_input(x, mode='tf', **kwargs)\n",
    "\n",
    "\n",
    "# This function is taken from the original tf repo.\n",
    "# It ensures that all layers have a channel number that is divisible by 8\n",
    "# It can be seen here:\n",
    "# https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet/mobilenet.py\n",
    "\n",
    "\n",
    "def _make_divisible(v, divisor, min_value=None):\n",
    "    if min_value is None:\n",
    "        min_value = divisor\n",
    "    new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)\n",
    "    # Make sure that round down does not go down by more than 10%.\n",
    "    if new_v < 0.9 * v:\n",
    "        new_v += divisor\n",
    "    return new_v\n",
    "\n",
    "\n",
    "def MobileNetV2(input_shape=None,\n",
    "                alpha=1.0,\n",
    "                include_top=True,\n",
    "                weights='imagenet',\n",
    "                input_tensor=None,\n",
    "                pooling=None,\n",
    "                classes=1000,\n",
    "                lambda_mask = None,\n",
    "                **kwargs):\n",
    "    \"\"\"Instantiates the MobileNetV2 architecture.\n",
    "\n",
    "    # Arguments\n",
    "        input_shape: optional shape tuple, to be specified if you would\n",
    "            like to use a model with an input img resolution that is not\n",
    "            (224, 224, 3).\n",
    "            It should have exactly 3 inputs channels (224, 224, 3).\n",
    "            You can also omit this option if you would like\n",
    "            to infer input_shape from an input_tensor.\n",
    "            If you choose to include both input_tensor and input_shape then\n",
    "            input_shape will be used if they match, if the shapes\n",
    "            do not match then we will throw an error.\n",
    "            E.g. `(160, 160, 3)` would be one valid value.\n",
    "        alpha: controls the width of the network. This is known as the\n",
    "        width multiplier in the MobileNetV2 paper, but the name is kept for\n",
    "        consistency with MobileNetV1 in Keras.\n",
    "            - If `alpha` < 1.0, proportionally decreases the number\n",
    "                of filters in each layer.\n",
    "            - If `alpha` > 1.0, proportionally increases the number\n",
    "                of filters in each layer.\n",
    "            - If `alpha` = 1, default number of filters from the paper\n",
    "                 are used at each layer.\n",
    "        include_top: whether to include the fully-connected\n",
    "            layer at the top of the network.\n",
    "        weights: one of `None` (random initialization),\n",
    "              'imagenet' (pre-training on ImageNet),\n",
    "              or the path to the weights file to be loaded.\n",
    "        input_tensor: optional Keras tensor (i.e. output of\n",
    "            `layers.Input()`)\n",
    "            to use as image input for the model.\n",
    "        pooling: Optional pooling mode for feature extraction\n",
    "            when `include_top` is `False`.\n",
    "            - `None` means that the output of the model\n",
    "                will be the 4D tensor output of the\n",
    "                last convolutional block.\n",
    "            - `avg` means that global average pooling\n",
    "                will be applied to the output of the\n",
    "                last convolutional block, and thus\n",
    "                the output of the model will be a\n",
    "                2D tensor.\n",
    "            - `max` means that global max pooling will\n",
    "                be applied.\n",
    "        classes: optional number of classes to classify images\n",
    "            into, only to be specified if `include_top` is True, and\n",
    "            if no `weights` argument is specified.\n",
    "\n",
    "    # Returns\n",
    "        A Keras model instance.\n",
    "\n",
    "    # Raises\n",
    "        ValueError: in case of invalid argument for `weights`,\n",
    "            or invalid input shape or invalid alpha, rows when\n",
    "            weights='imagenet'\n",
    "    \"\"\"\n",
    "    global backend, layers, models, keras_utils, debug\n",
    "    debug = False\n",
    "    backend= keras.backend\n",
    "    layers = keras.layers\n",
    "    models = keras.models\n",
    "    keras_utils = keras.utils\n",
    "\n",
    "    if not (weights in {'imagenet', None} or os.path.exists(weights)):\n",
    "        raise ValueError('The `weights` argument should be either '\n",
    "                         '`None` (random initialization), `imagenet` '\n",
    "                         '(pre-training on ImageNet), '\n",
    "                         'or the path to the weights file to be loaded.')\n",
    "\n",
    "    if weights == 'imagenet' and include_top and classes != 1000:\n",
    "        raise ValueError('If using `weights` as `\"imagenet\"` with `include_top` '\n",
    "                         'as true, `classes` should be 1000')\n",
    "\n",
    "    # Determine proper input shape and default size.\n",
    "    # If both input_shape and input_tensor are used, they should match\n",
    "    if input_shape is not None and input_tensor is not None:\n",
    "        try:\n",
    "            is_input_t_tensor = backend.is_keras_tensor(input_tensor)\n",
    "        except ValueError:\n",
    "            try:\n",
    "                is_input_t_tensor = backend.is_keras_tensor(\n",
    "                    keras_utils.get_source_inputs(input_tensor))\n",
    "            except ValueError:\n",
    "                raise ValueError('input_tensor: ', input_tensor,\n",
    "                                 'is not type input_tensor')\n",
    "        if is_input_t_tensor:\n",
    "            if backend.image_data_format == 'channels_first':\n",
    "                if backend.int_shape(input_tensor)[1] != input_shape[1]:\n",
    "                    raise ValueError('input_shape: ', input_shape,\n",
    "                                     'and input_tensor: ', input_tensor,\n",
    "                                     'do not meet the same shape requirements')\n",
    "            else:\n",
    "                if backend.int_shape(input_tensor)[2] != input_shape[1]:\n",
    "                    raise ValueError('input_shape: ', input_shape,\n",
    "                                     'and input_tensor: ', input_tensor,\n",
    "                                     'do not meet the same shape requirements')\n",
    "        else:\n",
    "            raise ValueError('input_tensor specified: ', input_tensor,\n",
    "                             'is not a keras tensor')\n",
    "\n",
    "    # If input_shape is None, infer shape from input_tensor\n",
    "    if input_shape is None and input_tensor is not None:\n",
    "\n",
    "        try:\n",
    "            backend.is_keras_tensor(input_tensor)\n",
    "        except ValueError:\n",
    "            raise ValueError('input_tensor: ', input_tensor,\n",
    "                             'is type: ', type(input_tensor),\n",
    "                             'which is not a valid type')\n",
    "\n",
    "        if input_shape is None and not backend.is_keras_tensor(input_tensor):\n",
    "            default_size = 224\n",
    "        elif input_shape is None and backend.is_keras_tensor(input_tensor):\n",
    "            if backend.image_data_format() == 'channels_first':\n",
    "                rows = backend.int_shape(input_tensor)[2]\n",
    "                cols = backend.int_shape(input_tensor)[3]\n",
    "            else:\n",
    "                rows = backend.int_shape(input_tensor)[1]\n",
    "                cols = backend.int_shape(input_tensor)[2]\n",
    "\n",
    "            if rows == cols and rows in [96, 128, 160, 192, 224]:\n",
    "                default_size = rows\n",
    "            else:\n",
    "                default_size = 224\n",
    "\n",
    "    # If input_shape is None and no input_tensor\n",
    "    elif input_shape is None:\n",
    "        default_size = 224\n",
    "\n",
    "    # If input_shape is not None, assume default size\n",
    "    else:\n",
    "        if backend.image_data_format() == 'channels_first':\n",
    "            rows = input_shape[1]\n",
    "            cols = input_shape[2]\n",
    "        else:\n",
    "            rows = input_shape[0]\n",
    "            cols = input_shape[1]\n",
    "\n",
    "        if rows == cols and rows in [96, 128, 160, 192, 224]:\n",
    "            default_size = rows\n",
    "        else:\n",
    "            default_size = 224\n",
    "\n",
    "    input_shape = utils._obtain_input_shape(input_shape,\n",
    "                                      default_size=default_size,\n",
    "                                      min_size=32,\n",
    "                                      data_format=backend.image_data_format(),\n",
    "                                      require_flatten=include_top,\n",
    "                                      weights=weights)\n",
    "\n",
    "    if backend.image_data_format() == 'channels_last':\n",
    "        row_axis, col_axis = (0, 1)\n",
    "    else:\n",
    "        row_axis, col_axis = (1, 2)\n",
    "    rows = input_shape[row_axis]\n",
    "    cols = input_shape[col_axis]\n",
    "\n",
    "    if weights == 'imagenet':\n",
    "        if alpha not in [0.35, 0.50, 0.75, 1.0, 1.3, 1.4]:\n",
    "            raise ValueError('If imagenet weights are being loaded, '\n",
    "                             'alpha can be one of `0.35`, `0.50`, `0.75`, '\n",
    "                             '`1.0`, `1.3` or `1.4` only.')\n",
    "\n",
    "        if rows != cols or rows not in [96, 128, 160, 192, 224]:\n",
    "            rows = 224\n",
    "            warnings.warn('`input_shape` is undefined or non-square, '\n",
    "                          'or `rows` is not in [96, 128, 160, 192, 224].'\n",
    "                          ' Weights for input shape (224, 224) will be'\n",
    "                          ' loaded as the default.')\n",
    "\n",
    "    if input_tensor is None:\n",
    "        img_input = layers.Input(shape=input_shape)\n",
    "    else:\n",
    "        if not backend.is_keras_tensor(input_tensor):\n",
    "            img_input = layers.Input(tensor=input_tensor, shape=input_shape)\n",
    "        else:\n",
    "            img_input = input_tensor\n",
    "\n",
    "    channel_axis = 1 if backend.image_data_format() == 'channels_first' else -1\n",
    "\n",
    "    first_block_filters = _make_divisible(32 * alpha, 8)\n",
    "    x = layers.ZeroPadding2D(padding=correct_pad(backend, img_input, 3),\n",
    "                             name='Conv1_pad')(img_input)\n",
    "    x = layers.Conv2D(first_block_filters,\n",
    "                      kernel_size=3,\n",
    "                      strides=(2, 2),\n",
    "                      padding='valid',\n",
    "                      use_bias=False,\n",
    "                      name='Conv1')(x)\n",
    "    global start_index, end_index\n",
    "    start_index = end_index = 0\n",
    "    #################\n",
    "    if lambda_mask is not None:\n",
    "        start_index = end_index\n",
    "        end_index = start_index + (default_size//2 * default_size//2 * first_block_filters)\n",
    "        x_mask  = np.reshape(lambda_mask[start_index:end_index], (default_size//2, default_size//2, first_block_filters))\n",
    "        if debug:\n",
    "            print('Conv_1',start_index,end_index)\n",
    "    else:\n",
    "        x_mask = np.ones(shape=((default_size//2, default_size//2, first_block_filters)))\n",
    "\n",
    "    x_mask  = backend.variable(x_mask)\n",
    "    x = Lambda(lambda z: z * x_mask)(x)\n",
    "    ####################\n",
    "    x = layers.BatchNormalization(axis=channel_axis,\n",
    "                                  epsilon=1e-3,\n",
    "                                  momentum=0.999,\n",
    "                                  name='bn_Conv1')(x)\n",
    "    #################\n",
    "    if lambda_mask is not None:\n",
    "        start_index = end_index\n",
    "        end_index = start_index + (default_size//2 * default_size//2 * first_block_filters)\n",
    "        x_mask  = np.reshape(lambda_mask[start_index:end_index], (default_size//2, default_size//2, first_block_filters))\n",
    "        if debug:\n",
    "            print('Conv_1_BN',start_index,end_index)\n",
    "    else:\n",
    "        x_mask = np.ones(shape=((default_size//2, default_size//2, first_block_filters)))\n",
    "\n",
    "    x_mask  = backend.variable(x_mask)\n",
    "    x = Lambda(lambda z: z * x_mask)(x)\n",
    "    ####################\n",
    "    x = layers.ReLU(6., name='Conv1_relu')(x)\n",
    "\n",
    "    x = _inverted_res_block(x, filters=16, alpha=alpha, stride=1,\n",
    "                            expansion=1, block_id=0, lambda_mask=lambda_mask)\n",
    "\n",
    "    x = _inverted_res_block(x, filters=24, alpha=alpha, stride=2,\n",
    "                            expansion=6, block_id=1, lambda_mask=lambda_mask)\n",
    "    x = _inverted_res_block(x, filters=24, alpha=alpha, stride=1,\n",
    "                            expansion=6, block_id=2, lambda_mask=lambda_mask)\n",
    "\n",
    "    x = _inverted_res_block(x, filters=32, alpha=alpha, stride=2,\n",
    "                            expansion=6, block_id=3, lambda_mask=lambda_mask)\n",
    "    x = _inverted_res_block(x, filters=32, alpha=alpha, stride=1,\n",
    "                            expansion=6, block_id=4, lambda_mask=lambda_mask)\n",
    "    x = _inverted_res_block(x, filters=32, alpha=alpha, stride=1,\n",
    "                            expansion=6, block_id=5, lambda_mask=lambda_mask)\n",
    "\n",
    "    x = _inverted_res_block(x, filters=64, alpha=alpha, stride=2,\n",
    "                            expansion=6, block_id=6, lambda_mask=lambda_mask)\n",
    "    x = _inverted_res_block(x, filters=64, alpha=alpha, stride=1,\n",
    "                            expansion=6, block_id=7, lambda_mask=lambda_mask)\n",
    "    x = _inverted_res_block(x, filters=64, alpha=alpha, stride=1,\n",
    "                            expansion=6, block_id=8, lambda_mask=lambda_mask)\n",
    "    x = _inverted_res_block(x, filters=64, alpha=alpha, stride=1,\n",
    "                            expansion=6, block_id=9, lambda_mask=lambda_mask)\n",
    "\n",
    "    x = _inverted_res_block(x, filters=96, alpha=alpha, stride=1,\n",
    "                            expansion=6, block_id=10, lambda_mask=lambda_mask)\n",
    "    x = _inverted_res_block(x, filters=96, alpha=alpha, stride=1,\n",
    "                            expansion=6, block_id=11, lambda_mask=lambda_mask)\n",
    "    x = _inverted_res_block(x, filters=96, alpha=alpha, stride=1,\n",
    "                            expansion=6, block_id=12, lambda_mask=lambda_mask)\n",
    "\n",
    "    x = _inverted_res_block(x, filters=160, alpha=alpha, stride=2,\n",
    "                            expansion=6, block_id=13, lambda_mask=lambda_mask)\n",
    "    x = _inverted_res_block(x, filters=160, alpha=alpha, stride=1,\n",
    "                            expansion=6, block_id=14, lambda_mask=lambda_mask)\n",
    "    x = _inverted_res_block(x, filters=160, alpha=alpha, stride=1,\n",
    "                            expansion=6, block_id=15, lambda_mask=lambda_mask)\n",
    "\n",
    "    x = _inverted_res_block(x, filters=320, alpha=alpha, stride=1,\n",
    "                            expansion=6, block_id=16, lambda_mask=lambda_mask)\n",
    "\n",
    "    # no alpha applied to last conv as stated in the paper:\n",
    "    # if the width multiplier is greater than 1 we\n",
    "    # increase the number of output channels\n",
    "    if alpha > 1.0:\n",
    "        last_block_filters = _make_divisible(1280 * alpha, 8)\n",
    "    else:\n",
    "        last_block_filters = 1280\n",
    "\n",
    "    x = layers.Conv2D(last_block_filters,\n",
    "                      kernel_size=1,\n",
    "                      use_bias=False,\n",
    "                      name='Conv_1')(x)\n",
    "    #################\n",
    "    if lambda_mask is not None:\n",
    "        start_index = end_index\n",
    "        end_index = start_index + (x.shape[1] * x.shape[2]* last_block_filters)\n",
    "        x_mask  = np.reshape(lambda_mask[start_index:end_index], (x.shape[1], x.shape[2], last_block_filters))\n",
    "        if debug:\n",
    "            print('Conv_1',start_index,end_index)\n",
    "    else:\n",
    "        x_mask = np.ones(shape=((x.shape[1], x.shape[2], last_block_filters)))\n",
    "\n",
    "    x_mask  = backend.variable(x_mask)\n",
    "    x = Lambda(lambda z: z * x_mask)(x)\n",
    "    ####################\n",
    "    x = layers.BatchNormalization(axis=channel_axis,\n",
    "                                  epsilon=1e-3,\n",
    "                                  momentum=0.999,\n",
    "                                  name='Conv_1_bn')(x)\n",
    "    #################\n",
    "    if lambda_mask is not None:\n",
    "        start_index = end_index\n",
    "        end_index = start_index + (x.shape[1] * x.shape[2] * last_block_filters)\n",
    "        x_mask  = np.reshape(lambda_mask[start_index:end_index], (x.shape[1], x.shape[2], last_block_filters))\n",
    "        if debug:\n",
    "            print('Conv_1_bn',start_index,end_index)\n",
    "    else:\n",
    "        x_mask = np.ones(shape=((x.shape[1],x.shape[2], last_block_filters)))\n",
    "\n",
    "    x_mask  = backend.variable(x_mask)\n",
    "    x = Lambda(lambda z: z * x_mask)(x)\n",
    "    ####################\n",
    "    x = layers.ReLU(6., name='out_relu')(x)\n",
    "\n",
    "    if include_top:\n",
    "        x = layers.GlobalAveragePooling2D()(x)\n",
    "        x = layers.Dense(classes, activation='softmax', use_bias=True, name='Logits')(x)\n",
    "    else:\n",
    "        if pooling == 'avg':\n",
    "            x = layers.GlobalAveragePooling2D()(x)\n",
    "        elif pooling == 'max':\n",
    "            x = layers.GlobalMaxPooling2D()(x)\n",
    "\n",
    "    # Ensure that the model takes into account\n",
    "    # any potential predecessors of `input_tensor`.\n",
    "    if input_tensor is not None:\n",
    "        inputs = keras_utils.get_source_inputs(input_tensor)\n",
    "    else:\n",
    "        inputs = img_input\n",
    "\n",
    "    # Create model.\n",
    "    model = models.Model(inputs, x, name='mobilenetv2_%0.2f_%s' % (alpha, rows))\n",
    "\n",
    "    # Load weights.\n",
    "    if weights == 'imagenet':\n",
    "        if include_top:\n",
    "            model_name = ('mobilenet_v2_weights_tf_dim_ordering_tf_kernels_' + str(alpha) + '_' + str(rows) + '.h5')\n",
    "            weight_path = BASE_WEIGHT_PATH + model_name\n",
    "            weights_path = keras_utils.get_file(model_name, weight_path, cache_subdir='models')\n",
    "        else:\n",
    "            model_name = ('mobilenet_v2_weights_tf_dim_ordering_tf_kernels_' + str(alpha) + '_' + str(rows) + '_no_top' + '.h5')\n",
    "            weight_path = BASE_WEIGHT_PATH + model_name\n",
    "            weights_path = keras_utils.get_file(model_name, weight_path, cache_subdir='models')\n",
    "        model.load_weights(weights_path)\n",
    "    elif weights is not None:\n",
    "        model.load_weights(weights)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def _inverted_res_block(inputs, expansion, stride, alpha, filters, block_id, lambda_mask = None):\n",
    "    global debug\n",
    "    channel_axis = 1 if backend.image_data_format() == 'channels_first' else -1\n",
    "    in_channels = backend.int_shape(inputs)[channel_axis]\n",
    "    pointwise_conv_filters = int(filters * alpha)\n",
    "    pointwise_filters = _make_divisible(pointwise_conv_filters, 8)\n",
    "    x = inputs\n",
    "    prefix = 'block_{}_'.format(block_id)\n",
    "    #print(prefix, inputs.shape,inputs.shape[0],inputs.shape[1] , in_channels, pointwise_conv_filters, pointwise_filters, filters)\n",
    "    global start_index, end_index\n",
    "    if block_id:\n",
    "        # Expand\n",
    "        x = layers.Conv2D(expansion * in_channels,\n",
    "                          kernel_size=1,\n",
    "                          padding='same',\n",
    "                          use_bias=False,\n",
    "                          activation=None,\n",
    "                          name=prefix + 'expand')(x)\n",
    "        #################\n",
    "        if lambda_mask is not None:\n",
    "            start_index = end_index\n",
    "            end_index = start_index + (inputs.shape[1] * inputs.shape[2] * inputs.shape[3]*expansion)\n",
    "            x_mask  = np.reshape(lambda_mask[start_index:end_index], (inputs.shape[1], inputs.shape[2], inputs.shape[3]*expansion))\n",
    "            if debug:\n",
    "                print(prefix + 'expand',start_index,end_index)\n",
    "        else:\n",
    "            x_mask = np.ones(shape=((inputs.shape[1], inputs.shape[2], inputs.shape[3]*expansion)))\n",
    "\n",
    "        x_mask  = backend.variable(x_mask)\n",
    "        x = Lambda(lambda z: z * x_mask)(x)\n",
    "        ####################\n",
    "        x = layers.BatchNormalization(axis=channel_axis,\n",
    "                                      epsilon=1e-3,\n",
    "                                      momentum=0.999,\n",
    "                                      name=prefix + 'expand_BN')(x)\n",
    "  \n",
    "        #################\n",
    "        if lambda_mask is not None:\n",
    "            start_index = end_index\n",
    "            end_index = start_index + (inputs.shape[1] * inputs.shape[2] * inputs.shape[3]*expansion)\n",
    "            x_mask  = np.reshape(lambda_mask[start_index:end_index], (inputs.shape[1], inputs.shape[2], inputs.shape[3]*expansion))\n",
    "            if debug:\n",
    "                print(prefix + 'expand_BN',start_index,end_index)\n",
    "        else:\n",
    "            x_mask = np.ones(shape=((inputs.shape[1], inputs.shape[2], inputs.shape[3]*expansion)))\n",
    "\n",
    "        x_mask  = backend.variable(x_mask)\n",
    "        x = Lambda(lambda z: z * x_mask)(x)\n",
    "        ####################\n",
    "        x = layers.ReLU(6., name=prefix + 'expand_relu')(x)\n",
    "    else:\n",
    "        prefix = 'expanded_conv_'\n",
    "\n",
    "    # Depthwise\n",
    "    if stride == 2:\n",
    "        x = layers.ZeroPadding2D(padding=correct_pad(backend, x, 3), name=prefix + 'pad')(x)\n",
    "    x = layers.DepthwiseConv2D(kernel_size=3, strides=stride, activation=None, use_bias=False,padding='same' if stride == 1 else 'valid', name=prefix + 'depthwise')(x)\n",
    "    #################\n",
    "    if lambda_mask is not None:\n",
    "        start_index = end_index\n",
    "        end_index = start_index + (inputs.shape[1]//stride * inputs.shape[2]//stride * inputs.shape[3]*expansion)\n",
    "        x_mask  = np.reshape(lambda_mask[start_index:end_index], (inputs.shape[1]//stride, inputs.shape[2]//stride, inputs.shape[3]*expansion))\n",
    "        if debug:\n",
    "            print(prefix + 'depthwise',start_index,end_index)\n",
    "    else:\n",
    "        x_mask = np.ones(shape=((inputs.shape[1]//stride, inputs.shape[2]//stride, inputs.shape[3]*expansion)))\n",
    "\n",
    "    x_mask  = backend.variable(x_mask)\n",
    "    x = Lambda(lambda z: z * x_mask)(x)\n",
    "    ####################\n",
    "    x = layers.BatchNormalization(axis=channel_axis,epsilon=1e-3,momentum=0.999, name=prefix + 'depthwise_BN')(x)\n",
    "    #################\n",
    "    if lambda_mask is not None:\n",
    "        start_index = end_index\n",
    "        end_index = start_index + (inputs.shape[1]//stride * inputs.shape[2]//stride * inputs.shape[3]*expansion)\n",
    "        x_mask  = np.reshape(lambda_mask[start_index:end_index], (inputs.shape[1]//stride, inputs.shape[2]//stride, inputs.shape[3]*expansion))\n",
    "        if debug:\n",
    "            print(prefix + 'depthwise_BN',start_index,end_index)\n",
    "    else:\n",
    "        x_mask = np.ones(shape=((inputs.shape[1]//stride, inputs.shape[2]//stride, inputs.shape[3]*expansion)))\n",
    "\n",
    "    x_mask  = backend.variable(x_mask)\n",
    "    x = Lambda(lambda z: z * x_mask)(x)\n",
    "    ####################\n",
    "\n",
    "    x = layers.ReLU(6., name=prefix + 'depthwise_relu')(x)\n",
    "\n",
    "    # Project\n",
    "    x = layers.Conv2D(pointwise_filters, kernel_size=1, padding='same', use_bias=False, activation=None, name=prefix + 'project')(x)\n",
    "    #################\n",
    "    if lambda_mask is not None:\n",
    "        start_index = end_index\n",
    "        end_index = start_index + (inputs.shape[1]//stride * inputs.shape[2]//stride * pointwise_filters)\n",
    "        x_mask  = np.reshape(lambda_mask[start_index:end_index], (inputs.shape[1]//stride, inputs.shape[2]//stride, pointwise_filters))\n",
    "        if debug:\n",
    "            print(prefix + 'project',start_index,end_index)\n",
    "    else:\n",
    "        x_mask = np.ones(shape=((inputs.shape[1]//stride, inputs.shape[2]//stride, pointwise_filters)))\n",
    "\n",
    "    x_mask  = backend.variable(x_mask)\n",
    "    x = Lambda(lambda z: z * x_mask)(x)\n",
    "    ####################\n",
    "    x = layers.BatchNormalization(axis=channel_axis, epsilon=1e-3, momentum=0.999, name=prefix + 'project_BN')(x)\n",
    "    #################\n",
    "    if lambda_mask is not None:\n",
    "        start_index = end_index\n",
    "        end_index = start_index + (inputs.shape[1]//stride * inputs.shape[2]//stride * pointwise_filters)\n",
    "        x_mask  = np.reshape(lambda_mask[start_index:end_index], (inputs.shape[1]//stride, inputs.shape[2]//stride, pointwise_filters))\n",
    "        if debug:\n",
    "            print(prefix + 'project_BN',start_index,end_index)\n",
    "    else:\n",
    "        x_mask = np.ones(shape=((inputs.shape[1]//stride, inputs.shape[2]//stride, pointwise_filters)))\n",
    "\n",
    "    x_mask  = backend.variable(x_mask)\n",
    "    x = Lambda(lambda z: z * x_mask)(x)\n",
    "    ####################\n",
    "\n",
    "    if in_channels == pointwise_filters and stride == 1:\n",
    "        return layers.Add(name=prefix + 'add')([inputs, x])\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to pre-process the input image to ensure uniform size and color\n",
    "def preprocess_image_batch(image_paths, img_size=None, crop_size=None, color_mode='rgb', out=None):\n",
    "    \"\"\"\n",
    "    Consistent preprocessing of images batches\n",
    "    :param image_paths: iterable: images to process\n",
    "    :param crop_size: tuple: crop images if specified\n",
    "    :param img_size: tuple: resize images if specified\n",
    "    :param color_mode: Use rgb or change to bgr mode based on type of model you want to use\n",
    "    :param out: append output to this iterable if specified\n",
    "    \"\"\"\n",
    "    img_list = []\n",
    "\n",
    "    for im_path in image_paths:\n",
    "        '''\n",
    "        img = imread(im_path,as_gray=False, pilmode=\"RGB\")\n",
    "        #print im_path\n",
    "        #print img.shape\n",
    "        if img_size:\n",
    "            img = resize(img, img_size)\n",
    "\n",
    "        img = img.astype('float32')\n",
    "        # We normalize the colors (in RGB space) with the empirical means on the training set\n",
    "        img[:, :, 0] -= 123.68\n",
    "        img[:, :, 1] -= 116.779\n",
    "        img[:, :, 2] -= 103.939\n",
    "        # We permute the colors to get them in the BGR order\n",
    "        if color_mode == 'bgr':\n",
    "            img[:, :, [0, 1, 2]] = img[:, :, [2, 1, 0]]\n",
    "        img = img.transpose((2, 0, 1))\n",
    "\n",
    "        if crop_size:\n",
    "            img = img[:, (img_size[0] - crop_size[0]) // 2:(img_size[0] + crop_size[0]) // 2\n",
    "            , (img_size[1] - crop_size[1]) // 2:(img_size[1] + crop_size[1]) // 2]\n",
    "\n",
    "        img_list.append(img)\n",
    "        '''\n",
    "        size = 224\n",
    "        ret = PIL.Image.open(im_path)\n",
    "        ret = ret.resize((size, size))\n",
    "        ret = np.asarray(ret, dtype=np.uint8).astype(np.float32)\n",
    "        if ret.ndim == 2:\n",
    "            ret.resize((size, size, 1))\n",
    "            ret = np.repeat(ret, 3, axis=-1)\n",
    "        #ret = ret.transpose((2, 0, 1))\n",
    "        #ret = np.flip(ret,0)\n",
    "        global backend\n",
    "        x = preprocess_input(ret, \n",
    "            data_format=backend.image_data_format())\n",
    "        img_list.append(x)\n",
    "\n",
    "\n",
    "    try:\n",
    "        img_batch = np.stack(img_list, axis=0)\n",
    "    except:\n",
    "        print(im_path)\n",
    "        raise ValueError('when img_size and crop_size are None, images'\n",
    "                ' in image_paths must have the same shapes.')\n",
    "\n",
    "    if out is not None and hasattr(out, 'append'):\n",
    "        out.append(img_batch)\n",
    "    else:\n",
    "        return img_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to predict the top 5 accuracy\n",
    "def top5accuracy(true, predicted):\n",
    "    assert len(true) == len(predicted)\n",
    "    result = []\n",
    "    flag  = 0\n",
    "    for i in range(len(true)):\n",
    "        flag  = 0\n",
    "        temp = true[i]\n",
    "        for j in predicted[i][0:5]:\n",
    "            if j == temp:\n",
    "                flag = 1\n",
    "                break\n",
    "        if flag == 1:\n",
    "            result.append(1)\n",
    "        else:\n",
    "            result.append(0)\n",
    "    counter = 0.\n",
    "    for i in result:\n",
    "        if i == 1:\n",
    "            counter += 1.\n",
    "    error = 1.0 - counter/float(len(result))\n",
    "    #print len(np.where(np.asarray(result) == 1)[0])\n",
    "    return len(np.where(np.asarray(result) == 1)[0]), error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the details of all the 1000 classes and the function to conver the synset id to words{\n",
    "meta_clsloc_file = '../../data/meta_clsloc.mat'\n",
    "synsets = loadmat(meta_clsloc_file)['synsets'][0]\n",
    "synsets_imagenet_sorted = sorted([(int(s[0]), str(s[1][0])) for s in synsets[:1000]],key=lambda v: v[1])\n",
    "corr = {}\n",
    "for j in range(1000):\n",
    "    corr[synsets_imagenet_sorted[j][0]] = j\n",
    "\n",
    "corr_inv = {}\n",
    "for j in range(1, 1001):\n",
    "    corr_inv[corr[j]] = j\n",
    "\n",
    "def id_to_words(id_):\n",
    "    return synsets[corr_inv[id_] - 1][2][0]\n",
    "\n",
    "def pprint_output(out, n_max_synsets=10):\n",
    "    wids = []\n",
    "    best_ids = out.argsort()[::-1][:10]\n",
    "    for u in best_ids:\n",
    "        wids.append(str(synsets[corr_inv[u] - 1][1][0]))\n",
    "    #print('%.2f' % round(100 * out[u], 2) + ' : ' + id_to_words(u)+' '+ str(synsets[corr_inv[u] - 1][1][0]))\n",
    "    return wids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code snippet to load the ground truth labels to measure the performance{\n",
    "truth = {}\n",
    "with open('../../data/ILSVRC2014_clsloc_validation_ground_truth.txt') as f:\n",
    "    line_num = 1\n",
    "    for line in f.readlines():\n",
    "        ind_ = int(line)\n",
    "        temp  = None\n",
    "        for i in synsets_imagenet_sorted:\n",
    "            if i[0] == ind_:\n",
    "                temp = i\n",
    "        #print ind_,temp\n",
    "        if temp != None:\n",
    "            truth[line_num] = temp\n",
    "        else:\n",
    "            print('##########', ind_)\n",
    "            pass\n",
    "        line_num += 1\n",
    "#}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nmodel = MobileNetV2( input_shape=None,\\n    alpha=0.35,\\n    include_top=True,\\n    weights=\"imagenet\",\\n    input_tensor=None,\\n    pooling=None,\\n    classes=1000,\\n    classifier_activation=\"softmax\")\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = 'MobileNetV2'\n",
    "'''\n",
    "model = MobileNetV2( input_shape=None,\n",
    "    alpha=0.35,\n",
    "    include_top=True,\n",
    "    weights=\"imagenet\",\n",
    "    input_tensor=None,\n",
    "    pooling=None,\n",
    "    classes=1000,\n",
    "    classifier_activation=\"softmax\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "K.image_data_format()\n",
    "keras.backend.clear_session()\n",
    "gc.collect()\n",
    "#del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#Test Cell 1\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "\n",
    "data_path = '../../data/'+folder+'/'\n",
    "classes = ['1A','1B']\n",
    "fold = 1\n",
    "\n",
    "with open(data_path+classes[0]+'_train_'+model_name+'.pkl','rb') as f:\n",
    "        X_fold = pickle.load(f)\n",
    "with open(data_path+classes[1]+'_train_'+model_name+'.pkl','rb') as f:\n",
    "        y_fold = pickle.load(f)\n",
    "    \n",
    "X = np.column_stack((X_fold,y_fold))\n",
    "X = np.float32(X)\n",
    "\n",
    "\n",
    "kmeans = MiniBatchKMeans(n_clusters=57255, #100x reduction in points\n",
    "                         max_iter=10).fit(X)\n",
    "\n",
    "pred_kmeans = kmeans.predict(X)\n",
    "\n",
    "X_new = kmeans.cluster_centers_\n",
    "\n",
    "with open('../../data/'+folder+'/'+model_name+'_kmeans_first_train_'+model_name+'.pkl', 'wb') as handle:\n",
    "    pickle.dump([X_new,pred_kmeans,kmeans], handle, protocol=pickle.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../data/'+folder+'/kmeans_first_'+str(fold)+'_'+model_name+'.pkl', 'wb') as handle:\n",
    "            pickle.dump([X_new,pred_kmeans,kmeans], handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perfoming Fold:  1\n",
      "GMM\n",
      "Cluster:  7 Label:  0\n",
      "4A 1 0 40 1.0\n",
      "4B 1 0 40 1.0\n",
      "Cluster:  7 Label:  1\n",
      "4A 1 13 40 0.675\n",
      "4B 1 13 40 0.675\n",
      "Cluster:  7 Label:  2\n",
      "4A 1 0 40 1.0\n",
      "4B 1 5 40 0.875\n",
      "Cluster:  7 Label:  3\n",
      "4A 1 0 40 1.0\n",
      "4B 1 1 40 0.975\n",
      "Cluster:  7 Label:  4\n",
      "4A 1 21 40 0.475\n",
      "4B 1 23 40 0.42500000000000004\n",
      "Cluster:  7 Label:  5\n",
      "4A 1 0 40 1.0\n",
      "4B 1 0 40 1.0\n",
      "Cluster:  7 Label:  6\n",
      "4A 1 8 40 0.8\n",
      "4B 1 17 40 0.575\n",
      "Cluster:  7 Label:  7\n",
      "4A 1 0 40 1.0\n",
      "4B 1 0 40 1.0\n",
      "Cluster:  7 Label:  8\n",
      "4A 1 31 40 0.22499999999999998\n",
      "4B 1 32 40 0.19999999999999996\n",
      "Cluster:  7 Label:  9\n",
      "4A 1 34 40 0.15000000000000002\n",
      "4B 1 30 40 0.25\n",
      "Cluster:  7 Label:  10\n",
      "4A 1 14 40 0.65\n",
      "4B 1 16 40 0.6\n",
      "Cluster:  7 Label:  11\n",
      "4A 1 1 40 0.975\n",
      "4B 1 8 40 0.8\n",
      "Cluster:  7 Label:  12\n",
      "4A 1 25 40 0.375\n",
      "4B 1 16 40 0.6\n",
      "Cluster:  7 Label:  13\n",
      "4A 1 0 40 1.0\n",
      "4B 1 0 40 1.0\n",
      "Cluster:  7 Label:  14\n",
      "4A 1 31 40 0.22499999999999998\n",
      "4B 1 29 40 0.275\n",
      "Cluster:  7 Label:  15\n",
      "4A 1 2 40 0.95\n",
      "4B 1 10 40 0.75\n",
      "Cluster:  7 Label:  16\n",
      "4A 1 36 40 0.09999999999999998\n",
      "4B 1 31 40 0.22499999999999998\n",
      "Cluster:  7 Label:  17\n",
      "4A 1 31 40 0.22499999999999998\n",
      "4B 1 24 40 0.4\n",
      "Cluster:  7 Label:  18\n",
      "4A 1 0 40 1.0\n",
      "4B 1 0 40 1.0\n",
      "Cluster:  7 Label:  19\n",
      "4A 1 33 40 0.17500000000000004\n",
      "4B 1 19 40 0.525\n",
      "Cluster:  7 Label:  20\n",
      "4A 1 0 40 1.0\n",
      "4B 1 0 40 1.0\n",
      "Cluster:  7 Label:  21\n",
      "4A 1 17 40 0.575\n",
      "4B 1 21 40 0.475\n",
      "Cluster:  7 Label:  22\n",
      "4A 1 0 40 1.0\n",
      "4B 1 0 40 1.0\n",
      "Cluster:  7 Label:  23\n",
      "4A 1 4 40 0.9\n",
      "4B 1 0 40 1.0\n",
      "Cluster:  7 Label:  24\n",
      "4A 1 29 40 0.275\n",
      "4B 1 26 40 0.35\n",
      "Cluster:  7 Label:  25\n",
      "4A 1 36 40 0.09999999999999998\n",
      "4B 1 31 40 0.22499999999999998\n",
      "Cluster:  7 Label:  26\n",
      "4A 1 0 40 1.0\n",
      "4B 1 0 40 1.0\n",
      "Cluster:  7 Label:  27\n",
      "4A 1 32 40 0.19999999999999996\n",
      "4B 1 31 40 0.22499999999999998\n",
      "Cluster:  7 Label:  28\n",
      "4A 1 21 40 0.475\n",
      "4B 1 27 40 0.32499999999999996\n",
      "Cluster:  7 Label:  29\n",
      "4A 1 31 40 0.22499999999999998\n",
      "4B 1 32 40 0.19999999999999996\n",
      "Cluster:  7 Label:  30\n",
      "4A 1 0 40 1.0\n",
      "4B 1 0 40 1.0\n",
      "Cluster:  7 Label:  31\n",
      "4A 1 35 40 0.125\n",
      "4B 1 31 40 0.22499999999999998\n",
      "Cluster:  7 Label:  32\n",
      "4A 1 15 40 0.625\n",
      "4B 1 24 40 0.4\n",
      "Cluster:  7 Label:  33\n",
      "4A 1 29 40 0.275\n",
      "4B 1 30 40 0.25\n",
      "Cluster:  7 Label:  34\n",
      "4A 1 0 40 1.0\n",
      "4B 1 0 40 1.0\n",
      "Cluster:  7 Label:  35\n",
      "4A 1 36 40 0.09999999999999998\n",
      "4B 1 31 40 0.22499999999999998\n",
      "Cluster:  7 Label:  36\n",
      "4A 1 24 40 0.4\n",
      "4B 1 29 40 0.275\n",
      "Cluster:  7 Label:  37\n",
      "4A 1 0 40 1.0\n",
      "4B 1 6 40 0.85\n",
      "Cluster:  7 Label:  38\n",
      "4A 1 36 40 0.09999999999999998\n",
      "4B 1 31 40 0.22499999999999998\n",
      "Cluster:  7 Label:  39\n",
      "4A 1 31 40 0.22499999999999998\n",
      "4B 1 25 40 0.375\n",
      "Cluster:  7 Label:  40\n",
      "4A 1 0 40 1.0\n",
      "4B 1 0 40 1.0\n",
      "Cluster:  7 Label:  41\n",
      "4A 1 23 40 0.42500000000000004\n",
      "4B 1 26 40 0.35\n",
      "Cluster:  7 Label:  42\n",
      "4A 1 32 40 0.19999999999999996\n",
      "4B 1 25 40 0.375\n",
      "Cluster:  7 Label:  43\n",
      "4A 1 19 40 0.525\n",
      "4B 1 23 40 0.42500000000000004\n",
      "Cluster:  7 Label:  44\n",
      "4A 1 0 40 1.0\n",
      "4B 1 5 40 0.875\n",
      "Cluster:  7 Label:  45\n",
      "4A 1 37 40 0.07499999999999996\n",
      "4B 1 30 40 0.25\n",
      "Cluster:  7 Label:  46\n",
      "4A 1 33 40 0.17500000000000004\n",
      "4B 1 31 40 0.22499999999999998\n",
      "Cluster:  7 Label:  47\n",
      "4A 1 0 40 1.0\n",
      "4B 1 0 40 1.0\n",
      "Cluster:  7 Label:  48\n",
      "4A 1 35 40 0.125\n",
      "4B 1 31 40 0.22499999999999998\n",
      "Cluster:  7 Label:  49\n",
      "4A 1 0 40 1.0\n",
      "4B 1 0 40 1.0\n",
      "Cluster:  7 Label:  50\n",
      "4A 1 18 40 0.55\n",
      "4B 1 17 40 0.575\n",
      "Cluster:  7 Label:  51\n",
      "4A 1 19 40 0.525\n",
      "4B 1 24 40 0.4\n",
      "Cluster:  7 Label:  52\n",
      "4A 1 32 40 0.19999999999999996\n",
      "4B 1 28 40 0.30000000000000004\n",
      "Cluster:  7 Label:  53\n",
      "4A 1 18 40 0.55\n",
      "4B 1 0 40 1.0\n",
      "Cluster:  7 Label:  54\n",
      "4A 1 0 40 1.0\n",
      "4B 1 0 40 1.0\n",
      "Cluster:  7 Label:  55\n",
      "4A 1 0 40 1.0\n",
      "4B 1 0 40 1.0\n",
      "Cluster:  7 Label:  56\n",
      "4A 1 33 40 0.17500000000000004\n",
      "4B 1 33 40 0.17500000000000004\n",
      "Cluster:  7 Label:  57\n",
      "4A 1 21 40 0.475\n",
      "4B 1 27 40 0.32499999999999996\n",
      "Cluster:  7 Label:  58\n",
      "4A 1 34 40 0.15000000000000002\n",
      "4B 1 30 40 0.25\n",
      "Cluster:  7 Label:  59\n",
      "4A 1 0 40 1.0\n",
      "4B 1 0 40 1.0\n",
      "Cluster:  7 Label:  60\n",
      "4A 1 25 40 0.375\n",
      "4B 1 25 40 0.375\n",
      "Cluster:  7 Label:  61\n",
      "4A 1 36 40 0.09999999999999998\n",
      "4B 1 32 40 0.19999999999999996\n",
      "Cluster:  7 Label:  62\n",
      "4A 1 17 40 0.575\n",
      "4B 1 10 40 0.75\n",
      "Cluster:  7 Label:  63\n",
      "4A 1 1 40 0.975\n",
      "4B 1 15 40 0.625\n",
      "Cluster:  7 Label:  64\n",
      "4A 1 34 40 0.15000000000000002\n",
      "4B 1 32 40 0.19999999999999996\n",
      "Cluster:  7 Label:  65\n",
      "4A 1 33 40 0.17500000000000004\n",
      "4B 1 27 40 0.32499999999999996\n",
      "Cluster:  7 Label:  66\n",
      "4A 1 32 40 0.19999999999999996\n",
      "4B 1 25 40 0.375\n",
      "Cluster:  7 Label:  67\n",
      "4A 1 36 40 0.09999999999999998\n",
      "4B 1 29 40 0.275\n",
      "Cluster:  7 Label:  68\n",
      "4A 1 34 40 0.15000000000000002\n",
      "4B 1 32 40 0.19999999999999996\n",
      "Cluster:  7 Label:  69\n",
      "4A 1 33 40 0.17500000000000004\n",
      "4B 1 31 40 0.22499999999999998\n",
      "Cluster:  7 Label:  70\n",
      "4A 1 26 40 0.35\n",
      "4B 1 25 40 0.375\n",
      "Cluster:  7 Label:  71\n",
      "4A 1 33 40 0.17500000000000004\n",
      "4B 1 29 40 0.275\n",
      "Cluster:  7 Label:  72\n",
      "4A 1 14 40 0.65\n",
      "4B 1 1 40 0.975\n",
      "Cluster:  7 Label:  73\n",
      "4A 1 32 40 0.19999999999999996\n",
      "4B 1 21 40 0.475\n",
      "Cluster:  7 Label:  74\n",
      "4A 1 35 40 0.125\n",
      "4B 1 30 40 0.25\n",
      "Cluster:  7 Label:  75\n",
      "4A 1 31 40 0.22499999999999998\n",
      "4B 1 30 40 0.25\n",
      "Cluster:  7 Label:  76\n",
      "4A 1 0 40 1.0\n",
      "4B 1 1 40 0.975\n",
      "Cluster:  7 Label:  77\n",
      "4A 1 34 40 0.15000000000000002\n",
      "4B 1 32 40 0.19999999999999996\n",
      "Cluster:  7 Label:  78\n",
      "4A 1 29 40 0.275\n",
      "4B 1 18 40 0.55\n",
      "Cluster:  7 Label:  79\n",
      "4A 1 24 40 0.4\n",
      "4B 1 20 40 0.5\n",
      "Cluster:  7 Label:  80\n",
      "4A 1 5 40 0.875\n",
      "4B 1 12 40 0.7\n",
      "Cluster:  7 Label:  81\n",
      "4A 1 0 40 1.0\n",
      "4B 1 0 40 1.0\n",
      "Cluster:  7 Label:  82\n",
      "4A 1 31 40 0.22499999999999998\n",
      "4B 1 25 40 0.375\n",
      "Cluster:  7 Label:  83\n",
      "4A 1 35 40 0.125\n",
      "4B 1 33 40 0.17500000000000004\n",
      "Cluster:  7 Label:  84\n",
      "4A 1 32 40 0.19999999999999996\n",
      "4B 1 31 40 0.22499999999999998\n",
      "Cluster:  7 Label:  85\n",
      "4A 1 11 40 0.725\n",
      "4B 1 11 40 0.725\n",
      "Cluster:  7 Label:  86\n",
      "4A 1 32 40 0.19999999999999996\n",
      "4B 1 18 40 0.55\n",
      "Cluster:  7 Label:  87\n",
      "4A 1 34 40 0.15000000000000002\n",
      "4B 1 31 40 0.22499999999999998\n",
      "Cluster:  7 Label:  88\n",
      "4A 1 35 40 0.125\n",
      "4B 1 31 40 0.22499999999999998\n",
      "Cluster:  7 Label:  89\n",
      "4A 1 0 40 1.0\n",
      "4B 1 0 40 1.0\n",
      "Cluster:  7 Label:  90\n",
      "4A 1 28 40 0.30000000000000004\n",
      "4B 1 26 40 0.35\n",
      "Cluster:  7 Label:  91\n",
      "4A 1 27 40 0.32499999999999996\n",
      "4B 1 21 40 0.475\n",
      "Cluster:  7 Label:  92\n",
      "4A 1 25 40 0.375\n",
      "4B 1 25 40 0.375\n",
      "Cluster:  7 Label:  93\n",
      "4A 1 30 40 0.25\n",
      "4B 1 29 40 0.275\n",
      "Cluster:  7 Label:  94\n",
      "4A 1 33 40 0.17500000000000004\n",
      "4B 1 28 40 0.30000000000000004\n",
      "Cluster:  7 Label:  95\n",
      "4A 1 31 40 0.22499999999999998\n",
      "4B 1 28 40 0.30000000000000004\n",
      "Cluster:  7 Label:  96\n",
      "4A 1 31 40 0.22499999999999998\n",
      "4B 1 29 40 0.275\n",
      "Cluster:  7 Label:  97\n",
      "4A 1 0 40 1.0\n",
      "4B 1 2 40 0.95\n",
      "Cluster:  7 Label:  98\n",
      "4A 1 30 40 0.25\n",
      "4B 1 29 40 0.275\n",
      "Cluster:  7 Label:  99\n",
      "4A 1 14 40 0.65\n",
      "4B 1 23 40 0.42500000000000004\n",
      "Cluster:  7 Label:  100\n",
      "4A 1 26 40 0.35\n",
      "4B 1 13 40 0.675\n",
      "Cluster:  7 Label:  101\n",
      "4A 1 18 40 0.55\n",
      "4B 1 14 40 0.65\n",
      "Cluster:  7 Label:  102\n",
      "4A 1 15 40 0.625\n",
      "4B 1 13 40 0.675\n",
      "Cluster:  7 Label:  103\n",
      "4A 1 9 40 0.775\n",
      "4B 1 9 40 0.775\n",
      "Cluster:  7 Label:  104\n",
      "4A 1 15 40 0.625\n",
      "4B 1 11 40 0.725\n",
      "Cluster:  7 Label:  105\n",
      "4A 1 32 40 0.19999999999999996\n",
      "4B 1 28 40 0.30000000000000004\n",
      "Cluster:  7 Label:  106\n",
      "4A 1 0 40 1.0\n",
      "4B 1 0 40 1.0\n",
      "Cluster:  7 Label:  107\n",
      "4A 1 35 40 0.125\n",
      "4B 1 31 40 0.22499999999999998\n",
      "Cluster:  7 Label:  108\n",
      "4A 1 35 40 0.125\n",
      "4B 1 31 40 0.22499999999999998\n",
      "Cluster:  7 Label:  109\n",
      "4A 1 27 40 0.32499999999999996\n",
      "4B 1 30 40 0.25\n",
      "Cluster:  7 Label:  110\n",
      "4A 1 32 40 0.19999999999999996\n",
      "4B 1 28 40 0.30000000000000004\n",
      "Cluster:  7 Label:  111\n",
      "4A 1 30 40 0.25\n",
      "4B 1 28 40 0.30000000000000004\n",
      "Cluster:  7 Label:  112\n",
      "4A 1 13 40 0.675\n",
      "4B 1 1 40 0.975\n",
      "Cluster:  7 Label:  113\n",
      "4A 1 0 40 1.0\n",
      "4B 1 16 40 0.6\n",
      "Cluster:  7 Label:  114\n",
      "4A 1 9 40 0.775\n",
      "4B 1 17 40 0.575\n",
      "Cluster:  7 Label:  115\n",
      "4A 1 35 40 0.125\n",
      "4B 1 27 40 0.32499999999999996\n",
      "Cluster:  7 Label:  116\n",
      "4A 1 24 40 0.4\n",
      "4B 1 16 40 0.6\n",
      "Cluster:  7 Label:  117\n",
      "4A 1 18 40 0.55\n",
      "4B 1 25 40 0.375\n",
      "Cluster:  7 Label:  118\n",
      "4A 1 28 40 0.30000000000000004\n",
      "4B 1 23 40 0.42500000000000004\n",
      "Cluster:  7 Label:  119\n",
      "4A 1 33 40 0.17500000000000004\n",
      "4B 1 30 40 0.25\n",
      "Cluster:  7 Label:  120\n",
      "4A 1 29 40 0.275\n",
      "4B 1 26 40 0.35\n",
      "Cluster:  7 Label:  121\n",
      "4A 1 34 40 0.15000000000000002\n",
      "4B 1 30 40 0.25\n",
      "Cluster:  7 Label:  122\n",
      "4A 1 25 40 0.375\n",
      "4B 1 28 40 0.30000000000000004\n",
      "Cluster:  7 Label:  123\n",
      "4A 1 34 40 0.15000000000000002\n",
      "4B 1 31 40 0.22499999999999998\n",
      "Cluster:  7 Label:  124\n",
      "4A 1 15 40 0.625\n",
      "4B 1 15 40 0.625\n",
      "Cluster:  7 Label:  125\n",
      "4A 1 26 40 0.35\n",
      "4B 1 22 40 0.44999999999999996\n",
      "Cluster:  7 Label:  126\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4A 1 34 40 0.15000000000000002\n",
      "4B 1 31 40 0.22499999999999998\n",
      "Cluster:  7 Label:  127\n",
      "4A 1 36 40 0.09999999999999998\n",
      "4B 1 29 40 0.275\n",
      "Cluster:  8 Label:  0\n",
      "4A 1 0 40 1.0\n",
      "4B 1 0 40 1.0\n",
      "Cluster:  8 Label:  1\n",
      "4A 1 17 40 0.575\n",
      "4B 1 15 40 0.625\n",
      "Cluster:  8 Label:  2\n",
      "4A 1 32 40 0.19999999999999996\n",
      "4B 1 32 40 0.19999999999999996\n",
      "Cluster:  8 Label:  3\n",
      "4A 1 7 40 0.825\n",
      "4B 1 1 40 0.975\n",
      "Cluster:  8 Label:  4\n",
      "4A 1 32 40 0.19999999999999996\n",
      "4B 1 29 40 0.275\n",
      "Cluster:  8 Label:  5\n",
      "4A 1 27 40 0.32499999999999996\n",
      "4B 1 23 40 0.42500000000000004\n",
      "Cluster:  8 Label:  6\n",
      "4A 1 35 40 0.125\n",
      "4B 1 31 40 0.22499999999999998\n",
      "Cluster:  8 Label:  7\n",
      "4A 1 34 40 0.15000000000000002\n",
      "4B 1 31 40 0.22499999999999998\n",
      "Cluster:  8 Label:  8\n",
      "4A 1 31 40 0.22499999999999998\n",
      "4B 1 27 40 0.32499999999999996\n",
      "Cluster:  8 Label:  9\n",
      "4A 1 11 40 0.725\n",
      "4B 1 5 40 0.875\n",
      "Cluster:  8 Label:  10\n",
      "4A 1 0 40 1.0\n",
      "4B 1 3 40 0.925\n",
      "Cluster:  8 Label:  11\n",
      "4A 1 33 40 0.17500000000000004\n",
      "4B 1 30 40 0.25\n",
      "Cluster:  8 Label:  12\n",
      "4A 1 15 40 0.625\n",
      "4B 1 20 40 0.5\n",
      "Cluster:  8 Label:  13\n",
      "4A 1 31 40 0.22499999999999998\n",
      "4B 1 28 40 0.30000000000000004\n",
      "Cluster:  8 Label:  14\n",
      "4A 1 12 40 0.7\n",
      "4B 1 23 40 0.42500000000000004\n",
      "Cluster:  8 Label:  15\n",
      "4A 1 34 40 0.15000000000000002\n",
      "4B 1 28 40 0.30000000000000004\n",
      "Cluster:  8 Label:  16\n",
      "4A 1 17 40 0.575\n",
      "4B 1 0 40 1.0\n",
      "Cluster:  8 Label:  17\n",
      "4A 1 33 40 0.17500000000000004\n",
      "4B 1 30 40 0.25\n",
      "Cluster:  8 Label:  18\n",
      "4A 1 18 40 0.55\n",
      "4B 1 23 40 0.42500000000000004\n",
      "Cluster:  8 Label:  19\n",
      "4A 1 33 40 0.17500000000000004\n",
      "4B 1 28 40 0.30000000000000004\n",
      "Cluster:  8 Label:  20\n",
      "4A 1 34 40 0.15000000000000002\n",
      "4B 1 31 40 0.22499999999999998\n",
      "Cluster:  8 Label:  21\n",
      "4A 1 30 40 0.25\n",
      "4B 1 19 40 0.525\n",
      "Cluster:  8 Label:  22\n",
      "4A 1 34 40 0.15000000000000002\n",
      "4B 1 27 40 0.32499999999999996\n",
      "Cluster:  8 Label:  23\n",
      "4A 1 32 40 0.19999999999999996\n",
      "4B 1 23 40 0.42500000000000004\n",
      "Cluster:  8 Label:  24\n",
      "4A 1 36 40 0.09999999999999998\n",
      "4B 1 31 40 0.22499999999999998\n",
      "Cluster:  8 Label:  25\n",
      "4A 1 35 40 0.125\n",
      "4B 1 31 40 0.22499999999999998\n",
      "Cluster:  8 Label:  26\n",
      "4A 1 31 40 0.22499999999999998\n",
      "4B 1 23 40 0.42500000000000004\n",
      "Cluster:  8 Label:  27\n",
      "4A 1 34 40 0.15000000000000002\n",
      "4B 1 30 40 0.25\n",
      "Cluster:  8 Label:  28\n",
      "4A 1 34 40 0.15000000000000002\n",
      "4B 1 32 40 0.19999999999999996\n",
      "Cluster:  8 Label:  29\n",
      "4A 1 0 40 1.0\n",
      "4B 1 0 40 1.0\n",
      "Cluster:  8 Label:  30\n",
      "4A 1 33 40 0.17500000000000004\n",
      "4B 1 29 40 0.275\n",
      "Cluster:  8 Label:  31\n",
      "4A 1 34 40 0.15000000000000002\n",
      "4B 1 28 40 0.30000000000000004\n",
      "Cluster:  8 Label:  32\n",
      "4A 1 9 40 0.775\n",
      "4B 1 15 40 0.625\n",
      "Cluster:  8 Label:  33\n",
      "4A 1 35 40 0.125\n",
      "4B 1 31 40 0.22499999999999998\n",
      "Cluster:  8 Label:  34\n",
      "4A 1 23 40 0.42500000000000004\n",
      "4B 1 24 40 0.4\n",
      "Cluster:  8 Label:  35\n",
      "4A 1 3 40 0.925\n",
      "4B 1 8 40 0.8\n",
      "Cluster:  8 Label:  36\n",
      "4A 1 0 40 1.0\n",
      "4B 1 0 40 1.0\n",
      "Cluster:  8 Label:  37\n",
      "4A 1 0 40 1.0\n",
      "4B 1 0 40 1.0\n",
      "Cluster:  8 Label:  38\n",
      "4A 1 35 40 0.125\n",
      "4B 1 31 40 0.22499999999999998\n",
      "Cluster:  8 Label:  39\n",
      "4A 1 33 40 0.17500000000000004\n",
      "4B 1 31 40 0.22499999999999998\n",
      "Cluster:  8 Label:  40\n",
      "4A 1 34 40 0.15000000000000002\n",
      "4B 1 28 40 0.30000000000000004\n",
      "Cluster:  8 Label:  41\n",
      "4A 1 25 40 0.375\n",
      "4B 1 28 40 0.30000000000000004\n",
      "Cluster:  8 Label:  42\n",
      "4A 1 32 40 0.19999999999999996\n",
      "4B 1 21 40 0.475\n",
      "Cluster:  8 Label:  43\n",
      "4A 1 35 40 0.125\n",
      "4B 1 30 40 0.25\n",
      "Cluster:  8 Label:  44\n",
      "4A 1 36 40 0.09999999999999998\n",
      "4B 1 27 40 0.32499999999999996\n",
      "Cluster:  8 Label:  45\n",
      "4A 1 7 40 0.825\n",
      "4B 1 9 40 0.775\n",
      "Cluster:  8 Label:  46\n",
      "4A 1 9 40 0.775\n",
      "4B 1 22 40 0.44999999999999996\n",
      "Cluster:  8 Label:  47\n",
      "4A 1 26 40 0.35\n",
      "4B 1 26 40 0.35\n",
      "Cluster:  8 Label:  48\n",
      "4A 1 36 40 0.09999999999999998\n",
      "4B 1 31 40 0.22499999999999998\n",
      "Cluster:  8 Label:  49\n",
      "4A 1 2 40 0.95\n",
      "4B 1 5 40 0.875\n",
      "Cluster:  8 Label:  50\n",
      "4A 1 29 40 0.275\n",
      "4B 1 31 40 0.22499999999999998\n",
      "Cluster:  8 Label:  51\n",
      "4A 1 36 40 0.09999999999999998\n",
      "4B 1 29 40 0.275\n",
      "Cluster:  8 Label:  52\n",
      "4A 1 34 40 0.15000000000000002\n",
      "4B 1 31 40 0.22499999999999998\n",
      "Cluster:  8 Label:  53\n",
      "4A 1 32 40 0.19999999999999996\n",
      "4B 1 27 40 0.32499999999999996\n",
      "Cluster:  8 Label:  54\n",
      "4A 1 35 40 0.125\n",
      "4B 1 31 40 0.22499999999999998\n",
      "Cluster:  8 Label:  55\n",
      "4A 1 33 40 0.17500000000000004\n",
      "4B 1 29 40 0.275\n",
      "Cluster:  8 Label:  56\n",
      "4A 1 31 40 0.22499999999999998\n",
      "4B 1 25 40 0.375\n",
      "Cluster:  8 Label:  57\n",
      "4A 1 2 40 0.95\n",
      "4B 1 11 40 0.725\n",
      "Cluster:  8 Label:  58\n",
      "4A 1 33 40 0.17500000000000004\n",
      "4B 1 30 40 0.25\n",
      "Cluster:  8 Label:  59\n",
      "4A 1 34 40 0.15000000000000002\n",
      "4B 1 31 40 0.22499999999999998\n",
      "Cluster:  8 Label:  60\n",
      "4A 1 2 40 0.95\n",
      "4B 1 22 40 0.44999999999999996\n",
      "Cluster:  8 Label:  61\n",
      "4A 1 32 40 0.19999999999999996\n",
      "4B 1 32 40 0.19999999999999996\n",
      "Cluster:  8 Label:  62\n",
      "4A 1 34 40 0.15000000000000002\n",
      "4B 1 29 40 0.275\n",
      "Cluster:  8 Label:  63\n",
      "4A 1 32 40 0.19999999999999996\n",
      "4B 1 28 40 0.30000000000000004\n",
      "Cluster:  8 Label:  64\n",
      "4A 1 34 40 0.15000000000000002\n",
      "4B 1 31 40 0.22499999999999998\n",
      "Cluster:  8 Label:  65\n",
      "4A 1 0 40 1.0\n",
      "4B 1 0 40 1.0\n",
      "Cluster:  8 Label:  66\n",
      "4A 1 11 40 0.725\n",
      "4B 1 9 40 0.775\n",
      "Cluster:  8 Label:  67\n",
      "4A 1 36 40 0.09999999999999998\n",
      "4B 1 31 40 0.22499999999999998\n",
      "Cluster:  8 Label:  68\n",
      "4A 1 36 40 0.09999999999999998\n",
      "4B 1 31 40 0.22499999999999998\n",
      "Cluster:  8 Label:  69\n",
      "4A 1 35 40 0.125\n",
      "4B 1 31 40 0.22499999999999998\n",
      "Cluster:  8 Label:  70\n",
      "4A 1 26 40 0.35\n",
      "4B 1 22 40 0.44999999999999996\n",
      "Cluster:  8 Label:  71\n",
      "4A 1 35 40 0.125\n",
      "4B 1 29 40 0.275\n",
      "Cluster:  8 Label:  72\n",
      "4A 1 36 40 0.09999999999999998\n",
      "4B 1 29 40 0.275\n",
      "Cluster:  8 Label:  73\n",
      "4A 1 26 40 0.35\n",
      "4B 1 26 40 0.35\n",
      "Cluster:  8 Label:  74\n",
      "4A 1 0 40 1.0\n",
      "4B 1 3 40 0.925\n",
      "Cluster:  8 Label:  75\n",
      "4A 1 30 40 0.25\n",
      "4B 1 31 40 0.22499999999999998\n",
      "Cluster:  8 Label:  76\n",
      "4A 1 35 40 0.125\n",
      "4B 1 31 40 0.22499999999999998\n",
      "Cluster:  8 Label:  77\n",
      "4A 1 0 40 1.0\n",
      "4B 1 0 40 1.0\n",
      "Cluster:  8 Label:  78\n",
      "4A 1 29 40 0.275\n",
      "4B 1 29 40 0.275\n",
      "Cluster:  8 Label:  79\n",
      "4A 1 35 40 0.125\n",
      "4B 1 31 40 0.22499999999999998\n",
      "Cluster:  8 Label:  80\n",
      "4A 1 35 40 0.125\n",
      "4B 1 28 40 0.30000000000000004\n",
      "Cluster:  8 Label:  81\n",
      "4A 1 32 40 0.19999999999999996\n",
      "4B 1 30 40 0.25\n",
      "Cluster:  8 Label:  82\n",
      "4A 1 34 40 0.15000000000000002\n",
      "4B 1 32 40 0.19999999999999996\n",
      "Cluster:  8 Label:  83\n",
      "4A 1 0 40 1.0\n",
      "4B 1 0 40 1.0\n",
      "Cluster:  8 Label:  84\n",
      "4A 1 25 40 0.375\n",
      "4B 1 22 40 0.44999999999999996\n",
      "Cluster:  8 Label:  85\n",
      "4A 1 33 40 0.17500000000000004\n",
      "4B 1 32 40 0.19999999999999996\n",
      "Cluster:  8 Label:  86\n",
      "4A 1 35 40 0.125\n",
      "4B 1 30 40 0.25\n",
      "Cluster:  8 Label:  87\n",
      "4A 1 0 40 1.0\n",
      "4B 1 0 40 1.0\n",
      "Cluster:  8 Label:  88\n",
      "4A 1 27 40 0.32499999999999996\n",
      "4B 1 29 40 0.275\n",
      "Cluster:  8 Label:  89\n",
      "4A 1 30 40 0.25\n",
      "4B 1 30 40 0.25\n",
      "Cluster:  8 Label:  90\n",
      "4A 1 23 40 0.42500000000000004\n",
      "4B 1 12 40 0.7\n",
      "Cluster:  8 Label:  91\n",
      "4A 1 32 40 0.19999999999999996\n",
      "4B 1 19 40 0.525\n",
      "Cluster:  8 Label:  92\n",
      "4A 1 0 40 1.0\n",
      "4B 1 15 40 0.625\n",
      "Cluster:  8 Label:  93\n",
      "4A 1 34 40 0.15000000000000002\n",
      "4B 1 33 40 0.17500000000000004\n",
      "Cluster:  8 Label:  94\n",
      "4A 1 31 40 0.22499999999999998\n",
      "4B 1 28 40 0.30000000000000004\n",
      "Cluster:  8 Label:  95\n",
      "4A 1 35 40 0.125\n",
      "4B 1 31 40 0.22499999999999998\n",
      "Cluster:  8 Label:  96\n",
      "4A 1 35 40 0.125\n",
      "4B 1 31 40 0.22499999999999998\n",
      "Cluster:  8 Label:  97\n",
      "4A 1 32 40 0.19999999999999996\n",
      "4B 1 31 40 0.22499999999999998\n",
      "Cluster:  8 Label:  98\n",
      "4A 1 31 40 0.22499999999999998\n",
      "4B 1 27 40 0.32499999999999996\n",
      "Cluster:  8 Label:  99\n",
      "4A 1 33 40 0.17500000000000004\n",
      "4B 1 33 40 0.17500000000000004\n",
      "Cluster:  8 Label:  100\n",
      "4A 1 36 40 0.09999999999999998\n",
      "4B 1 31 40 0.22499999999999998\n",
      "Cluster:  8 Label:  101\n",
      "4A 1 34 40 0.15000000000000002\n",
      "4B 1 28 40 0.30000000000000004\n",
      "Cluster:  8 Label:  102\n",
      "4A 1 34 40 0.15000000000000002\n",
      "4B 1 30 40 0.25\n",
      "Cluster:  8 Label:  103\n",
      "4A 1 36 40 0.09999999999999998\n",
      "4B 1 27 40 0.32499999999999996\n",
      "Cluster:  8 Label:  104\n",
      "4A 1 34 40 0.15000000000000002\n",
      "4B 1 31 40 0.22499999999999998\n",
      "Cluster:  8 Label:  105\n",
      "4A 1 3 40 0.925\n",
      "4B 1 0 40 1.0\n",
      "Cluster:  8 Label:  106\n",
      "4A 1 32 40 0.19999999999999996\n",
      "4B 1 24 40 0.4\n",
      "Cluster:  8 Label:  107\n",
      "4A 1 35 40 0.125\n",
      "4B 1 31 40 0.22499999999999998\n",
      "Cluster:  8 Label:  108\n",
      "4A 1 35 40 0.125\n",
      "4B 1 29 40 0.275\n",
      "Cluster:  8 Label:  109\n",
      "4A 1 34 40 0.15000000000000002\n",
      "4B 1 29 40 0.275\n",
      "Cluster:  8 Label:  110\n",
      "4A 1 31 40 0.22499999999999998\n",
      "4B 1 33 40 0.17500000000000004\n",
      "Cluster:  8 Label:  111\n",
      "4A 1 9 40 0.775\n",
      "4B 1 22 40 0.44999999999999996\n",
      "Cluster:  8 Label:  112\n",
      "4A 1 35 40 0.125\n",
      "4B 1 32 40 0.19999999999999996\n",
      "Cluster:  8 Label:  113\n",
      "4A 1 35 40 0.125\n",
      "4B 1 33 40 0.17500000000000004\n",
      "Cluster:  8 Label:  114\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4A 1 34 40 0.15000000000000002\n",
      "4B 1 31 40 0.22499999999999998\n",
      "Cluster:  8 Label:  115\n",
      "4A 1 0 40 1.0\n",
      "4B 1 0 40 1.0\n",
      "Cluster:  8 Label:  116\n",
      "4A 1 29 40 0.275\n",
      "4B 1 4 40 0.9\n",
      "Cluster:  8 Label:  117\n",
      "4A 1 29 40 0.275\n",
      "4B 1 6 40 0.85\n",
      "Cluster:  8 Label:  118\n",
      "4A 1 35 40 0.125\n",
      "4B 1 31 40 0.22499999999999998\n",
      "Cluster:  8 Label:  119\n",
      "4A 1 36 40 0.09999999999999998\n",
      "4B 1 31 40 0.22499999999999998\n",
      "Cluster:  8 Label:  120\n",
      "4A 1 29 40 0.275\n",
      "4B 1 26 40 0.35\n",
      "Cluster:  8 Label:  121\n",
      "4A 1 35 40 0.125\n",
      "4B 1 31 40 0.22499999999999998\n",
      "Cluster:  8 Label:  122\n",
      "4A 1 0 40 1.0\n",
      "4B 1 0 40 1.0\n",
      "Cluster:  8 Label:  123\n",
      "4A 1 29 40 0.275\n",
      "4B 1 20 40 0.5\n",
      "Cluster:  8 Label:  124\n",
      "4A 1 36 40 0.09999999999999998\n",
      "4B 1 32 40 0.19999999999999996\n",
      "Cluster:  8 Label:  125\n",
      "4A 1 35 40 0.125\n",
      "4B 1 31 40 0.22499999999999998\n",
      "Cluster:  8 Label:  126\n",
      "4A 1 19 40 0.525\n",
      "4B 1 18 40 0.55\n",
      "Cluster:  8 Label:  127\n",
      "4A 1 36 40 0.09999999999999998\n",
      "4B 1 32 40 0.19999999999999996\n",
      "Cluster:  8 Label:  128\n",
      "4A 1 17 40 0.575\n",
      "4B 1 14 40 0.65\n",
      "Cluster:  8 Label:  129\n",
      "4A 1 34 40 0.15000000000000002\n",
      "4B 1 31 40 0.22499999999999998\n",
      "Cluster:  8 Label:  130\n",
      "4A 1 3 40 0.925\n",
      "4B 1 2 40 0.95\n",
      "Cluster:  8 Label:  131\n",
      "4A 1 33 40 0.17500000000000004\n",
      "4B 1 31 40 0.22499999999999998\n",
      "Cluster:  8 Label:  132\n",
      "4A 1 22 40 0.44999999999999996\n",
      "4B 1 18 40 0.55\n",
      "Cluster:  8 Label:  133\n",
      "4A 1 34 40 0.15000000000000002\n",
      "4B 1 25 40 0.375\n",
      "Cluster:  8 Label:  134\n",
      "4A 1 34 40 0.15000000000000002\n",
      "4B 1 32 40 0.19999999999999996\n",
      "Cluster:  8 Label:  135\n",
      "4A 1 12 40 0.7\n",
      "4B 1 16 40 0.6\n",
      "Cluster:  8 Label:  136\n",
      "4A 1 36 40 0.09999999999999998\n",
      "4B 1 31 40 0.22499999999999998\n",
      "Cluster:  8 Label:  137\n",
      "4A 1 26 40 0.35\n",
      "4B 1 9 40 0.775\n",
      "Cluster:  8 Label:  138\n",
      "4A 1 0 40 1.0\n",
      "4B 1 0 40 1.0\n",
      "Cluster:  8 Label:  139\n",
      "4A 1 34 40 0.15000000000000002\n",
      "4B 1 27 40 0.32499999999999996\n",
      "Cluster:  8 Label:  140\n",
      "4A 1 26 40 0.35\n",
      "4B 1 30 40 0.25\n",
      "Cluster:  8 Label:  141\n",
      "4A 1 36 40 0.09999999999999998\n",
      "4B 1 31 40 0.22499999999999998\n",
      "Cluster:  8 Label:  142\n",
      "4A 1 32 40 0.19999999999999996\n",
      "4B 1 31 40 0.22499999999999998\n",
      "Cluster:  8 Label:  143\n",
      "4A 1 30 40 0.25\n",
      "4B 1 28 40 0.30000000000000004\n",
      "Cluster:  8 Label:  144\n",
      "4A 1 32 40 0.19999999999999996\n",
      "4B 1 30 40 0.25\n",
      "Cluster:  8 Label:  145\n",
      "4A 1 9 40 0.775\n",
      "4B 1 21 40 0.475\n",
      "Cluster:  8 Label:  146\n",
      "4A 1 34 40 0.15000000000000002\n",
      "4B 1 31 40 0.22499999999999998\n",
      "Cluster:  8 Label:  147\n",
      "4A 1 32 40 0.19999999999999996\n",
      "4B 1 32 40 0.19999999999999996\n",
      "Cluster:  8 Label:  148\n",
      "4A 1 21 40 0.475\n",
      "4B 1 4 40 0.9\n",
      "Cluster:  8 Label:  149\n",
      "4A 1 35 40 0.125\n",
      "4B 1 32 40 0.19999999999999996\n",
      "Cluster:  8 Label:  150\n",
      "4A 1 35 40 0.125\n",
      "4B 1 31 40 0.22499999999999998\n",
      "Cluster:  8 Label:  151\n",
      "4A 1 31 40 0.22499999999999998\n",
      "4B 1 29 40 0.275\n",
      "Cluster:  8 Label:  152\n",
      "4A 1 31 40 0.22499999999999998\n",
      "4B 1 29 40 0.275\n",
      "Cluster:  8 Label:  153\n",
      "4A 1 32 40 0.19999999999999996\n",
      "4B 1 27 40 0.32499999999999996\n",
      "Cluster:  8 Label:  154\n",
      "4A 1 32 40 0.19999999999999996\n",
      "4B 1 29 40 0.275\n",
      "Cluster:  8 Label:  155\n",
      "4A 1 36 40 0.09999999999999998\n",
      "4B 1 28 40 0.30000000000000004\n",
      "Cluster:  8 Label:  156\n",
      "4A 1 0 40 1.0\n",
      "4B 1 4 40 0.9\n",
      "Cluster:  8 Label:  157\n",
      "4A 1 34 40 0.15000000000000002\n",
      "4B 1 30 40 0.25\n",
      "Cluster:  8 Label:  158\n",
      "4A 1 18 40 0.55\n",
      "4B 1 22 40 0.44999999999999996\n",
      "Cluster:  8 Label:  159\n",
      "4A 1 33 40 0.17500000000000004\n",
      "4B 1 30 40 0.25\n",
      "Cluster:  8 Label:  160\n",
      "4A 1 33 40 0.17500000000000004\n",
      "4B 1 31 40 0.22499999999999998\n",
      "Cluster:  8 Label:  161\n",
      "4A 1 36 40 0.09999999999999998\n",
      "4B 1 32 40 0.19999999999999996\n",
      "Cluster:  8 Label:  162\n",
      "4A 1 35 40 0.125\n",
      "4B 1 31 40 0.22499999999999998\n",
      "Cluster:  8 Label:  163\n",
      "4A 1 14 40 0.65\n",
      "4B 1 8 40 0.8\n",
      "Cluster:  8 Label:  164\n",
      "4A 1 35 40 0.125\n",
      "4B 1 30 40 0.25\n",
      "Cluster:  8 Label:  165\n",
      "4A 1 36 40 0.09999999999999998\n",
      "4B 1 32 40 0.19999999999999996\n",
      "Cluster:  8 Label:  166\n",
      "4A 1 10 40 0.75\n",
      "4B 1 16 40 0.6\n",
      "Cluster:  8 Label:  167\n",
      "4A 1 33 40 0.17500000000000004\n",
      "4B 1 27 40 0.32499999999999996\n",
      "Cluster:  8 Label:  168\n",
      "4A 1 18 40 0.55\n",
      "4B 1 5 40 0.875\n",
      "Cluster:  8 Label:  169\n",
      "4A 1 35 40 0.125\n",
      "4B 1 31 40 0.22499999999999998\n",
      "Cluster:  8 Label:  170\n",
      "4A 1 3 40 0.925\n",
      "4B 1 11 40 0.725\n",
      "Cluster:  8 Label:  171\n",
      "4A 1 33 40 0.17500000000000004\n",
      "4B 1 30 40 0.25\n",
      "Cluster:  8 Label:  172\n",
      "4A 1 32 40 0.19999999999999996\n",
      "4B 1 30 40 0.25\n",
      "Cluster:  8 Label:  173\n",
      "4A 1 35 40 0.125\n",
      "4B 1 31 40 0.22499999999999998\n",
      "Cluster:  8 Label:  174\n",
      "4A 1 25 40 0.375\n",
      "4B 1 31 40 0.22499999999999998\n",
      "Cluster:  8 Label:  175\n",
      "4A 1 35 40 0.125\n",
      "4B 1 31 40 0.22499999999999998\n",
      "Cluster:  8 Label:  176\n",
      "4A 1 35 40 0.125\n",
      "4B 1 32 40 0.19999999999999996\n",
      "Cluster:  8 Label:  177\n",
      "4A 1 35 40 0.125\n",
      "4B 1 31 40 0.22499999999999998\n",
      "Cluster:  8 Label:  178\n",
      "4A 1 34 40 0.15000000000000002\n",
      "4B 1 32 40 0.19999999999999996\n",
      "Cluster:  8 Label:  179\n",
      "4A 1 35 40 0.125\n",
      "4B 1 24 40 0.4\n",
      "Cluster:  8 Label:  180\n",
      "4A 1 30 40 0.25\n",
      "4B 1 32 40 0.19999999999999996\n",
      "Cluster:  8 Label:  181\n",
      "4A 1 29 40 0.275\n",
      "4B 1 28 40 0.30000000000000004\n",
      "Cluster:  8 Label:  182\n",
      "4A 1 32 40 0.19999999999999996\n",
      "4B 1 32 40 0.19999999999999996\n",
      "Cluster:  8 Label:  183\n",
      "4A 1 34 40 0.15000000000000002\n",
      "4B 1 25 40 0.375\n",
      "Cluster:  8 Label:  184\n",
      "4A 1 22 40 0.44999999999999996\n",
      "4B 1 24 40 0.4\n",
      "Cluster:  8 Label:  185\n",
      "4A 1 28 40 0.30000000000000004\n",
      "4B 1 23 40 0.42500000000000004\n",
      "Cluster:  8 Label:  186\n",
      "4A 1 33 40 0.17500000000000004\n",
      "4B 1 31 40 0.22499999999999998\n",
      "Cluster:  8 Label:  187\n",
      "4A 1 31 40 0.22499999999999998\n",
      "4B 1 31 40 0.22499999999999998\n",
      "Cluster:  8 Label:  188\n",
      "4A 1 27 40 0.32499999999999996\n",
      "4B 1 25 40 0.375\n",
      "Cluster:  8 Label:  189\n",
      "4A 1 33 40 0.17500000000000004\n",
      "4B 1 30 40 0.25\n",
      "Cluster:  8 Label:  190\n",
      "4A 1 27 40 0.32499999999999996\n",
      "4B 1 30 40 0.25\n",
      "Cluster:  8 Label:  191\n",
      "4A 1 34 40 0.15000000000000002\n",
      "4B 1 31 40 0.22499999999999998\n",
      "Cluster:  8 Label:  192\n",
      "4A 1 24 40 0.4\n",
      "4B 1 25 40 0.375\n",
      "Cluster:  8 Label:  193\n",
      "4A 1 34 40 0.15000000000000002\n",
      "4B 1 31 40 0.22499999999999998\n",
      "Cluster:  8 Label:  194\n",
      "4A 1 24 40 0.4\n",
      "4B 1 14 40 0.65\n",
      "Cluster:  8 Label:  195\n",
      "4A 1 35 40 0.125\n",
      "4B 1 31 40 0.22499999999999998\n",
      "Cluster:  8 Label:  196\n",
      "4A 1 26 40 0.35\n",
      "4B 1 17 40 0.575\n",
      "Cluster:  8 Label:  197\n",
      "4A 1 0 40 1.0\n",
      "4B 1 0 40 1.0\n",
      "Cluster:  8 Label:  198\n",
      "4A 1 0 40 1.0\n",
      "4B 1 0 40 1.0\n",
      "Cluster:  8 Label:  199\n",
      "4A 1 33 40 0.17500000000000004\n",
      "4B 1 31 40 0.22499999999999998\n",
      "Cluster:  8 Label:  200\n",
      "4A 1 1 40 0.975\n",
      "4B 1 3 40 0.925\n",
      "Cluster:  8 Label:  201\n",
      "4A 1 33 40 0.17500000000000004\n",
      "4B 1 25 40 0.375\n",
      "Cluster:  8 Label:  202\n",
      "4A 1 35 40 0.125\n",
      "4B 1 32 40 0.19999999999999996\n",
      "Cluster:  8 Label:  203\n",
      "4A 1 28 40 0.30000000000000004\n",
      "4B 1 32 40 0.19999999999999996\n",
      "Cluster:  8 Label:  204\n",
      "4A 1 27 40 0.32499999999999996\n",
      "4B 1 26 40 0.35\n",
      "Cluster:  8 Label:  205\n",
      "4A 1 36 40 0.09999999999999998\n",
      "4B 1 32 40 0.19999999999999996\n",
      "Cluster:  8 Label:  206\n",
      "4A 1 31 40 0.22499999999999998\n",
      "4B 1 25 40 0.375\n",
      "Cluster:  8 Label:  207\n",
      "4A 1 36 40 0.09999999999999998\n",
      "4B 1 31 40 0.22499999999999998\n",
      "Cluster:  8 Label:  208\n",
      "4A 1 34 40 0.15000000000000002\n",
      "4B 1 28 40 0.30000000000000004\n",
      "Cluster:  8 Label:  209\n",
      "4A 1 35 40 0.125\n",
      "4B 1 28 40 0.30000000000000004\n",
      "Cluster:  8 Label:  210\n",
      "4A 1 36 40 0.09999999999999998\n",
      "4B 1 31 40 0.22499999999999998\n",
      "Cluster:  8 Label:  211\n",
      "4A 1 0 40 1.0\n",
      "4B 1 0 40 1.0\n",
      "Cluster:  8 Label:  212\n",
      "4A 1 35 40 0.125\n",
      "4B 1 32 40 0.19999999999999996\n",
      "Cluster:  8 Label:  213\n",
      "4A 1 34 40 0.15000000000000002\n",
      "4B 1 31 40 0.22499999999999998\n",
      "Cluster:  8 Label:  214\n",
      "4A 1 31 40 0.22499999999999998\n",
      "4B 1 26 40 0.35\n",
      "Cluster:  8 Label:  215\n",
      "4A 1 0 40 1.0\n",
      "4B 1 0 40 1.0\n",
      "Cluster:  8 Label:  216\n",
      "4A 1 30 40 0.25\n",
      "4B 1 28 40 0.30000000000000004\n",
      "Cluster:  8 Label:  217\n",
      "4A 1 37 40 0.07499999999999996\n",
      "4B 1 30 40 0.25\n",
      "Cluster:  8 Label:  218\n",
      "4A 1 34 40 0.15000000000000002\n",
      "4B 1 30 40 0.25\n",
      "Cluster:  8 Label:  219\n",
      "4A 1 34 40 0.15000000000000002\n",
      "4B 1 30 40 0.25\n",
      "Cluster:  8 Label:  220\n",
      "4A 1 35 40 0.125\n",
      "4B 1 29 40 0.275\n",
      "Cluster:  8 Label:  221\n",
      "4A 1 25 40 0.375\n",
      "4B 1 28 40 0.30000000000000004\n",
      "Cluster:  8 Label:  222\n",
      "4A 1 29 40 0.275\n",
      "4B 1 29 40 0.275\n",
      "Cluster:  8 Label:  223\n",
      "4A 1 33 40 0.17500000000000004\n",
      "4B 1 30 40 0.25\n",
      "Cluster:  8 Label:  224\n",
      "4A 1 0 40 1.0\n",
      "4B 1 2 40 0.95\n",
      "Cluster:  8 Label:  225\n",
      "4A 1 21 40 0.475\n",
      "4B 1 20 40 0.5\n",
      "Cluster:  8 Label:  226\n",
      "4A 1 33 40 0.17500000000000004\n",
      "4B 1 31 40 0.22499999999999998\n",
      "Cluster:  8 Label:  227\n",
      "4A 1 0 40 1.0\n",
      "4B 1 0 40 1.0\n",
      "Cluster:  8 Label:  228\n",
      "4A 1 36 40 0.09999999999999998\n",
      "4B 1 32 40 0.19999999999999996\n",
      "Cluster:  8 Label:  229\n",
      "4A 1 0 40 1.0\n",
      "4B 1 0 40 1.0\n",
      "Cluster:  8 Label:  230\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4A 1 1 40 0.975\n",
      "4B 1 6 40 0.85\n",
      "Cluster:  8 Label:  231\n",
      "4A 1 0 40 1.0\n",
      "4B 1 0 40 1.0\n",
      "Cluster:  8 Label:  232\n",
      "4A 1 1 40 0.975\n",
      "4B 1 2 40 0.95\n",
      "Cluster:  8 Label:  233\n",
      "4A 1 33 40 0.17500000000000004\n",
      "4B 1 31 40 0.22499999999999998\n",
      "Cluster:  8 Label:  234\n",
      "4A 1 18 40 0.55\n",
      "4B 1 24 40 0.4\n",
      "Cluster:  8 Label:  235\n",
      "4A 1 34 40 0.15000000000000002\n",
      "4B 1 32 40 0.19999999999999996\n",
      "Cluster:  8 Label:  236\n",
      "4A 1 5 40 0.875\n",
      "4B 1 13 40 0.675\n",
      "Cluster:  8 Label:  237\n",
      "4A 1 0 40 1.0\n",
      "4B 1 0 40 1.0\n",
      "Cluster:  8 Label:  238\n",
      "4A 1 36 40 0.09999999999999998\n",
      "4B 1 32 40 0.19999999999999996\n",
      "Cluster:  8 Label:  239\n",
      "4A 1 34 40 0.15000000000000002\n",
      "4B 1 30 40 0.25\n",
      "Cluster:  8 Label:  240\n",
      "4A 1 9 40 0.775\n",
      "4B 1 4 40 0.9\n",
      "Cluster:  8 Label:  241\n",
      "4A 1 31 40 0.22499999999999998\n",
      "4B 1 28 40 0.30000000000000004\n",
      "Cluster:  8 Label:  242\n",
      "4A 1 35 40 0.125\n",
      "4B 1 29 40 0.275\n",
      "Cluster:  8 Label:  243\n",
      "4A 1 33 40 0.17500000000000004\n",
      "4B 1 28 40 0.30000000000000004\n",
      "Cluster:  8 Label:  244\n",
      "4A 1 22 40 0.44999999999999996\n",
      "4B 1 23 40 0.42500000000000004\n",
      "Cluster:  8 Label:  245\n",
      "4A 1 34 40 0.15000000000000002\n",
      "4B 1 32 40 0.19999999999999996\n",
      "Cluster:  8 Label:  246\n",
      "4A 1 0 40 1.0\n",
      "4B 1 10 40 0.75\n",
      "Cluster:  8 Label:  247\n",
      "4A 1 31 40 0.22499999999999998\n",
      "4B 1 28 40 0.30000000000000004\n",
      "Cluster:  8 Label:  248\n",
      "4A 1 33 40 0.17500000000000004\n",
      "4B 1 30 40 0.25\n",
      "Cluster:  8 Label:  249\n",
      "4A 1 8 40 0.8\n",
      "4B 1 11 40 0.725\n",
      "Cluster:  8 Label:  250\n",
      "4A 1 36 40 0.09999999999999998\n",
      "4B 1 31 40 0.22499999999999998\n",
      "Cluster:  8 Label:  251\n",
      "4A 1 27 40 0.32499999999999996\n",
      "4B 1 30 40 0.25\n",
      "Cluster:  8 Label:  252\n",
      "4A 1 35 40 0.125\n",
      "4B 1 31 40 0.22499999999999998\n",
      "Cluster:  8 Label:  253\n",
      "4A 1 32 40 0.19999999999999996\n",
      "4B 1 26 40 0.35\n",
      "Cluster:  8 Label:  254\n",
      "4A 1 32 40 0.19999999999999996\n",
      "4B 1 32 40 0.19999999999999996\n",
      "Cluster:  8 Label:  255\n",
      "4A 1 23 40 0.42500000000000004\n",
      "4B 1 27 40 0.32499999999999996\n",
      "Perfoming Fold:  2\n"
     ]
    }
   ],
   "source": [
    "#%%time\n",
    "#Version 1 - Reading pkl files from step 0 and clustering it{\n",
    "data_path = '../../data/'+folder+'/'\n",
    "classes = ['4A','4B']\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "import gc\n",
    "result= {}\n",
    "\n",
    "k = 4 #Total Number of folds\n",
    "fold = 1\n",
    "\n",
    "for i in range(k): \n",
    "    print('Perfoming Fold: ', fold)\n",
    "    clf_result = {}\n",
    "    \n",
    "    if os.path.exists('../../data/'+folder+'/kmeans_first_'+str(fold)+'_'+model_name+'.pkl'):\n",
    "        with open('../../data/'+folder+'/kmeans_first_'+str(fold)+'_'+model_name+'.pkl',\"rb\") as f:\n",
    "            X_new,pred_kmeans,kmeans = pickle.load(f)\n",
    "    else:   \n",
    "        with open(data_path+classes[0]+'_fold_'+str(fold)+'_train_'+model_name+'.pkl','rb') as f:\n",
    "            X_fold = pickle.load(f)\n",
    "        with open(data_path+classes[1]+'_fold_'+str(fold)+'_train_'+model_name+'.pkl','rb') as f:\n",
    "            y_fold = pickle.load(f)\n",
    "\n",
    "        X = np.column_stack((X_fold,y_fold))\n",
    "        kmeans = MiniBatchKMeans(n_clusters=57255,random_state=42).fit(X) #100x reduction\n",
    "        #print kmeans.cluster_centers_\n",
    "        pred_kmeans = kmeans.predict(X)\n",
    "        X_new = kmeans.cluster_centers_\n",
    "\n",
    "        with open('../../data/'+folder+'/kmeans_first_'+str(fold)+'_'+model_name+'.pkl', 'wb') as handle:\n",
    "            pickle.dump([X_new,pred_kmeans,kmeans], handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    #DO CLUSTERING AND GET CLUSTERS\n",
    "    \n",
    "    from sklearn.cluster import KMeans\n",
    "\n",
    "    from sklearn.mixture import GaussianMixture\n",
    "    \n",
    "    method ='GMM'\n",
    "    print(method)\n",
    "    for j in range(1,14,1):\n",
    "        if j == 7 or j == 8:\n",
    "            pass\n",
    "        else:\n",
    "            continue\n",
    "        clf_result[j] = {}\n",
    "\n",
    "        # clf = KMeans(n_clusters=j)    \n",
    "        clf = GaussianMixture(n_components=2**j, covariance_type='full', random_state=42)\n",
    "        y_pred = clf.fit_predict(X_new)\n",
    "        #print clf.cluster_centers_\n",
    "\n",
    "        for label in set(y_pred):\n",
    "            print('Cluster: ',j,'Label: ', label)\n",
    "            \n",
    "            #Lesioning and measuring performance\n",
    "            pred = y_pred.copy()\n",
    "            loc = np.where(pred==label)\n",
    "            loc_temp = kmeans.predict(X_new[loc[0]])\n",
    "            loc_new =[]\n",
    "            for entry in set(loc_temp):\n",
    "                temp = np.where(pred_kmeans==entry)[0]\n",
    "                loc_new.extend(temp)\n",
    "\n",
    "            lambda_mask = np.ones(shape=((5725552,)))   \n",
    "            lambda_mask[loc_new] = 0.\n",
    "\n",
    "            #plt.scatter(X[:,0],X[:,1], c=y_pred) \n",
    "            #Change Model\n",
    "            model = MobileNetV2(include_top=True,\n",
    "                                weights=\"imagenet\",\n",
    "                                input_tensor=None,\n",
    "                                input_shape=None,\n",
    "                                alpha = 0.35,\n",
    "                                pooling=None,\n",
    "                                classes=1000,\n",
    "                                lambda_mask=lambda_mask,\n",
    "                                classifier_activation=\"softmax\")\n",
    "            flag = 0\n",
    "            dprime = 0.\n",
    "            for p in classes:\n",
    "                im_valid_test = []\n",
    "                image_list_valid = '../../data/'+folder+'/'+p+'_image_list_valid_fold_'+str(fold)+'.txt'\n",
    "                with open(image_list_valid,'r') as f:\n",
    "                    for line in f.readlines():\n",
    "                        im_valid_test.append(line.strip('\\n'))\n",
    "                im_temp = preprocess_image_batch(im_valid_test,img_size=(256,256), crop_size=(224,224), color_mode=\"rgb\")\n",
    "                out = model.predict(im_temp,batch_size=64)\n",
    "\n",
    "                true_valid_wids = []\n",
    "                for i in im_valid_test:\n",
    "                        temp1 = i.split('/')[4]\n",
    "                        temp = temp1.split('.')[0].split('_')[2]\n",
    "                        true_valid_wids.append(truth[int(temp)][1])\n",
    "\n",
    "                predicted_valid_wids = []\n",
    "                for i in range(len(im_valid_test)):\n",
    "                    #print im_list[i], pprint_output(out[i]), true_wids[i]\n",
    "                    predicted_valid_wids.append(pprint_output(out[i]))\n",
    "\n",
    "                count, error  = top5accuracy(true_valid_wids, predicted_valid_wids)\n",
    "                print(str(p)+' '+str(fold)+' '+str(count)+' '+str(len(im_valid_test))+' '+str(error))\n",
    "                \n",
    "                if flag == 0:\n",
    "                    dprime = error\n",
    "                    flag = 1\n",
    "                else:\n",
    "                    dprime -= error\n",
    "                    \n",
    "            keras.backend.clear_session()\n",
    "            gc.collect()\n",
    "            del model\n",
    "            clf_result[j][label] = [dprime,loc_new]\n",
    "    \n",
    "    with open('../../data/'+folder+'/'+str(method)+'_multi_scree_fold_'+str(fold)+'_'+model_name+'.pkl', 'wb') as handle:\n",
    "        pickle.dump(clf_result, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "    result[fold] = clf_result\n",
    "    fold += 1\n",
    "#}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading the pickle files\n",
    "model_name = \"MobileNetV2\"\n",
    "method ='GMM'\n",
    "\n",
    "k = 4\n",
    "result ={}\n",
    "for i in range(1,k+1,1):\n",
    "    name = '../../data/pkl_mobile/'+str(method)+'_multi_scree_fold_'+str(i)+'_'+model_name+'.pkl'   #CHANGE\n",
    "    with open(name,\"rb\") as f:\n",
    "        result[i] = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = list(result[3][8].values())\n",
    "max(p), min(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = 1\n",
    "clf_result = result[f]\n",
    "from scipy.signal import savgol_filter\n",
    "\n",
    "fig = plt.figure(1,figsize=(9,9))\n",
    "X = range(1,14,1)\n",
    "xticks = []\n",
    "#X = range(2,51,1)\n",
    "anat = []\n",
    "inat = []\n",
    "for cl in X:\n",
    "    xticks.append(2**cl)\n",
    "    i = 0\n",
    "    temp = []\n",
    "    for item in clf_result[cl].keys():\n",
    "        plt.plot(cl,clf_result[cl][item],'go',color='grey',alpha=0.2)\n",
    "        temp.append(clf_result[cl][item])\n",
    "        i += 1\n",
    "    anat.append(np.max(temp))\n",
    "    inat.append(np.min(temp))\n",
    "\n",
    "ahat = savgol_filter(anat, 5, 3)\n",
    "ihat =  savgol_filter(inat, 5, 3)       \n",
    "plt.plot(X,ahat, color='b',linewidth=2)\n",
    "plt.plot(X,ihat, color='C1',linewidth=2)\n",
    "plt.ylim([-1,1])\n",
    "plt.xlim([1,14])\n",
    "plt.xticks(X,xticks)\n",
    "plt.xlabel('Number of cluster(s) k')\n",
    "plt.ylabel(\"Maximum Potential Performance Impact\")\n",
    "plt.title('Cluster Impact Plot (Animate vs Inanimate) for fold '+ str(f))\n",
    "plt.axvline(x=23 , color='black', linestyle='--',alpha=0.3)\n",
    "#plt.gca().xaxis.set_major_locator(MaxNLocator(prune='lower'))\n",
    "\n",
    "plt.savefig('../../results/'+str(method)+'_results_fold_'+str(f)+'.png', format='png', dpi=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.ticker import MaxNLocator\n",
    "f = 1\n",
    "clf_result = result[f]\n",
    "\n",
    "\n",
    "fig = plt.figure(1)\n",
    "X = range(1,31,1)\n",
    "#X = range(2,51,1)\n",
    "for cl in X:\n",
    "    i = 0\n",
    "    for item in clf_result[cl].keys():\n",
    "        plt.plot(cl,clf_result[cl][item],'ro')\n",
    "        i += 1\n",
    "        \n",
    "plt.xticks(X)\n",
    "plt.ylim([-1,1])\n",
    "plt.xlabel('Number of cluster(s) k')\n",
    "plt.ylabel(\"Performance Impact\")\n",
    "plt.title('Performance Impact(Animate vs Inanimate) '+ str(f))\n",
    "plt.gca().xaxis.set_major_locator(MaxNLocator(prune='lower'))\n",
    "#plt.savefig('../../results/'+str(method)+'_results_fold_'+str(f)+'.eps', format='eps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = 2\n",
    "clf_result = result[f]\n",
    "\n",
    "\n",
    "fig = plt.figure(1)\n",
    "X = range(1,51,1)\n",
    "#X = range(2,51,1)\n",
    "for cl in X:\n",
    "    i = 0\n",
    "    for item in clf_result[cl].keys():\n",
    "        plt.plot(cl,clf_result[cl][item],'ro')\n",
    "        i += 1\n",
    "        \n",
    "plt.xticks(X)\n",
    "plt.xlabel('Number of cluster(s) k')\n",
    "plt.ylabel(\"Performance Impact(Animate vs Inanimate)\")\n",
    "plt.title('Scree Plot for fold '+ str(f))\n",
    "plt.gca().xaxis.set_major_locator(MaxNLocator(prune='lower'))\n",
    "#plt.savefig('../../results/scree/'+str(method)+'_results_fold_'+str(f)+'.png', format='png', dpi=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = 3\n",
    "clf_result = result[f]\n",
    "\n",
    "\n",
    "fig = plt.figure(1)\n",
    "X = range(1,51,1)\n",
    "#X = range(2,51,1)\n",
    "for cl in X:\n",
    "    i = 0\n",
    "    for item in clf_result[cl].keys():\n",
    "        plt.plot(cl,clf_result[cl][item],'ro')\n",
    "        i += 1\n",
    "        \n",
    "plt.xticks(X)\n",
    "plt.xlabel('Number of cluster(s) k')\n",
    "plt.ylabel(\"Performance Impact(Animate vs Inanimate)\")\n",
    "plt.title('Scree Plot for fold '+ str(f))\n",
    "plt.gca().xaxis.set_major_locator(MaxNLocator(prune='lower'))\n",
    "#plt.savefig('../../results/scree/'+str(method)+'_results_fold_'+str(f)+'.png', format='png', dpi=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = 4\n",
    "clf_result = result[f]\n",
    "\n",
    "\n",
    "fig = plt.figure(1)\n",
    "X = range(1,51,1)\n",
    "#X = range(2,51,1)\n",
    "for cl in X:\n",
    "    i = 0\n",
    "    for item in clf_result[cl].keys():\n",
    "        plt.plot(cl,clf_result[cl][item],'ro')\n",
    "        i += 1\n",
    "        \n",
    "plt.xticks(X)\n",
    "plt.xlabel('Number of cluster(s) k')\n",
    "plt.ylabel(\"Performance Impact(Animate vs Inanimate)\")\n",
    "plt.title('Scree Plot for fold '+ str(f))\n",
    "plt.gca().xaxis.set_major_locator(MaxNLocator(prune='lower'))\n",
    "#plt.savefig('../../results/scree/'+str(method)+'_results_fold_'+str(f)+'.png', format='png', dpi=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find MaxAd', MaxId' and its average\n",
    "plt.figure()\n",
    "noc = 1\n",
    "for i in range(1,noc+1,1):\n",
    "    X = []\n",
    "    Y = []\n",
    "    for j in range(1,31,1):\n",
    "        X.append(j)\n",
    "        temp = []\n",
    "        for key, value in result[i][j].items():\n",
    "            temp.append(value)\n",
    "        maxa = max(temp)\n",
    "        maxi = min(temp)\n",
    "        avg = float(maxa - maxi)\n",
    "        Y.append(avg)\n",
    "    #print X,Y\n",
    "    plt.plot(X,Y)\n",
    "print(Y.index(max(Y)), Y[8], Y[28])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Smooth average graph\n",
    "from scipy.interpolate import spline\n",
    "noc = 4\n",
    "flag = 0\n",
    "X = range(2,51,1)\n",
    "an_fold =[]\n",
    "ian_fold = []\n",
    "Y = []\n",
    "for i in range(1,noc+1,1):\n",
    "    if i == 2:\n",
    "        flag = 1\n",
    "    for j in range(2,51,1):\n",
    "        temp = []\n",
    "        for key, value in result[i][j].items():\n",
    "            temp.append(value)\n",
    "        maxa = max(temp)\n",
    "        maxi = min(temp)\n",
    "        if flag == 0:\n",
    "            an_fold.append(maxa)\n",
    "            ian_fold.append(maxi)\n",
    "        else:\n",
    "            an_fold[j-2] += maxa\n",
    "            ian_fold[j-2] = maxi\n",
    "\n",
    "for j in range(2,51,1):\n",
    "    maxa = (an_fold[j-2]) / 4.\n",
    "    maxi = (ian_fold[j-2]) /4.\n",
    "    diff = maxa - maxi\n",
    "    Y.append(diff)\n",
    "    \n",
    "x_sm = np.array(X)\n",
    "y_sm = np.array(Y)\n",
    "\n",
    "x_smooth = np.linspace(x_sm.min(), x_sm.max(), 200)\n",
    "y_smooth = spline(X, Y, x_smooth)\n",
    "\n",
    "plt.plot(x_smooth, y_smooth, 'r', linewidth=1)\n",
    "plt.plot(Y.index(max(Y))+1,max(Y),'o')\n",
    "plt.xlabel('Number of cluster(s) k')\n",
    "plt.ylabel(\"Average Performance\")\n",
    "plt.savefig('../../results/scree/'+str(method)+'_results_fold_avg.png', format='png', dpi=200)\n",
    "print(max(Y), Y.index(max(Y)) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37",
   "language": "python",
   "name": "py37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
