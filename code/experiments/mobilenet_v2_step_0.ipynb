{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#To hide warnings export PYTHONWARNINGS=\"ignore\"\n",
    "#Imports{\n",
    "\n",
    "import os\n",
    "from os.path import dirname\n",
    "from os.path import join\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' #Cha\n",
    "\n",
    "\n",
    "import sys\n",
    "import warnings\n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    \n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from imageio import imread\n",
    "from skimage.transform import resize\n",
    "from scipy.io import loadmat\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "#K.set_image_dim_ordering('tf')\n",
    "from keras_applications import imagenet_utils as utils\n",
    "from keras_applications import correct_pad\n",
    "from keras.layers import Lambda\n",
    "\n",
    "import PIL.Image\n",
    "#}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code snippet needed to read activation values from each layer of the pre-trained artificial neural networks\n",
    "def get_activations(model, layer, X_batch):\n",
    "    #keras.backend.function(inputs, outputs, updates=None)\n",
    "    get_activations = keras.backend.function([model.layers[0].input, keras.backend.learning_phase()], [model.layers[layer].output,])\n",
    "    #The learning phase flag is a bool tensor (0 = test, 1 = train)\n",
    "    activations = get_activations([X_batch,0])\n",
    "    return activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'channels_last'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "backend= keras.backend\n",
    "layers = keras.layers\n",
    "models = keras.models\n",
    "keras_utils = keras.utils\n",
    "keras.backend.image_data_format()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"MobileNet v2 models for Keras.\n",
    "\n",
    "MobileNetV2 is a general architecture and can be used for multiple use cases.\n",
    "Depending on the use case, it can use different input layer size and\n",
    "different width factors. This allows different width models to reduce\n",
    "the number of multiply-adds and thereby\n",
    "reduce inference cost on mobile devices.\n",
    "\n",
    "MobileNetV2 is very similar to the original MobileNet,\n",
    "except that it uses inverted residual blocks with\n",
    "bottlenecking features. It has a drastically lower\n",
    "parameter count than the original MobileNet.\n",
    "MobileNets support any input size greater\n",
    "than 32 x 32, with larger image sizes\n",
    "offering better performance.\n",
    "\n",
    "The number of parameters and number of multiply-adds\n",
    "can be modified by using the `alpha` parameter,\n",
    "which increases/decreases the number of filters in each layer.\n",
    "By altering the image size and `alpha` parameter,\n",
    "all 22 models from the paper can be built, with ImageNet weights provided.\n",
    "\n",
    "The paper demonstrates the performance of MobileNets using `alpha` values of\n",
    "1.0 (also called 100 % MobileNet), 0.35, 0.5, 0.75, 1.0, 1.3, and 1.4\n",
    "\n",
    "For each of these `alpha` values, weights for 5 different input image sizes\n",
    "are provided (224, 192, 160, 128, and 96).\n",
    "\n",
    "\n",
    "The following table describes the performance of\n",
    "MobileNet on various input sizes:\n",
    "------------------------------------------------------------------------\n",
    "MACs stands for Multiply Adds\n",
    "\n",
    " Classification Checkpoint| MACs (M) | Parameters (M)| Top 1 Accuracy| Top 5 Accuracy\n",
    "--------------------------|------------|---------------|---------|----|-------------\n",
    "| [mobilenet_v2_1.4_224]  | 582 | 6.06 |          75.0 | 92.5 |\n",
    "| [mobilenet_v2_1.3_224]  | 509 | 5.34 |          74.4 | 92.1 |\n",
    "| [mobilenet_v2_1.0_224]  | 300 | 3.47 |          71.8 | 91.0 |\n",
    "| [mobilenet_v2_1.0_192]  | 221 | 3.47 |          70.7 | 90.1 |\n",
    "| [mobilenet_v2_1.0_160]  | 154 | 3.47 |          68.8 | 89.0 |\n",
    "| [mobilenet_v2_1.0_128]  | 99  | 3.47 |          65.3 | 86.9 |\n",
    "| [mobilenet_v2_1.0_96]   | 56  | 3.47 |          60.3 | 83.2 |\n",
    "| [mobilenet_v2_0.75_224] | 209 | 2.61 |          69.8 | 89.6 |\n",
    "| [mobilenet_v2_0.75_192] | 153 | 2.61 |          68.7 | 88.9 |\n",
    "| [mobilenet_v2_0.75_160] | 107 | 2.61 |          66.4 | 87.3 |\n",
    "| [mobilenet_v2_0.75_128] | 69  | 2.61 |          63.2 | 85.3 |\n",
    "| [mobilenet_v2_0.75_96]  | 39  | 2.61 |          58.8 | 81.6 |\n",
    "| [mobilenet_v2_0.5_224]  | 97  | 1.95 |          65.4 | 86.4 |\n",
    "| [mobilenet_v2_0.5_192]  | 71  | 1.95 |          63.9 | 85.4 |\n",
    "| [mobilenet_v2_0.5_160]  | 50  | 1.95 |          61.0 | 83.2 |\n",
    "| [mobilenet_v2_0.5_128]  | 32  | 1.95 |          57.7 | 80.8 |\n",
    "| [mobilenet_v2_0.5_96]   | 18  | 1.95 |          51.2 | 75.8 |\n",
    "| [mobilenet_v2_0.35_224] | 59  | 1.66 |          60.3 | 82.9 |\n",
    "| [mobilenet_v2_0.35_192] | 43  | 1.66 |          58.2 | 81.2 |\n",
    "| [mobilenet_v2_0.35_160] | 30  | 1.66 |          55.7 | 79.1 |\n",
    "| [mobilenet_v2_0.35_128] | 20  | 1.66 |          50.8 | 75.0 |\n",
    "| [mobilenet_v2_0.35_96]  | 11  | 1.66 |          45.5 | 70.4 |\n",
    "\n",
    "The weights for all 16 models are obtained and\n",
    "translated from the Tensorflow checkpoints\n",
    "from TensorFlow checkpoints found [here]\n",
    "(https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet/README.md).\n",
    "\n",
    "# Reference\n",
    "\n",
    "This file contains building code for MobileNetV2, based on\n",
    "[MobileNetV2: Inverted Residuals and Linear Bottlenecks]\n",
    "(https://arxiv.org/abs/1801.04381) (CVPR 2018)\n",
    "\n",
    "Tests comparing this model to the existing Tensorflow model can be\n",
    "found at [mobilenet_v2_keras]\n",
    "(https://github.com/JonathanCMitchell/mobilenet_v2_keras)\n",
    "\"\"\"\n",
    "from __future__ import print_function\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "\n",
    "import os\n",
    "import warnings\n",
    "import numpy as np\n",
    "\n",
    "# TODO Change path to v1.1\n",
    "BASE_WEIGHT_PATH = ('https://github.com/JonathanCMitchell/mobilenet_v2_keras/'\n",
    "                    'releases/download/v1.1/')\n",
    "\n",
    "backend= keras.backend\n",
    "layers = keras.layers\n",
    "models = keras.models\n",
    "keras_utils = keras.utils\n",
    "\n",
    "\n",
    "def preprocess_input(x, **kwargs):\n",
    "    \"\"\"Preprocesses a numpy array encoding a batch of images.\n",
    "\n",
    "    # Arguments\n",
    "        x: a 4D numpy array consists of RGB values within [0, 255].\n",
    "\n",
    "    # Returns\n",
    "        Preprocessed array.\n",
    "    \"\"\"\n",
    "    return utils.preprocess_input(x, mode='tf', **kwargs)\n",
    "\n",
    "\n",
    "# This function is taken from the original tf repo.\n",
    "# It ensures that all layers have a channel number that is divisible by 8\n",
    "# It can be seen here:\n",
    "# https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet/mobilenet.py\n",
    "\n",
    "\n",
    "def _make_divisible(v, divisor, min_value=None):\n",
    "    if min_value is None:\n",
    "        min_value = divisor\n",
    "    new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)\n",
    "    # Make sure that round down does not go down by more than 10%.\n",
    "    if new_v < 0.9 * v:\n",
    "        new_v += divisor\n",
    "    return new_v\n",
    "\n",
    "\n",
    "def MobileNetV2(input_shape=None,\n",
    "                alpha=1.0,\n",
    "                include_top=True,\n",
    "                weights='imagenet',\n",
    "                input_tensor=None,\n",
    "                pooling=None,\n",
    "                classes=1000,\n",
    "                lambda_mask = None,\n",
    "                **kwargs):\n",
    "    \"\"\"Instantiates the MobileNetV2 architecture.\n",
    "\n",
    "    # Arguments\n",
    "        input_shape: optional shape tuple, to be specified if you would\n",
    "            like to use a model with an input img resolution that is not\n",
    "            (224, 224, 3).\n",
    "            It should have exactly 3 inputs channels (224, 224, 3).\n",
    "            You can also omit this option if you would like\n",
    "            to infer input_shape from an input_tensor.\n",
    "            If you choose to include both input_tensor and input_shape then\n",
    "            input_shape will be used if they match, if the shapes\n",
    "            do not match then we will throw an error.\n",
    "            E.g. `(160, 160, 3)` would be one valid value.\n",
    "        alpha: controls the width of the network. This is known as the\n",
    "        width multiplier in the MobileNetV2 paper, but the name is kept for\n",
    "        consistency with MobileNetV1 in Keras.\n",
    "            - If `alpha` < 1.0, proportionally decreases the number\n",
    "                of filters in each layer.\n",
    "            - If `alpha` > 1.0, proportionally increases the number\n",
    "                of filters in each layer.\n",
    "            - If `alpha` = 1, default number of filters from the paper\n",
    "                 are used at each layer.\n",
    "        include_top: whether to include the fully-connected\n",
    "            layer at the top of the network.\n",
    "        weights: one of `None` (random initialization),\n",
    "              'imagenet' (pre-training on ImageNet),\n",
    "              or the path to the weights file to be loaded.\n",
    "        input_tensor: optional Keras tensor (i.e. output of\n",
    "            `layers.Input()`)\n",
    "            to use as image input for the model.\n",
    "        pooling: Optional pooling mode for feature extraction\n",
    "            when `include_top` is `False`.\n",
    "            - `None` means that the output of the model\n",
    "                will be the 4D tensor output of the\n",
    "                last convolutional block.\n",
    "            - `avg` means that global average pooling\n",
    "                will be applied to the output of the\n",
    "                last convolutional block, and thus\n",
    "                the output of the model will be a\n",
    "                2D tensor.\n",
    "            - `max` means that global max pooling will\n",
    "                be applied.\n",
    "        classes: optional number of classes to classify images\n",
    "            into, only to be specified if `include_top` is True, and\n",
    "            if no `weights` argument is specified.\n",
    "\n",
    "    # Returns\n",
    "        A Keras model instance.\n",
    "\n",
    "    # Raises\n",
    "        ValueError: in case of invalid argument for `weights`,\n",
    "            or invalid input shape or invalid alpha, rows when\n",
    "            weights='imagenet'\n",
    "    \"\"\"\n",
    "    global backend, layers, models, keras_utils, debug\n",
    "    debug = True\n",
    "    backend= keras.backend\n",
    "    layers = keras.layers\n",
    "    models = keras.models\n",
    "    keras_utils = keras.utils\n",
    "\n",
    "    if not (weights in {'imagenet', None} or os.path.exists(weights)):\n",
    "        raise ValueError('The `weights` argument should be either '\n",
    "                         '`None` (random initialization), `imagenet` '\n",
    "                         '(pre-training on ImageNet), '\n",
    "                         'or the path to the weights file to be loaded.')\n",
    "\n",
    "    if weights == 'imagenet' and include_top and classes != 1000:\n",
    "        raise ValueError('If using `weights` as `\"imagenet\"` with `include_top` '\n",
    "                         'as true, `classes` should be 1000')\n",
    "\n",
    "    # Determine proper input shape and default size.\n",
    "    # If both input_shape and input_tensor are used, they should match\n",
    "    if input_shape is not None and input_tensor is not None:\n",
    "        try:\n",
    "            is_input_t_tensor = backend.is_keras_tensor(input_tensor)\n",
    "        except ValueError:\n",
    "            try:\n",
    "                is_input_t_tensor = backend.is_keras_tensor(\n",
    "                    keras_utils.get_source_inputs(input_tensor))\n",
    "            except ValueError:\n",
    "                raise ValueError('input_tensor: ', input_tensor,\n",
    "                                 'is not type input_tensor')\n",
    "        if is_input_t_tensor:\n",
    "            if backend.image_data_format == 'channels_first':\n",
    "                if backend.int_shape(input_tensor)[1] != input_shape[1]:\n",
    "                    raise ValueError('input_shape: ', input_shape,\n",
    "                                     'and input_tensor: ', input_tensor,\n",
    "                                     'do not meet the same shape requirements')\n",
    "            else:\n",
    "                if backend.int_shape(input_tensor)[2] != input_shape[1]:\n",
    "                    raise ValueError('input_shape: ', input_shape,\n",
    "                                     'and input_tensor: ', input_tensor,\n",
    "                                     'do not meet the same shape requirements')\n",
    "        else:\n",
    "            raise ValueError('input_tensor specified: ', input_tensor,\n",
    "                             'is not a keras tensor')\n",
    "\n",
    "    # If input_shape is None, infer shape from input_tensor\n",
    "    if input_shape is None and input_tensor is not None:\n",
    "\n",
    "        try:\n",
    "            backend.is_keras_tensor(input_tensor)\n",
    "        except ValueError:\n",
    "            raise ValueError('input_tensor: ', input_tensor,\n",
    "                             'is type: ', type(input_tensor),\n",
    "                             'which is not a valid type')\n",
    "\n",
    "        if input_shape is None and not backend.is_keras_tensor(input_tensor):\n",
    "            default_size = 224\n",
    "        elif input_shape is None and backend.is_keras_tensor(input_tensor):\n",
    "            if backend.image_data_format() == 'channels_first':\n",
    "                rows = backend.int_shape(input_tensor)[2]\n",
    "                cols = backend.int_shape(input_tensor)[3]\n",
    "            else:\n",
    "                rows = backend.int_shape(input_tensor)[1]\n",
    "                cols = backend.int_shape(input_tensor)[2]\n",
    "\n",
    "            if rows == cols and rows in [96, 128, 160, 192, 224]:\n",
    "                default_size = rows\n",
    "            else:\n",
    "                default_size = 224\n",
    "\n",
    "    # If input_shape is None and no input_tensor\n",
    "    elif input_shape is None:\n",
    "        default_size = 224\n",
    "\n",
    "    # If input_shape is not None, assume default size\n",
    "    else:\n",
    "        if backend.image_data_format() == 'channels_first':\n",
    "            rows = input_shape[1]\n",
    "            cols = input_shape[2]\n",
    "        else:\n",
    "            rows = input_shape[0]\n",
    "            cols = input_shape[1]\n",
    "\n",
    "        if rows == cols and rows in [96, 128, 160, 192, 224]:\n",
    "            default_size = rows\n",
    "        else:\n",
    "            default_size = 224\n",
    "\n",
    "    input_shape = utils._obtain_input_shape(input_shape,\n",
    "                                      default_size=default_size,\n",
    "                                      min_size=32,\n",
    "                                      data_format=backend.image_data_format(),\n",
    "                                      require_flatten=include_top,\n",
    "                                      weights=weights)\n",
    "\n",
    "    if backend.image_data_format() == 'channels_last':\n",
    "        row_axis, col_axis = (0, 1)\n",
    "    else:\n",
    "        row_axis, col_axis = (1, 2)\n",
    "    rows = input_shape[row_axis]\n",
    "    cols = input_shape[col_axis]\n",
    "\n",
    "    if weights == 'imagenet':\n",
    "        if alpha not in [0.35, 0.50, 0.75, 1.0, 1.3, 1.4]:\n",
    "            raise ValueError('If imagenet weights are being loaded, '\n",
    "                             'alpha can be one of `0.35`, `0.50`, `0.75`, '\n",
    "                             '`1.0`, `1.3` or `1.4` only.')\n",
    "\n",
    "        if rows != cols or rows not in [96, 128, 160, 192, 224]:\n",
    "            rows = 224\n",
    "            warnings.warn('`input_shape` is undefined or non-square, '\n",
    "                          'or `rows` is not in [96, 128, 160, 192, 224].'\n",
    "                          ' Weights for input shape (224, 224) will be'\n",
    "                          ' loaded as the default.')\n",
    "\n",
    "    if input_tensor is None:\n",
    "        img_input = layers.Input(shape=input_shape)\n",
    "    else:\n",
    "        if not backend.is_keras_tensor(input_tensor):\n",
    "            img_input = layers.Input(tensor=input_tensor, shape=input_shape)\n",
    "        else:\n",
    "            img_input = input_tensor\n",
    "\n",
    "    channel_axis = 1 if backend.image_data_format() == 'channels_first' else -1\n",
    "\n",
    "    first_block_filters = _make_divisible(32 * alpha, 8)\n",
    "    x = layers.ZeroPadding2D(padding=correct_pad(backend, img_input, 3),\n",
    "                             name='Conv1_pad')(img_input)\n",
    "    x = layers.Conv2D(first_block_filters,\n",
    "                      kernel_size=3,\n",
    "                      strides=(2, 2),\n",
    "                      padding='valid',\n",
    "                      use_bias=False,\n",
    "                      name='Conv1')(x)\n",
    "    global start_index, end_index\n",
    "    start_index = end_index = 0\n",
    "    #################\n",
    "    if lambda_mask is not None:\n",
    "        start_index = end_index\n",
    "        end_index = start_index + (default_size//2 * default_size//2 * first_block_filters)\n",
    "        x_mask  = np.reshape(lambda_mask[start_index:end_index], (default_size//2, default_size//2, first_block_filters))\n",
    "        if debug:\n",
    "            print('Conv_1',start_index,end_index)\n",
    "    else:\n",
    "        x_mask = np.ones(shape=((default_size//2, default_size//2, first_block_filters)))\n",
    "\n",
    "    x_mask  = backend.variable(x_mask)\n",
    "    x = Lambda(lambda z: z * x_mask)(x)\n",
    "    ####################\n",
    "    x = layers.BatchNormalization(axis=channel_axis,\n",
    "                                  epsilon=1e-3,\n",
    "                                  momentum=0.999,\n",
    "                                  name='bn_Conv1')(x)\n",
    "    #################\n",
    "    if lambda_mask is not None:\n",
    "        start_index = end_index\n",
    "        end_index = start_index + (default_size//2 * default_size//2 * first_block_filters)\n",
    "        x_mask  = np.reshape(lambda_mask[start_index:end_index], (default_size//2, default_size//2, first_block_filters))\n",
    "        if debug:\n",
    "            print('Conv_1_BN',start_index,end_index)\n",
    "    else:\n",
    "        x_mask = np.ones(shape=((default_size//2, default_size//2, first_block_filters)))\n",
    "\n",
    "    x_mask  = backend.variable(x_mask)\n",
    "    x = Lambda(lambda z: z * x_mask)(x)\n",
    "    ####################\n",
    "    x = layers.ReLU(6., name='Conv1_relu')(x)\n",
    "\n",
    "    x = _inverted_res_block(x, filters=16, alpha=alpha, stride=1,\n",
    "                            expansion=1, block_id=0, lambda_mask=lambda_mask)\n",
    "\n",
    "    x = _inverted_res_block(x, filters=24, alpha=alpha, stride=2,\n",
    "                            expansion=6, block_id=1, lambda_mask=lambda_mask)\n",
    "    x = _inverted_res_block(x, filters=24, alpha=alpha, stride=1,\n",
    "                            expansion=6, block_id=2, lambda_mask=lambda_mask)\n",
    "\n",
    "    x = _inverted_res_block(x, filters=32, alpha=alpha, stride=2,\n",
    "                            expansion=6, block_id=3, lambda_mask=lambda_mask)\n",
    "    x = _inverted_res_block(x, filters=32, alpha=alpha, stride=1,\n",
    "                            expansion=6, block_id=4, lambda_mask=lambda_mask)\n",
    "    x = _inverted_res_block(x, filters=32, alpha=alpha, stride=1,\n",
    "                            expansion=6, block_id=5, lambda_mask=lambda_mask)\n",
    "\n",
    "    x = _inverted_res_block(x, filters=64, alpha=alpha, stride=2,\n",
    "                            expansion=6, block_id=6, lambda_mask=lambda_mask)\n",
    "    x = _inverted_res_block(x, filters=64, alpha=alpha, stride=1,\n",
    "                            expansion=6, block_id=7, lambda_mask=lambda_mask)\n",
    "    x = _inverted_res_block(x, filters=64, alpha=alpha, stride=1,\n",
    "                            expansion=6, block_id=8, lambda_mask=lambda_mask)\n",
    "    x = _inverted_res_block(x, filters=64, alpha=alpha, stride=1,\n",
    "                            expansion=6, block_id=9, lambda_mask=lambda_mask)\n",
    "\n",
    "    x = _inverted_res_block(x, filters=96, alpha=alpha, stride=1,\n",
    "                            expansion=6, block_id=10, lambda_mask=lambda_mask)\n",
    "    x = _inverted_res_block(x, filters=96, alpha=alpha, stride=1,\n",
    "                            expansion=6, block_id=11, lambda_mask=lambda_mask)\n",
    "    x = _inverted_res_block(x, filters=96, alpha=alpha, stride=1,\n",
    "                            expansion=6, block_id=12, lambda_mask=lambda_mask)\n",
    "\n",
    "    x = _inverted_res_block(x, filters=160, alpha=alpha, stride=2,\n",
    "                            expansion=6, block_id=13, lambda_mask=lambda_mask)\n",
    "    x = _inverted_res_block(x, filters=160, alpha=alpha, stride=1,\n",
    "                            expansion=6, block_id=14, lambda_mask=lambda_mask)\n",
    "    x = _inverted_res_block(x, filters=160, alpha=alpha, stride=1,\n",
    "                            expansion=6, block_id=15, lambda_mask=lambda_mask)\n",
    "\n",
    "    x = _inverted_res_block(x, filters=320, alpha=alpha, stride=1,\n",
    "                            expansion=6, block_id=16, lambda_mask=lambda_mask)\n",
    "\n",
    "    # no alpha applied to last conv as stated in the paper:\n",
    "    # if the width multiplier is greater than 1 we\n",
    "    # increase the number of output channels\n",
    "    if alpha > 1.0:\n",
    "        last_block_filters = _make_divisible(1280 * alpha, 8)\n",
    "    else:\n",
    "        last_block_filters = 1280\n",
    "\n",
    "    x = layers.Conv2D(last_block_filters,\n",
    "                      kernel_size=1,\n",
    "                      use_bias=False,\n",
    "                      name='Conv_1')(x)\n",
    "    #################\n",
    "    if lambda_mask is not None:\n",
    "        start_index = end_index\n",
    "        end_index = start_index + (x.shape[1] * x.shape[2]* last_block_filters)\n",
    "        x_mask  = np.reshape(lambda_mask[start_index:end_index], (x.shape[1], x.shape[2], last_block_filters))\n",
    "        if debug:\n",
    "            print('Conv_1',start_index,end_index)\n",
    "    else:\n",
    "        x_mask = np.ones(shape=((x.shape[1], x.shape[2], last_block_filters)))\n",
    "\n",
    "    x_mask  = backend.variable(x_mask)\n",
    "    x = Lambda(lambda z: z * x_mask)(x)\n",
    "    ####################\n",
    "    x = layers.BatchNormalization(axis=channel_axis,\n",
    "                                  epsilon=1e-3,\n",
    "                                  momentum=0.999,\n",
    "                                  name='Conv_1_bn')(x)\n",
    "    #################\n",
    "    if lambda_mask is not None:\n",
    "        start_index = end_index\n",
    "        end_index = start_index + (x.shape[1] * x.shape[2] * last_block_filters)\n",
    "        x_mask  = np.reshape(lambda_mask[start_index:end_index], (x.shape[1], x.shape[2], last_block_filters))\n",
    "        if debug:\n",
    "            print('Conv_1_bn',start_index,end_index)\n",
    "    else:\n",
    "        x_mask = np.ones(shape=((x.shape[1],x.shape[2], last_block_filters)))\n",
    "\n",
    "    x_mask  = backend.variable(x_mask)\n",
    "    x = Lambda(lambda z: z * x_mask)(x)\n",
    "    ####################\n",
    "    x = layers.ReLU(6., name='out_relu')(x)\n",
    "\n",
    "    if include_top:\n",
    "        x = layers.GlobalAveragePooling2D()(x)\n",
    "        x = layers.Dense(classes, activation='softmax', use_bias=True, name='Logits')(x)\n",
    "    else:\n",
    "        if pooling == 'avg':\n",
    "            x = layers.GlobalAveragePooling2D()(x)\n",
    "        elif pooling == 'max':\n",
    "            x = layers.GlobalMaxPooling2D()(x)\n",
    "\n",
    "    # Ensure that the model takes into account\n",
    "    # any potential predecessors of `input_tensor`.\n",
    "    if input_tensor is not None:\n",
    "        inputs = keras_utils.get_source_inputs(input_tensor)\n",
    "    else:\n",
    "        inputs = img_input\n",
    "\n",
    "    # Create model.\n",
    "    model = models.Model(inputs, x, name='mobilenetv2_%0.2f_%s' % (alpha, rows))\n",
    "\n",
    "    # Load weights.\n",
    "    if weights == 'imagenet':\n",
    "        if include_top:\n",
    "            model_name = ('mobilenet_v2_weights_tf_dim_ordering_tf_kernels_' + str(alpha) + '_' + str(rows) + '.h5')\n",
    "            weight_path = BASE_WEIGHT_PATH + model_name\n",
    "            weights_path = keras_utils.get_file(model_name, weight_path, cache_subdir='models')\n",
    "        else:\n",
    "            model_name = ('mobilenet_v2_weights_tf_dim_ordering_tf_kernels_' + str(alpha) + '_' + str(rows) + '_no_top' + '.h5')\n",
    "            weight_path = BASE_WEIGHT_PATH + model_name\n",
    "            weights_path = keras_utils.get_file(model_name, weight_path, cache_subdir='models')\n",
    "        model.load_weights(weights_path)\n",
    "    elif weights is not None:\n",
    "        model.load_weights(weights)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def _inverted_res_block(inputs, expansion, stride, alpha, filters, block_id, lambda_mask = None):\n",
    "    global debug\n",
    "    channel_axis = 1 if backend.image_data_format() == 'channels_first' else -1\n",
    "    in_channels = backend.int_shape(inputs)[channel_axis]\n",
    "    pointwise_conv_filters = int(filters * alpha)\n",
    "    pointwise_filters = _make_divisible(pointwise_conv_filters, 8)\n",
    "    x = inputs\n",
    "    prefix = 'block_{}_'.format(block_id)\n",
    "    #print(prefix, inputs.shape,inputs.shape[0],inputs.shape[1] , in_channels, pointwise_conv_filters, pointwise_filters, filters)\n",
    "    global start_index, end_index\n",
    "    if block_id:\n",
    "        # Expand\n",
    "        x = layers.Conv2D(expansion * in_channels,\n",
    "                          kernel_size=1,\n",
    "                          padding='same',\n",
    "                          use_bias=False,\n",
    "                          activation=None,\n",
    "                          name=prefix + 'expand')(x)\n",
    "        #################\n",
    "        if lambda_mask is not None:\n",
    "            start_index = end_index\n",
    "            end_index = start_index + (inputs.shape[1] * inputs.shape[2] * inputs.shape[3]*expansion)\n",
    "            x_mask  = np.reshape(lambda_mask[start_index:end_index], (inputs.shape[1], inputs.shape[2], inputs.shape[3]*expansion))\n",
    "            if debug:\n",
    "                print(prefix + 'expand',start_index,end_index)\n",
    "        else:\n",
    "            x_mask = np.ones(shape=((inputs.shape[1], inputs.shape[2], inputs.shape[3]*expansion)))\n",
    "\n",
    "        x_mask  = backend.variable(x_mask)\n",
    "        x = Lambda(lambda z: z * x_mask)(x)\n",
    "        ####################\n",
    "        x = layers.BatchNormalization(axis=channel_axis,\n",
    "                                      epsilon=1e-3,\n",
    "                                      momentum=0.999,\n",
    "                                      name=prefix + 'expand_BN')(x)\n",
    "  \n",
    "        #################\n",
    "        if lambda_mask is not None:\n",
    "            start_index = end_index\n",
    "            end_index = start_index + (inputs.shape[1] * inputs.shape[2] * inputs.shape[3]*expansion)\n",
    "            x_mask  = np.reshape(lambda_mask[start_index:end_index], (inputs.shape[1], inputs.shape[2], inputs.shape[3]*expansion))\n",
    "            if debug:\n",
    "                print(prefix + 'expand_BN',start_index,end_index)\n",
    "        else:\n",
    "            x_mask = np.ones(shape=((inputs.shape[1], inputs.shape[2], inputs.shape[3]*expansion)))\n",
    "\n",
    "        x_mask  = backend.variable(x_mask)\n",
    "        x = Lambda(lambda z: z * x_mask)(x)\n",
    "        ####################\n",
    "        x = layers.ReLU(6., name=prefix + 'expand_relu')(x)\n",
    "    else:\n",
    "        prefix = 'expanded_conv_'\n",
    "\n",
    "    # Depthwise\n",
    "    if stride == 2:\n",
    "        x = layers.ZeroPadding2D(padding=correct_pad(backend, x, 3), name=prefix + 'pad')(x)\n",
    "    x = layers.DepthwiseConv2D(kernel_size=3, strides=stride, activation=None, use_bias=False,padding='same' if stride == 1 else 'valid', name=prefix + 'depthwise')(x)\n",
    "    #################\n",
    "    if lambda_mask is not None:\n",
    "        start_index = end_index\n",
    "        end_index = start_index + (inputs.shape[1]//stride * inputs.shape[2]//stride * inputs.shape[3]*expansion)\n",
    "        x_mask  = np.reshape(lambda_mask[start_index:end_index], (inputs.shape[1]//stride, inputs.shape[2]//stride, inputs.shape[3]*expansion))\n",
    "        if debug:\n",
    "            print(prefix + 'depthwise',start_index,end_index)\n",
    "    else:\n",
    "        x_mask = np.ones(shape=((inputs.shape[1]//stride, inputs.shape[2]//stride, inputs.shape[3]*expansion)))\n",
    "\n",
    "    x_mask  = backend.variable(x_mask)\n",
    "    x = Lambda(lambda z: z * x_mask)(x)\n",
    "    ####################\n",
    "    x = layers.BatchNormalization(axis=channel_axis,epsilon=1e-3,momentum=0.999, name=prefix + 'depthwise_BN')(x)\n",
    "    #################\n",
    "    if lambda_mask is not None:\n",
    "        start_index = end_index\n",
    "        end_index = start_index + (inputs.shape[1]//stride * inputs.shape[2]//stride * inputs.shape[3]*expansion)\n",
    "        x_mask  = np.reshape(lambda_mask[start_index:end_index], (inputs.shape[1]//stride, inputs.shape[2]//stride, inputs.shape[3]*expansion))\n",
    "        if debug:\n",
    "            print(prefix + 'depthwise_BN',start_index,end_index)\n",
    "    else:\n",
    "        x_mask = np.ones(shape=((inputs.shape[1]//stride, inputs.shape[2]//stride, inputs.shape[3]*expansion)))\n",
    "\n",
    "    x_mask  = backend.variable(x_mask)\n",
    "    x = Lambda(lambda z: z * x_mask)(x)\n",
    "    ####################\n",
    "\n",
    "    x = layers.ReLU(6., name=prefix + 'depthwise_relu')(x)\n",
    "\n",
    "    # Project\n",
    "    x = layers.Conv2D(pointwise_filters, kernel_size=1, padding='same', use_bias=False, activation=None, name=prefix + 'project')(x)\n",
    "    #################\n",
    "    if lambda_mask is not None:\n",
    "        start_index = end_index\n",
    "        end_index = start_index + (inputs.shape[1]//stride * inputs.shape[2]//stride * pointwise_filters)\n",
    "        x_mask  = np.reshape(lambda_mask[start_index:end_index], (inputs.shape[1]//stride, inputs.shape[2]//stride, pointwise_filters))\n",
    "        if debug:\n",
    "            print(prefix + 'project',start_index,end_index)\n",
    "    else:\n",
    "        x_mask = np.ones(shape=((inputs.shape[1]//stride, inputs.shape[2]//stride, pointwise_filters)))\n",
    "\n",
    "    x_mask  = backend.variable(x_mask)\n",
    "    x = Lambda(lambda z: z * x_mask)(x)\n",
    "    ####################\n",
    "    x = layers.BatchNormalization(axis=channel_axis, epsilon=1e-3, momentum=0.999, name=prefix + 'project_BN')(x)\n",
    "    #################\n",
    "    if lambda_mask is not None:\n",
    "        start_index = end_index\n",
    "        end_index = start_index + (inputs.shape[1]//stride * inputs.shape[2]//stride * pointwise_filters)\n",
    "        x_mask  = np.reshape(lambda_mask[start_index:end_index], (inputs.shape[1]//stride, inputs.shape[2]//stride, pointwise_filters))\n",
    "        if debug:\n",
    "            print(prefix + 'project_BN',start_index,end_index)\n",
    "    else:\n",
    "        x_mask = np.ones(shape=((inputs.shape[1]//stride, inputs.shape[2]//stride, pointwise_filters)))\n",
    "\n",
    "    x_mask  = backend.variable(x_mask)\n",
    "    x = Lambda(lambda z: z * x_mask)(x)\n",
    "    ####################\n",
    "\n",
    "    if in_channels == pointwise_filters and stride == 1:\n",
    "        return layers.Add(name=prefix + 'add')([inputs, x])\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to pre-process the input image to ensure uniform size and color\n",
    "def preprocess_image_batch(image_paths, img_size=None, crop_size=None, color_mode='rgb', out=None):\n",
    "    \"\"\"\n",
    "    Consistent preprocessing of images batches\n",
    "    :param image_paths: iterable: images to process\n",
    "    :param crop_size: tuple: crop images if specified\n",
    "    :param img_size: tuple: resize images if specified\n",
    "    :param color_mode: Use rgb or change to bgr mode based on type of model you want to use\n",
    "    :param out: append output to this iterable if specified\n",
    "    \"\"\"\n",
    "    img_list = []\n",
    "\n",
    "    for im_path in image_paths:\n",
    "        '''\n",
    "        img = imread(im_path,as_gray=False, pilmode=\"RGB\")\n",
    "        #print im_path\n",
    "        #print img.shape\n",
    "        if img_size:\n",
    "            img = resize(img, img_size)\n",
    "\n",
    "        img = img.astype('float32')\n",
    "        # We normalize the colors (in RGB space) with the empirical means on the training set\n",
    "        img[:, :, 0] -= 123.68\n",
    "        img[:, :, 1] -= 116.779\n",
    "        img[:, :, 2] -= 103.939\n",
    "        # We permute the colors to get them in the BGR order\n",
    "        if color_mode == 'bgr':\n",
    "            img[:, :, [0, 1, 2]] = img[:, :, [2, 1, 0]]\n",
    "        img = img.transpose((2, 0, 1))\n",
    "\n",
    "        if crop_size:\n",
    "            img = img[:, (img_size[0] - crop_size[0]) // 2:(img_size[0] + crop_size[0]) // 2\n",
    "            , (img_size[1] - crop_size[1]) // 2:(img_size[1] + crop_size[1]) // 2]\n",
    "\n",
    "        img_list.append(img)\n",
    "        '''\n",
    "        size = 224\n",
    "        ret = PIL.Image.open(im_path)\n",
    "        ret = ret.resize((size, size))\n",
    "        ret = np.asarray(ret, dtype=np.uint8).astype(np.float32)\n",
    "        if ret.ndim == 2:\n",
    "            ret.resize((size, size, 1))\n",
    "            ret = np.repeat(ret, 3, axis=-1)\n",
    "        #ret = ret.transpose((2, 0, 1))\n",
    "        #ret = np.flip(ret,0)\n",
    "        global backend\n",
    "        x = preprocess_input(ret, \n",
    "            data_format=backend.image_data_format())\n",
    "        img_list.append(x)\n",
    "\n",
    "\n",
    "    try:\n",
    "        img_batch = np.stack(img_list, axis=0)\n",
    "    except:\n",
    "        print(im_path)\n",
    "        raise ValueError('when img_size and crop_size are None, images'\n",
    "                ' in image_paths must have the same shapes.')\n",
    "\n",
    "    if out is not None and hasattr(out, 'append'):\n",
    "        out.append(img_batch)\n",
    "    else:\n",
    "        return img_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to predict the top 5 accuracy\n",
    "def top5accuracy(true, predicted):\n",
    "    assert len(true) == len(predicted)\n",
    "    result = []\n",
    "    flag  = 0\n",
    "    for i in range(len(true)):\n",
    "        flag  = 0\n",
    "        temp = true[i]\n",
    "        for j in predicted[i][0:5]:\n",
    "            if j == temp:\n",
    "                flag = 1\n",
    "                break\n",
    "        if flag == 1:\n",
    "            result.append(1)\n",
    "        else:\n",
    "            result.append(0)\n",
    "    counter = 0.\n",
    "    for i in result:\n",
    "        if i == 1:\n",
    "            counter += 1.\n",
    "    error = 1.0 - counter/float(len(result))\n",
    "    #print len(np.where(np.asarray(result) == 1)[0])\n",
    "    return len(np.where(np.asarray(result) == 1)[0]), error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the details of all the 1000 classes and the function to conver the synset id to words{\n",
    "meta_clsloc_file = '../../data/meta_clsloc.mat'\n",
    "synsets = loadmat(meta_clsloc_file)['synsets'][0]\n",
    "synsets_imagenet_sorted = sorted([(int(s[0]), str(s[1][0])) for s in synsets[:1000]],key=lambda v: v[1])\n",
    "corr = {}\n",
    "for j in range(1000):\n",
    "    corr[synsets_imagenet_sorted[j][0]] = j\n",
    "\n",
    "corr_inv = {}\n",
    "for j in range(1, 1001):\n",
    "    corr_inv[corr[j]] = j\n",
    "\n",
    "def id_to_words(id_):\n",
    "    return synsets[corr_inv[id_] - 1][2][0]\n",
    "\n",
    "def pprint_output(out, n_max_synsets=10):\n",
    "    wids = []\n",
    "    best_ids = out.argsort()[::-1][:10]\n",
    "    for u in best_ids:\n",
    "        wids.append(str(synsets[corr_inv[u] - 1][1][0]))\n",
    "    #print('%.2f' % round(100 * out[u], 2) + ' : ' + id_to_words(u)+' '+ str(synsets[corr_inv[u] - 1][1][0]))\n",
    "    return wids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code snippet to load the ground truth labels to measure the performance{\n",
    "truth = {}\n",
    "with open('../../data/ILSVRC2014_clsloc_validation_ground_truth.txt') as f:\n",
    "    line_num = 1\n",
    "    for line in f.readlines():\n",
    "        ind_ = int(line)\n",
    "        temp  = None\n",
    "        for i in synsets_imagenet_sorted:\n",
    "            if i[0] == ind_:\n",
    "                temp = i\n",
    "        #print ind_,temp\n",
    "        if temp != None:\n",
    "            truth[line_num] = temp\n",
    "        else:\n",
    "            print('##########', ind_)\n",
    "            pass\n",
    "        line_num += 1\n",
    "#}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T1A\n"
     ]
    }
   ],
   "source": [
    "# Loading the folder to be procesed from command line{\n",
    "p = 'T1A'\n",
    "tmp = p.replace('/','_')\n",
    "print(tmp)\n",
    "\n",
    "folder = 'pkl_test'\n",
    "p_num = 1\n",
    "url_path = '../../data/'+p+'/'\n",
    "#}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "178\n"
     ]
    }
   ],
   "source": [
    "# Prepare the image list and pre-process them{\n",
    "true_wids = []\n",
    "im_list = []\n",
    "for i in os.listdir(url_path):\n",
    "    if not i.startswith('~') and not i.startswith('.'):\n",
    "        #print i, truth\n",
    "        temp = i.split('.')[0].split('_')[2]\n",
    "        true_wids.append(truth[int(temp)][1])\n",
    "        im_list.append(url_path+i)\n",
    "print(len(im_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'MobileNetV2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "im = preprocess_image_batch(im_list,img_size=(256,256), crop_size=(224,224), color_mode=\"rgb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/abhijit/anaconda3/envs/lesion/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/abhijit/anaconda3/envs/lesion/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/abhijit/anaconda3/envs/lesion/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "Conv_1 0 200704\n",
      "WARNING:tensorflow:From /home/abhijit/anaconda3/envs/lesion/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/abhijit/anaconda3/envs/lesion/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/abhijit/anaconda3/envs/lesion/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:1834: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n",
      "Conv_1_BN 200704 401408\n",
      "expanded_conv_depthwise 401408 602112\n",
      "expanded_conv_depthwise_BN 602112 802816\n",
      "expanded_conv_project 802816 903168\n",
      "expanded_conv_project_BN 903168 1003520\n",
      "block_1_expand 1003520 1605632\n",
      "block_1_expand_BN 1605632 2207744\n",
      "block_1_depthwise 2207744 2358272\n",
      "block_1_depthwise_BN 2358272 2508800\n",
      "block_1_project 2508800 2533888\n",
      "block_1_project_BN 2533888 2558976\n",
      "block_2_expand 2558976 2709504\n",
      "block_2_expand_BN 2709504 2860032\n",
      "block_2_depthwise 2860032 3010560\n",
      "block_2_depthwise_BN 3010560 3161088\n",
      "block_2_project 3161088 3186176\n",
      "block_2_project_BN 3186176 3211264\n",
      "block_3_expand 3211264 3361792\n",
      "block_3_expand_BN 3361792 3512320\n",
      "block_3_depthwise 3512320 3549952\n",
      "block_3_depthwise_BN 3549952 3587584\n",
      "block_3_project 3587584 3600128\n",
      "block_3_project_BN 3600128 3612672\n",
      "block_4_expand 3612672 3687936\n",
      "block_4_expand_BN 3687936 3763200\n",
      "block_4_depthwise 3763200 3838464\n",
      "block_4_depthwise_BN 3838464 3913728\n",
      "block_4_project 3913728 3926272\n",
      "block_4_project_BN 3926272 3938816\n",
      "block_5_expand 3938816 4014080\n",
      "block_5_expand_BN 4014080 4089344\n",
      "block_5_depthwise 4089344 4164608\n",
      "block_5_depthwise_BN 4164608 4239872\n",
      "block_5_project 4239872 4252416\n",
      "block_5_project_BN 4252416 4264960\n",
      "block_6_expand 4264960 4340224\n",
      "block_6_expand_BN 4340224 4415488\n",
      "block_6_depthwise 4415488 4434304\n",
      "block_6_depthwise_BN 4434304 4453120\n",
      "block_6_project 4453120 4457824\n",
      "block_6_project_BN 4457824 4462528\n",
      "block_7_expand 4462528 4490752\n",
      "block_7_expand_BN 4490752 4518976\n",
      "block_7_depthwise 4518976 4547200\n",
      "block_7_depthwise_BN 4547200 4575424\n",
      "block_7_project 4575424 4580128\n",
      "block_7_project_BN 4580128 4584832\n",
      "block_8_expand 4584832 4613056\n",
      "block_8_expand_BN 4613056 4641280\n",
      "block_8_depthwise 4641280 4669504\n",
      "block_8_depthwise_BN 4669504 4697728\n",
      "block_8_project 4697728 4702432\n",
      "block_8_project_BN 4702432 4707136\n",
      "block_9_expand 4707136 4735360\n",
      "block_9_expand_BN 4735360 4763584\n",
      "block_9_depthwise 4763584 4791808\n",
      "block_9_depthwise_BN 4791808 4820032\n",
      "block_9_project 4820032 4824736\n",
      "block_9_project_BN 4824736 4829440\n",
      "block_10_expand 4829440 4857664\n",
      "block_10_expand_BN 4857664 4885888\n",
      "block_10_depthwise 4885888 4914112\n",
      "block_10_depthwise_BN 4914112 4942336\n",
      "block_10_project 4942336 4948608\n",
      "block_10_project_BN 4948608 4954880\n",
      "block_11_expand 4954880 4992512\n",
      "block_11_expand_BN 4992512 5030144\n",
      "block_11_depthwise 5030144 5067776\n",
      "block_11_depthwise_BN 5067776 5105408\n",
      "block_11_project 5105408 5111680\n",
      "block_11_project_BN 5111680 5117952\n",
      "block_12_expand 5117952 5155584\n",
      "block_12_expand_BN 5155584 5193216\n",
      "block_12_depthwise 5193216 5230848\n",
      "block_12_depthwise_BN 5230848 5268480\n",
      "block_12_project 5268480 5274752\n",
      "block_12_project_BN 5274752 5281024\n",
      "block_13_expand 5281024 5318656\n",
      "block_13_expand_BN 5318656 5356288\n",
      "block_13_depthwise 5356288 5365696\n",
      "block_13_depthwise_BN 5365696 5375104\n",
      "block_13_project 5375104 5377848\n",
      "block_13_project_BN 5377848 5380592\n",
      "block_14_expand 5380592 5397056\n",
      "block_14_expand_BN 5397056 5413520\n",
      "block_14_depthwise 5413520 5429984\n",
      "block_14_depthwise_BN 5429984 5446448\n",
      "block_14_project 5446448 5449192\n",
      "block_14_project_BN 5449192 5451936\n",
      "block_15_expand 5451936 5468400\n",
      "block_15_expand_BN 5468400 5484864\n",
      "block_15_depthwise 5484864 5501328\n",
      "block_15_depthwise_BN 5501328 5517792\n",
      "block_15_project 5517792 5520536\n",
      "block_15_project_BN 5520536 5523280\n",
      "block_16_expand 5523280 5539744\n",
      "block_16_expand_BN 5539744 5556208\n",
      "block_16_depthwise 5556208 5572672\n",
      "block_16_depthwise_BN 5572672 5589136\n",
      "block_16_project 5589136 5594624\n",
      "block_16_project_BN 5594624 5600112\n",
      "Conv_1 5600112 5662832\n",
      "Conv_1_bn 5662832 5725552\n"
     ]
    }
   ],
   "source": [
    "# Model parmeters and running the model from the loaded weights{\n",
    "\n",
    "\n",
    "#vals = random.standard_normal(5725552)\n",
    "vals = np.ones(())\n",
    "model_name = 'MobileNetV2'\n",
    "model = MobileNetV2( input_shape=None,\n",
    "    alpha=0.35,\n",
    "    include_top=True,\n",
    "    weights=\"imagenet\",\n",
    "    input_tensor=None,\n",
    "    pooling=None,\n",
    "    lambda_mask = np.ones(5725552),\n",
    "    classes=1000,\n",
    "    classifier_activation=\"softmax\")\n",
    "#KFold\n",
    "k = 4\n",
    "\n",
    "im_train, im_test = train_test_split(im_list, test_size=0.25, random_state=42)\n",
    "#}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 1 (3, 3, 3, 16)\n",
      "2 1 (133, 112, 112, 16)\n",
      "bn_Conv1 4 (16,)\n",
      "4 1 (133, 112, 112, 16)\n",
      "expanded_conv_depthwise 1 (3, 3, 16, 1)\n",
      "7 1 (133, 112, 112, 16)\n",
      "expanded_conv_depthwise_BN 4 (16,)\n",
      "9 1 (133, 112, 112, 16)\n",
      "expanded_conv_project 1 (1, 1, 16, 8)\n",
      "12 1 (133, 112, 112, 8)\n",
      "expanded_conv_project_BN 4 (8,)\n",
      "14 1 (133, 112, 112, 8)\n",
      "block_1_expand 1 (1, 1, 8, 48)\n",
      "16 1 (133, 112, 112, 48)\n",
      "block_1_expand_BN 4 (48,)\n",
      "18 1 (133, 112, 112, 48)\n",
      "block_1_depthwise 1 (3, 3, 48, 1)\n",
      "22 1 (133, 56, 56, 48)\n",
      "block_1_depthwise_BN 4 (48,)\n",
      "24 1 (133, 56, 56, 48)\n",
      "block_1_project 1 (1, 1, 48, 8)\n",
      "27 1 (133, 56, 56, 8)\n",
      "block_1_project_BN 4 (8,)\n",
      "29 1 (133, 56, 56, 8)\n",
      "block_2_expand 1 (1, 1, 8, 48)\n",
      "31 1 (133, 56, 56, 48)\n",
      "block_2_expand_BN 4 (48,)\n",
      "33 1 (133, 56, 56, 48)\n",
      "block_2_depthwise 1 (3, 3, 48, 1)\n",
      "36 1 (133, 56, 56, 48)\n",
      "block_2_depthwise_BN 4 (48,)\n",
      "38 1 (133, 56, 56, 48)\n",
      "block_2_project 1 (1, 1, 48, 8)\n",
      "41 1 (133, 56, 56, 8)\n",
      "block_2_project_BN 4 (8,)\n",
      "43 1 (133, 56, 56, 8)\n",
      "block_3_expand 1 (1, 1, 8, 48)\n",
      "46 1 (133, 56, 56, 48)\n",
      "block_3_expand_BN 4 (48,)\n",
      "48 1 (133, 56, 56, 48)\n",
      "block_3_depthwise 1 (3, 3, 48, 1)\n",
      "52 1 (133, 28, 28, 48)\n",
      "block_3_depthwise_BN 4 (48,)\n",
      "54 1 (133, 28, 28, 48)\n",
      "block_3_project 1 (1, 1, 48, 16)\n",
      "57 1 (133, 28, 28, 16)\n",
      "block_3_project_BN 4 (16,)\n",
      "59 1 (133, 28, 28, 16)\n",
      "block_4_expand 1 (1, 1, 16, 96)\n",
      "61 1 (133, 28, 28, 96)\n",
      "block_4_expand_BN 4 (96,)\n",
      "63 1 (133, 28, 28, 96)\n",
      "block_4_depthwise 1 (3, 3, 96, 1)\n",
      "66 1 (133, 28, 28, 96)\n",
      "block_4_depthwise_BN 4 (96,)\n",
      "68 1 (133, 28, 28, 96)\n",
      "block_4_project 1 (1, 1, 96, 16)\n",
      "71 1 (133, 28, 28, 16)\n",
      "block_4_project_BN 4 (16,)\n",
      "73 1 (133, 28, 28, 16)\n",
      "block_5_expand 1 (1, 1, 16, 96)\n",
      "76 1 (133, 28, 28, 96)\n",
      "block_5_expand_BN 4 (96,)\n",
      "78 1 (133, 28, 28, 96)\n",
      "block_5_depthwise 1 (3, 3, 96, 1)\n",
      "81 1 (133, 28, 28, 96)\n",
      "block_5_depthwise_BN 4 (96,)\n",
      "83 1 (133, 28, 28, 96)\n",
      "block_5_project 1 (1, 1, 96, 16)\n",
      "86 1 (133, 28, 28, 16)\n",
      "block_5_project_BN 4 (16,)\n",
      "88 1 (133, 28, 28, 16)\n",
      "block_6_expand 1 (1, 1, 16, 96)\n",
      "91 1 (133, 28, 28, 96)\n",
      "block_6_expand_BN 4 (96,)\n",
      "93 1 (133, 28, 28, 96)\n",
      "block_6_depthwise 1 (3, 3, 96, 1)\n",
      "97 1 (133, 14, 14, 96)\n",
      "block_6_depthwise_BN 4 (96,)\n",
      "99 1 (133, 14, 14, 96)\n",
      "block_6_project 1 (1, 1, 96, 24)\n",
      "102 1 (133, 14, 14, 24)\n",
      "block_6_project_BN 4 (24,)\n",
      "104 1 (133, 14, 14, 24)\n",
      "block_7_expand 1 (1, 1, 24, 144)\n",
      "106 1 (133, 14, 14, 144)\n",
      "block_7_expand_BN 4 (144,)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-0f472250a161>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m             \u001b[0mactivations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_activations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mim_temp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactivations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivations\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mi\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-eccd4f8914ff>\u001b[0m in \u001b[0;36mget_activations\u001b[0;34m(model, layer, X_batch)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mget_activations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearning_phase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m#The learning phase flag is a bool tensor (0 = test, 1 = train)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mactivations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_activations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mX_batch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mactivations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/lesion/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/lesion/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2669\u001b[0m                                 \u001b[0mfeed_symbols\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2670\u001b[0m                                 \u001b[0msymbol_vals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2671\u001b[0;31m                                 session)\n\u001b[0m\u001b[1;32m   2672\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/lesion/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_make_callable\u001b[0;34m(self, feed_arrays, feed_symbols, symbol_vals, session)\u001b[0m\n\u001b[1;32m   2621\u001b[0m             \u001b[0mcallable_opts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_options\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCopyFrom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2622\u001b[0m         \u001b[0;31m# Create callable.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2623\u001b[0;31m         \u001b[0mcallable_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_callable_from_options\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallable_opts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2624\u001b[0m         \u001b[0;31m# Cache parameters corresponding to the generated callable, so that\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2625\u001b[0m         \u001b[0;31m# we can detect future mismatches and refresh the callable.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/lesion/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_make_callable_from_options\u001b[0;34m(self, callable_options)\u001b[0m\n\u001b[1;32m   1486\u001b[0m       \u001b[0mA\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0mto\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mnew\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1487\u001b[0m     \"\"\"\n\u001b[0;32m-> 1488\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1489\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mBaseSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_Callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallable_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1490\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/lesion/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_extend_graph\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1372\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1373\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session_run_lock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1374\u001b[0;31m       \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExtendSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1375\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1376\u001b[0m   \u001b[0;31m# The threshold to run garbage collection to delete dead tensors.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "im_temp = preprocess_image_batch(im_train,img_size=(256,256), crop_size=(224,224), color_mode=\"rgb\")\n",
    "i = 0\n",
    "for layer in model.layers:\n",
    "    weights = layer.get_weights()\n",
    "    if len(weights) > 0:\n",
    "        print(layer.name, len(weights), weights[0].shape)\n",
    "        if len(weights) > 0:\n",
    "            activations = get_activations(model,i,im_temp)\n",
    "            print(i, len(activations), activations[0].shape)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "155 155 155\n",
      "136 0.1225806451612903\n",
      "Conv1 1 1 (155, 112, 112, 16) (112, 112, 16) (200704,)\n",
      "bn_Conv1 4 1 (155, 112, 112, 16) (112, 112, 16) (200704,)\n",
      "expanded_conv_depthwise 1 1 (155, 112, 112, 16) (112, 112, 16) (200704,)\n",
      "expanded_conv_depthwise_BN 4 1 (155, 112, 112, 16) (112, 112, 16) (200704,)\n",
      "expanded_conv_project 1 1 (155, 112, 112, 8) (112, 112, 8) (100352,)\n",
      "expanded_conv_project_BN 4 1 (155, 112, 112, 8) (112, 112, 8) (100352,)\n",
      "block_1_expand 1 1 (155, 112, 112, 48) (112, 112, 48) (602112,)\n",
      "block_1_expand_BN 4 1 (155, 112, 112, 48) (112, 112, 48) (602112,)\n",
      "block_1_depthwise 1 1 (155, 56, 56, 48) (56, 56, 48) (150528,)\n",
      "block_1_depthwise_BN 4 1 (155, 56, 56, 48) (56, 56, 48) (150528,)\n",
      "block_1_project 1 1 (155, 56, 56, 8) (56, 56, 8) (25088,)\n",
      "block_1_project_BN 4 1 (155, 56, 56, 8) (56, 56, 8) (25088,)\n",
      "block_2_expand 1 1 (155, 56, 56, 48) (56, 56, 48) (150528,)\n",
      "block_2_expand_BN 4 1 (155, 56, 56, 48) (56, 56, 48) (150528,)\n",
      "block_2_depthwise 1 1 (155, 56, 56, 48) (56, 56, 48) (150528,)\n",
      "block_2_depthwise_BN 4 1 (155, 56, 56, 48) (56, 56, 48) (150528,)\n",
      "block_2_project 1 1 (155, 56, 56, 8) (56, 56, 8) (25088,)\n",
      "block_2_project_BN 4 1 (155, 56, 56, 8) (56, 56, 8) (25088,)\n",
      "block_3_expand 1 1 (155, 56, 56, 48) (56, 56, 48) (150528,)\n",
      "block_3_expand_BN 4 1 (155, 56, 56, 48) (56, 56, 48) (150528,)\n",
      "block_3_depthwise 1 1 (155, 28, 28, 48) (28, 28, 48) (37632,)\n",
      "block_3_depthwise_BN 4 1 (155, 28, 28, 48) (28, 28, 48) (37632,)\n",
      "block_3_project 1 1 (155, 28, 28, 16) (28, 28, 16) (12544,)\n",
      "block_3_project_BN 4 1 (155, 28, 28, 16) (28, 28, 16) (12544,)\n",
      "block_4_expand 1 1 (155, 28, 28, 96) (28, 28, 96) (75264,)\n",
      "block_4_expand_BN 4 1 (155, 28, 28, 96) (28, 28, 96) (75264,)\n",
      "block_4_depthwise 1 1 (155, 28, 28, 96) (28, 28, 96) (75264,)\n",
      "block_4_depthwise_BN 4 1 (155, 28, 28, 96) (28, 28, 96) (75264,)\n",
      "block_4_project 1 1 (155, 28, 28, 16) (28, 28, 16) (12544,)\n",
      "block_4_project_BN 4 1 (155, 28, 28, 16) (28, 28, 16) (12544,)\n",
      "block_5_expand 1 1 (155, 28, 28, 96) (28, 28, 96) (75264,)\n",
      "block_5_expand_BN 4 1 (155, 28, 28, 96) (28, 28, 96) (75264,)\n",
      "block_5_depthwise 1 1 (155, 28, 28, 96) (28, 28, 96) (75264,)\n",
      "block_5_depthwise_BN 4 1 (155, 28, 28, 96) (28, 28, 96) (75264,)\n",
      "block_5_project 1 1 (155, 28, 28, 16) (28, 28, 16) (12544,)\n",
      "block_5_project_BN 4 1 (155, 28, 28, 16) (28, 28, 16) (12544,)\n",
      "block_6_expand 1 1 (155, 28, 28, 96) (28, 28, 96) (75264,)\n",
      "block_6_expand_BN 4 1 (155, 28, 28, 96) (28, 28, 96) (75264,)\n",
      "block_6_depthwise 1 1 (155, 14, 14, 96) (14, 14, 96) (18816,)\n",
      "block_6_depthwise_BN 4 1 (155, 14, 14, 96) (14, 14, 96) (18816,)\n",
      "block_6_project 1 1 (155, 14, 14, 24) (14, 14, 24) (4704,)\n",
      "block_6_project_BN 4 1 (155, 14, 14, 24) (14, 14, 24) (4704,)\n",
      "block_7_expand 1 1 (155, 14, 14, 144) (14, 14, 144) (28224,)\n",
      "block_7_expand_BN 4 1 (155, 14, 14, 144) (14, 14, 144) (28224,)\n",
      "block_7_depthwise 1 1 (155, 14, 14, 144) (14, 14, 144) (28224,)\n",
      "block_7_depthwise_BN 4 1 (155, 14, 14, 144) (14, 14, 144) (28224,)\n",
      "block_7_project 1 1 (155, 14, 14, 24) (14, 14, 24) (4704,)\n",
      "block_7_project_BN 4 1 (155, 14, 14, 24) (14, 14, 24) (4704,)\n",
      "block_8_expand 1 1 (155, 14, 14, 144) (14, 14, 144) (28224,)\n",
      "block_8_expand_BN 4 1 (155, 14, 14, 144) (14, 14, 144) (28224,)\n",
      "block_8_depthwise 1 1 (155, 14, 14, 144) (14, 14, 144) (28224,)\n",
      "block_8_depthwise_BN 4 1 (155, 14, 14, 144) (14, 14, 144) (28224,)\n",
      "block_8_project 1 1 (155, 14, 14, 24) (14, 14, 24) (4704,)\n",
      "block_8_project_BN 4 1 (155, 14, 14, 24) (14, 14, 24) (4704,)\n",
      "block_9_expand 1 1 (155, 14, 14, 144) (14, 14, 144) (28224,)\n",
      "block_9_expand_BN 4 1 (155, 14, 14, 144) (14, 14, 144) (28224,)\n",
      "block_9_depthwise 1 1 (155, 14, 14, 144) (14, 14, 144) (28224,)\n",
      "block_9_depthwise_BN 4 1 (155, 14, 14, 144) (14, 14, 144) (28224,)\n",
      "block_9_project 1 1 (155, 14, 14, 24) (14, 14, 24) (4704,)\n",
      "block_9_project_BN 4 1 (155, 14, 14, 24) (14, 14, 24) (4704,)\n",
      "block_10_expand 1 1 (155, 14, 14, 144) (14, 14, 144) (28224,)\n",
      "block_10_expand_BN 4 1 (155, 14, 14, 144) (14, 14, 144) (28224,)\n",
      "block_10_depthwise 1 1 (155, 14, 14, 144) (14, 14, 144) (28224,)\n",
      "block_10_depthwise_BN 4 1 (155, 14, 14, 144) (14, 14, 144) (28224,)\n",
      "block_10_project 1 1 (155, 14, 14, 32) (14, 14, 32) (6272,)\n",
      "block_10_project_BN 4 1 (155, 14, 14, 32) (14, 14, 32) (6272,)\n",
      "block_11_expand 1 1 (155, 14, 14, 192) (14, 14, 192) (37632,)\n",
      "block_11_expand_BN 4 1 (155, 14, 14, 192) (14, 14, 192) (37632,)\n",
      "block_11_depthwise 1 1 (155, 14, 14, 192) (14, 14, 192) (37632,)\n",
      "block_11_depthwise_BN 4 1 (155, 14, 14, 192) (14, 14, 192) (37632,)\n",
      "block_11_project 1 1 (155, 14, 14, 32) (14, 14, 32) (6272,)\n",
      "block_11_project_BN 4 1 (155, 14, 14, 32) (14, 14, 32) (6272,)\n",
      "block_12_expand 1 1 (155, 14, 14, 192) (14, 14, 192) (37632,)\n",
      "block_12_expand_BN 4 1 (155, 14, 14, 192) (14, 14, 192) (37632,)\n",
      "block_12_depthwise 1 1 (155, 14, 14, 192) (14, 14, 192) (37632,)\n",
      "block_12_depthwise_BN 4 1 (155, 14, 14, 192) (14, 14, 192) (37632,)\n",
      "block_12_project 1 1 (155, 14, 14, 32) (14, 14, 32) (6272,)\n",
      "block_12_project_BN 4 1 (155, 14, 14, 32) (14, 14, 32) (6272,)\n",
      "block_13_expand 1 1 (155, 14, 14, 192) (14, 14, 192) (37632,)\n",
      "block_13_expand_BN 4 1 (155, 14, 14, 192) (14, 14, 192) (37632,)\n",
      "block_13_depthwise 1 1 (155, 7, 7, 192) (7, 7, 192) (9408,)\n",
      "block_13_depthwise_BN 4 1 (155, 7, 7, 192) (7, 7, 192) (9408,)\n",
      "block_13_project 1 1 (155, 7, 7, 56) (7, 7, 56) (2744,)\n",
      "block_13_project_BN 4 1 (155, 7, 7, 56) (7, 7, 56) (2744,)\n",
      "block_14_expand 1 1 (155, 7, 7, 336) (7, 7, 336) (16464,)\n",
      "block_14_expand_BN 4 1 (155, 7, 7, 336) (7, 7, 336) (16464,)\n",
      "block_14_depthwise 1 1 (155, 7, 7, 336) (7, 7, 336) (16464,)\n",
      "block_14_depthwise_BN 4 1 (155, 7, 7, 336) (7, 7, 336) (16464,)\n",
      "block_14_project 1 1 (155, 7, 7, 56) (7, 7, 56) (2744,)\n",
      "block_14_project_BN 4 1 (155, 7, 7, 56) (7, 7, 56) (2744,)\n",
      "block_15_expand 1 1 (155, 7, 7, 336) (7, 7, 336) (16464,)\n",
      "block_15_expand_BN 4 1 (155, 7, 7, 336) (7, 7, 336) (16464,)\n",
      "block_15_depthwise 1 1 (155, 7, 7, 336) (7, 7, 336) (16464,)\n",
      "block_15_depthwise_BN 4 1 (155, 7, 7, 336) (7, 7, 336) (16464,)\n",
      "block_15_project 1 1 (155, 7, 7, 56) (7, 7, 56) (2744,)\n",
      "block_15_project_BN 4 1 (155, 7, 7, 56) (7, 7, 56) (2744,)\n",
      "block_16_expand 1 1 (155, 7, 7, 336) (7, 7, 336) (16464,)\n",
      "block_16_expand_BN 4 1 (155, 7, 7, 336) (7, 7, 336) (16464,)\n",
      "block_16_depthwise 1 1 (155, 7, 7, 336) (7, 7, 336) (16464,)\n",
      "block_16_depthwise_BN 4 1 (155, 7, 7, 336) (7, 7, 336) (16464,)\n",
      "block_16_project 1 1 (155, 7, 7, 112) (7, 7, 112) (5488,)\n",
      "block_16_project_BN 4 1 (155, 7, 7, 112) (7, 7, 112) (5488,)\n",
      "Conv_1 1 1 (155, 7, 7, 1280) (7, 7, 1280) (62720,)\n",
      "Conv_1_bn 4 1 (155, 7, 7, 1280) (7, 7, 1280) (62720,)\n",
      "(5725552,)\n"
     ]
    }
   ],
   "source": [
    "#Training data pkl\n",
    "fp_name = '../../data/pkl_mobile/'+str(p)+'_'+str(model_name)+'_train_no_lesion_performance.txt'\n",
    "fp = open(fp_name,'a+')\n",
    "im_temp = preprocess_image_batch(im_train,img_size=(256,256), crop_size=(224,224), color_mode=\"rgb\")\n",
    "out = model.predict(im_temp)\n",
    "\n",
    "true_valid_wids = []\n",
    "for i in im_train:\n",
    "        temp1 = i.split('/')[4]\n",
    "        temp = temp1.split('.')[0].split('_')[2]\n",
    "        true_valid_wids.append(truth[int(temp)][1])\n",
    "\n",
    "predicted_valid_wids = []\n",
    "for i in range(len(im_train)):\n",
    "    #print(im_list[i], pprint_output(out[i]), true_wids[i])\n",
    "    predicted_valid_wids.append(pprint_output(out[i]))\n",
    "\n",
    "count, error  = top5accuracy(true_valid_wids, predicted_valid_wids)\n",
    "\n",
    "fp.write(str(p)+' '+str(count)+' '+str(len(im_train))+' '+str(error)+'\\n')\n",
    "\n",
    "\n",
    "print(len(true_valid_wids), len(predicted_valid_wids), len(im_train))\n",
    "print(count, error)\n",
    "\n",
    "\n",
    "#}\n",
    "# Code snippet to get the activation values and saving information{\n",
    "data = np.array([])\n",
    "\n",
    "i = 0\n",
    "result ={}\n",
    "for layer in model.layers:\n",
    "    weights = layer.get_weights()\n",
    "    #print(layer.name, len(weights))\n",
    "    if len(weights) > 0:\n",
    "        activations = get_activations(model,i,im_temp)\n",
    "        if result.get(layer.name, None) is None:\n",
    "            result[layer.name] = activations[0]\n",
    "            temp = np.mean(activations[0], axis=0).ravel()\n",
    "            if layer.name != 'Logits':\n",
    "                print(layer.name,len(weights),len(activations), activations[0].shape, np.mean(activations[0], axis=0).shape, temp.shape)\n",
    "                data = np.append(data, temp)\n",
    "    i += 1\n",
    "\n",
    "print(data.shape)\n",
    "fp.close()\n",
    "with open('../../data/pkl_mobile/'+str(p)+'_train_'+model_name+'.pkl', 'wb') as f:\n",
    "    pickle.dump(data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39 39 39\n",
      "35 0.10256410256410253\n",
      "Conv1 1 1 (39, 112, 112, 16) (112, 112, 16) (200704,)\n",
      "bn_Conv1 4 1 (39, 112, 112, 16) (112, 112, 16) (200704,)\n",
      "expanded_conv_depthwise 1 1 (39, 112, 112, 16) (112, 112, 16) (200704,)\n",
      "expanded_conv_depthwise_BN 4 1 (39, 112, 112, 16) (112, 112, 16) (200704,)\n",
      "expanded_conv_project 1 1 (39, 112, 112, 8) (112, 112, 8) (100352,)\n",
      "expanded_conv_project_BN 4 1 (39, 112, 112, 8) (112, 112, 8) (100352,)\n",
      "block_1_expand 1 1 (39, 112, 112, 48) (112, 112, 48) (602112,)\n",
      "block_1_expand_BN 4 1 (39, 112, 112, 48) (112, 112, 48) (602112,)\n",
      "block_1_depthwise 1 1 (39, 56, 56, 48) (56, 56, 48) (150528,)\n",
      "block_1_depthwise_BN 4 1 (39, 56, 56, 48) (56, 56, 48) (150528,)\n",
      "block_1_project 1 1 (39, 56, 56, 8) (56, 56, 8) (25088,)\n",
      "block_1_project_BN 4 1 (39, 56, 56, 8) (56, 56, 8) (25088,)\n",
      "block_2_expand 1 1 (39, 56, 56, 48) (56, 56, 48) (150528,)\n",
      "block_2_expand_BN 4 1 (39, 56, 56, 48) (56, 56, 48) (150528,)\n",
      "block_2_depthwise 1 1 (39, 56, 56, 48) (56, 56, 48) (150528,)\n",
      "block_2_depthwise_BN 4 1 (39, 56, 56, 48) (56, 56, 48) (150528,)\n",
      "block_2_project 1 1 (39, 56, 56, 8) (56, 56, 8) (25088,)\n",
      "block_2_project_BN 4 1 (39, 56, 56, 8) (56, 56, 8) (25088,)\n",
      "block_3_expand 1 1 (39, 56, 56, 48) (56, 56, 48) (150528,)\n",
      "block_3_expand_BN 4 1 (39, 56, 56, 48) (56, 56, 48) (150528,)\n",
      "block_3_depthwise 1 1 (39, 28, 28, 48) (28, 28, 48) (37632,)\n",
      "block_3_depthwise_BN 4 1 (39, 28, 28, 48) (28, 28, 48) (37632,)\n",
      "block_3_project 1 1 (39, 28, 28, 16) (28, 28, 16) (12544,)\n",
      "block_3_project_BN 4 1 (39, 28, 28, 16) (28, 28, 16) (12544,)\n",
      "block_4_expand 1 1 (39, 28, 28, 96) (28, 28, 96) (75264,)\n",
      "block_4_expand_BN 4 1 (39, 28, 28, 96) (28, 28, 96) (75264,)\n",
      "block_4_depthwise 1 1 (39, 28, 28, 96) (28, 28, 96) (75264,)\n",
      "block_4_depthwise_BN 4 1 (39, 28, 28, 96) (28, 28, 96) (75264,)\n",
      "block_4_project 1 1 (39, 28, 28, 16) (28, 28, 16) (12544,)\n",
      "block_4_project_BN 4 1 (39, 28, 28, 16) (28, 28, 16) (12544,)\n",
      "block_5_expand 1 1 (39, 28, 28, 96) (28, 28, 96) (75264,)\n",
      "block_5_expand_BN 4 1 (39, 28, 28, 96) (28, 28, 96) (75264,)\n",
      "block_5_depthwise 1 1 (39, 28, 28, 96) (28, 28, 96) (75264,)\n",
      "block_5_depthwise_BN 4 1 (39, 28, 28, 96) (28, 28, 96) (75264,)\n",
      "block_5_project 1 1 (39, 28, 28, 16) (28, 28, 16) (12544,)\n",
      "block_5_project_BN 4 1 (39, 28, 28, 16) (28, 28, 16) (12544,)\n",
      "block_6_expand 1 1 (39, 28, 28, 96) (28, 28, 96) (75264,)\n",
      "block_6_expand_BN 4 1 (39, 28, 28, 96) (28, 28, 96) (75264,)\n",
      "block_6_depthwise 1 1 (39, 14, 14, 96) (14, 14, 96) (18816,)\n",
      "block_6_depthwise_BN 4 1 (39, 14, 14, 96) (14, 14, 96) (18816,)\n",
      "block_6_project 1 1 (39, 14, 14, 24) (14, 14, 24) (4704,)\n",
      "block_6_project_BN 4 1 (39, 14, 14, 24) (14, 14, 24) (4704,)\n",
      "block_7_expand 1 1 (39, 14, 14, 144) (14, 14, 144) (28224,)\n",
      "block_7_expand_BN 4 1 (39, 14, 14, 144) (14, 14, 144) (28224,)\n",
      "block_7_depthwise 1 1 (39, 14, 14, 144) (14, 14, 144) (28224,)\n",
      "block_7_depthwise_BN 4 1 (39, 14, 14, 144) (14, 14, 144) (28224,)\n",
      "block_7_project 1 1 (39, 14, 14, 24) (14, 14, 24) (4704,)\n",
      "block_7_project_BN 4 1 (39, 14, 14, 24) (14, 14, 24) (4704,)\n",
      "block_8_expand 1 1 (39, 14, 14, 144) (14, 14, 144) (28224,)\n",
      "block_8_expand_BN 4 1 (39, 14, 14, 144) (14, 14, 144) (28224,)\n",
      "block_8_depthwise 1 1 (39, 14, 14, 144) (14, 14, 144) (28224,)\n",
      "block_8_depthwise_BN 4 1 (39, 14, 14, 144) (14, 14, 144) (28224,)\n",
      "block_8_project 1 1 (39, 14, 14, 24) (14, 14, 24) (4704,)\n",
      "block_8_project_BN 4 1 (39, 14, 14, 24) (14, 14, 24) (4704,)\n",
      "block_9_expand 1 1 (39, 14, 14, 144) (14, 14, 144) (28224,)\n",
      "block_9_expand_BN 4 1 (39, 14, 14, 144) (14, 14, 144) (28224,)\n",
      "block_9_depthwise 1 1 (39, 14, 14, 144) (14, 14, 144) (28224,)\n",
      "block_9_depthwise_BN 4 1 (39, 14, 14, 144) (14, 14, 144) (28224,)\n",
      "block_9_project 1 1 (39, 14, 14, 24) (14, 14, 24) (4704,)\n",
      "block_9_project_BN 4 1 (39, 14, 14, 24) (14, 14, 24) (4704,)\n",
      "block_10_expand 1 1 (39, 14, 14, 144) (14, 14, 144) (28224,)\n",
      "block_10_expand_BN 4 1 (39, 14, 14, 144) (14, 14, 144) (28224,)\n",
      "block_10_depthwise 1 1 (39, 14, 14, 144) (14, 14, 144) (28224,)\n",
      "block_10_depthwise_BN 4 1 (39, 14, 14, 144) (14, 14, 144) (28224,)\n",
      "block_10_project 1 1 (39, 14, 14, 32) (14, 14, 32) (6272,)\n",
      "block_10_project_BN 4 1 (39, 14, 14, 32) (14, 14, 32) (6272,)\n",
      "block_11_expand 1 1 (39, 14, 14, 192) (14, 14, 192) (37632,)\n",
      "block_11_expand_BN 4 1 (39, 14, 14, 192) (14, 14, 192) (37632,)\n",
      "block_11_depthwise 1 1 (39, 14, 14, 192) (14, 14, 192) (37632,)\n",
      "block_11_depthwise_BN 4 1 (39, 14, 14, 192) (14, 14, 192) (37632,)\n",
      "block_11_project 1 1 (39, 14, 14, 32) (14, 14, 32) (6272,)\n",
      "block_11_project_BN 4 1 (39, 14, 14, 32) (14, 14, 32) (6272,)\n",
      "block_12_expand 1 1 (39, 14, 14, 192) (14, 14, 192) (37632,)\n",
      "block_12_expand_BN 4 1 (39, 14, 14, 192) (14, 14, 192) (37632,)\n",
      "block_12_depthwise 1 1 (39, 14, 14, 192) (14, 14, 192) (37632,)\n",
      "block_12_depthwise_BN 4 1 (39, 14, 14, 192) (14, 14, 192) (37632,)\n",
      "block_12_project 1 1 (39, 14, 14, 32) (14, 14, 32) (6272,)\n",
      "block_12_project_BN 4 1 (39, 14, 14, 32) (14, 14, 32) (6272,)\n",
      "block_13_expand 1 1 (39, 14, 14, 192) (14, 14, 192) (37632,)\n",
      "block_13_expand_BN 4 1 (39, 14, 14, 192) (14, 14, 192) (37632,)\n",
      "block_13_depthwise 1 1 (39, 7, 7, 192) (7, 7, 192) (9408,)\n",
      "block_13_depthwise_BN 4 1 (39, 7, 7, 192) (7, 7, 192) (9408,)\n",
      "block_13_project 1 1 (39, 7, 7, 56) (7, 7, 56) (2744,)\n",
      "block_13_project_BN 4 1 (39, 7, 7, 56) (7, 7, 56) (2744,)\n",
      "block_14_expand 1 1 (39, 7, 7, 336) (7, 7, 336) (16464,)\n",
      "block_14_expand_BN 4 1 (39, 7, 7, 336) (7, 7, 336) (16464,)\n",
      "block_14_depthwise 1 1 (39, 7, 7, 336) (7, 7, 336) (16464,)\n",
      "block_14_depthwise_BN 4 1 (39, 7, 7, 336) (7, 7, 336) (16464,)\n",
      "block_14_project 1 1 (39, 7, 7, 56) (7, 7, 56) (2744,)\n",
      "block_14_project_BN 4 1 (39, 7, 7, 56) (7, 7, 56) (2744,)\n",
      "block_15_expand 1 1 (39, 7, 7, 336) (7, 7, 336) (16464,)\n",
      "block_15_expand_BN 4 1 (39, 7, 7, 336) (7, 7, 336) (16464,)\n",
      "block_15_depthwise 1 1 (39, 7, 7, 336) (7, 7, 336) (16464,)\n",
      "block_15_depthwise_BN 4 1 (39, 7, 7, 336) (7, 7, 336) (16464,)\n",
      "block_15_project 1 1 (39, 7, 7, 56) (7, 7, 56) (2744,)\n",
      "block_15_project_BN 4 1 (39, 7, 7, 56) (7, 7, 56) (2744,)\n",
      "block_16_expand 1 1 (39, 7, 7, 336) (7, 7, 336) (16464,)\n",
      "block_16_expand_BN 4 1 (39, 7, 7, 336) (7, 7, 336) (16464,)\n",
      "block_16_depthwise 1 1 (39, 7, 7, 336) (7, 7, 336) (16464,)\n",
      "block_16_depthwise_BN 4 1 (39, 7, 7, 336) (7, 7, 336) (16464,)\n",
      "block_16_project 1 1 (39, 7, 7, 112) (7, 7, 112) (5488,)\n",
      "block_16_project_BN 4 1 (39, 7, 7, 112) (7, 7, 112) (5488,)\n",
      "Conv_1 1 1 (39, 7, 7, 1280) (7, 7, 1280) (62720,)\n",
      "Conv_1_bn 4 1 (39, 7, 7, 1280) (7, 7, 1280) (62720,)\n",
      "(5725552,)\n"
     ]
    }
   ],
   "source": [
    "#Testing data pkl\n",
    "fp_name = '../../data/pkl_mobile/'+str(p)+'_'+str(model_name)+'_test_no_lesion_performance.txt'\n",
    "fp = open(fp_name,'a+')\n",
    "\n",
    "im_temp = preprocess_image_batch(im_test,img_size=(256,256), crop_size=(224,224), color_mode=\"rgb\")\n",
    "out = model.predict(im_temp,batch_size=64)\n",
    "\n",
    "true_valid_wids = []\n",
    "for i in im_test:\n",
    "        temp1 = i.split('/')[4]\n",
    "        temp = temp1.split('.')[0].split('_')[2]\n",
    "        true_valid_wids.append(truth[int(temp)][1])\n",
    "\n",
    "predicted_valid_wids = []\n",
    "for i in range(len(im_test)):\n",
    "    #print im_list[i], pprint_output(out[i]), true_wids[i]\n",
    "    predicted_valid_wids.append(pprint_output(out[i]))\n",
    "\n",
    "count, error  = top5accuracy(true_valid_wids, predicted_valid_wids)\n",
    "\n",
    "fp.write(str(p)+' '+str(count)+' '+str(len(im_test))+' '+str(error)+'\\n')\n",
    "\n",
    "\n",
    "print(len(true_valid_wids), len(predicted_valid_wids), len(im_test))\n",
    "print(count, error)\n",
    "\n",
    "\n",
    "#}\n",
    "# Code snippet to get the activation values and saving information{\n",
    "data = np.array([])\n",
    "\n",
    "i = 0\n",
    "result ={}\n",
    "for layer in model.layers:\n",
    "    weights = layer.get_weights()\n",
    "    if len(weights) > 0:\n",
    "        activations = get_activations(model,i,im_temp)\n",
    "        if result.get(layer.name, None) is None:\n",
    "            result[layer.name] = activations[0]\n",
    "            temp = np.mean(activations[0], axis=0).ravel()\n",
    "            if layer.name != 'Logits':\n",
    "                print(layer.name,len(weights),len(activations), activations[0].shape, np.mean(activations[0], axis=0).shape, temp.shape)\n",
    "                data = np.append(data, temp)\n",
    "    i += 1\n",
    "    \n",
    "print(data.shape)\n",
    "fp.close()\n",
    "with open('../../data/pkl_mobile/'+str(p)+'_test_'+model_name+'.pkl', 'wb') as f:\n",
    "    pickle.dump(data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Fold:  1\n",
      "39 39 39\n",
      "31 0.20512820512820518\n",
      "Conv1 1 1 (39, 112, 112, 16) (112, 112, 16) (200704,)\n",
      "bn_Conv1 4 1 (39, 112, 112, 16) (112, 112, 16) (200704,)\n",
      "expanded_conv_depthwise 1 1 (39, 112, 112, 16) (112, 112, 16) (200704,)\n",
      "expanded_conv_depthwise_BN 4 1 (39, 112, 112, 16) (112, 112, 16) (200704,)\n",
      "expanded_conv_project 1 1 (39, 112, 112, 8) (112, 112, 8) (100352,)\n",
      "expanded_conv_project_BN 4 1 (39, 112, 112, 8) (112, 112, 8) (100352,)\n",
      "block_1_expand 1 1 (39, 112, 112, 48) (112, 112, 48) (602112,)\n",
      "block_1_expand_BN 4 1 (39, 112, 112, 48) (112, 112, 48) (602112,)\n",
      "block_1_depthwise 1 1 (39, 56, 56, 48) (56, 56, 48) (150528,)\n",
      "block_1_depthwise_BN 4 1 (39, 56, 56, 48) (56, 56, 48) (150528,)\n",
      "block_1_project 1 1 (39, 56, 56, 8) (56, 56, 8) (25088,)\n",
      "block_1_project_BN 4 1 (39, 56, 56, 8) (56, 56, 8) (25088,)\n",
      "block_2_expand 1 1 (39, 56, 56, 48) (56, 56, 48) (150528,)\n",
      "block_2_expand_BN 4 1 (39, 56, 56, 48) (56, 56, 48) (150528,)\n",
      "block_2_depthwise 1 1 (39, 56, 56, 48) (56, 56, 48) (150528,)\n",
      "block_2_depthwise_BN 4 1 (39, 56, 56, 48) (56, 56, 48) (150528,)\n",
      "block_2_project 1 1 (39, 56, 56, 8) (56, 56, 8) (25088,)\n",
      "block_2_project_BN 4 1 (39, 56, 56, 8) (56, 56, 8) (25088,)\n",
      "block_3_expand 1 1 (39, 56, 56, 48) (56, 56, 48) (150528,)\n",
      "block_3_expand_BN 4 1 (39, 56, 56, 48) (56, 56, 48) (150528,)\n",
      "block_3_depthwise 1 1 (39, 28, 28, 48) (28, 28, 48) (37632,)\n",
      "block_3_depthwise_BN 4 1 (39, 28, 28, 48) (28, 28, 48) (37632,)\n",
      "block_3_project 1 1 (39, 28, 28, 16) (28, 28, 16) (12544,)\n",
      "block_3_project_BN 4 1 (39, 28, 28, 16) (28, 28, 16) (12544,)\n",
      "block_4_expand 1 1 (39, 28, 28, 96) (28, 28, 96) (75264,)\n",
      "block_4_expand_BN 4 1 (39, 28, 28, 96) (28, 28, 96) (75264,)\n",
      "block_4_depthwise 1 1 (39, 28, 28, 96) (28, 28, 96) (75264,)\n",
      "block_4_depthwise_BN 4 1 (39, 28, 28, 96) (28, 28, 96) (75264,)\n",
      "block_4_project 1 1 (39, 28, 28, 16) (28, 28, 16) (12544,)\n",
      "block_4_project_BN 4 1 (39, 28, 28, 16) (28, 28, 16) (12544,)\n",
      "block_5_expand 1 1 (39, 28, 28, 96) (28, 28, 96) (75264,)\n",
      "block_5_expand_BN 4 1 (39, 28, 28, 96) (28, 28, 96) (75264,)\n",
      "block_5_depthwise 1 1 (39, 28, 28, 96) (28, 28, 96) (75264,)\n",
      "block_5_depthwise_BN 4 1 (39, 28, 28, 96) (28, 28, 96) (75264,)\n",
      "block_5_project 1 1 (39, 28, 28, 16) (28, 28, 16) (12544,)\n",
      "block_5_project_BN 4 1 (39, 28, 28, 16) (28, 28, 16) (12544,)\n",
      "block_6_expand 1 1 (39, 28, 28, 96) (28, 28, 96) (75264,)\n",
      "block_6_expand_BN 4 1 (39, 28, 28, 96) (28, 28, 96) (75264,)\n",
      "block_6_depthwise 1 1 (39, 14, 14, 96) (14, 14, 96) (18816,)\n",
      "block_6_depthwise_BN 4 1 (39, 14, 14, 96) (14, 14, 96) (18816,)\n",
      "block_6_project 1 1 (39, 14, 14, 24) (14, 14, 24) (4704,)\n",
      "block_6_project_BN 4 1 (39, 14, 14, 24) (14, 14, 24) (4704,)\n",
      "block_7_expand 1 1 (39, 14, 14, 144) (14, 14, 144) (28224,)\n",
      "block_7_expand_BN 4 1 (39, 14, 14, 144) (14, 14, 144) (28224,)\n",
      "block_7_depthwise 1 1 (39, 14, 14, 144) (14, 14, 144) (28224,)\n",
      "block_7_depthwise_BN 4 1 (39, 14, 14, 144) (14, 14, 144) (28224,)\n",
      "block_7_project 1 1 (39, 14, 14, 24) (14, 14, 24) (4704,)\n",
      "block_7_project_BN 4 1 (39, 14, 14, 24) (14, 14, 24) (4704,)\n",
      "block_8_expand 1 1 (39, 14, 14, 144) (14, 14, 144) (28224,)\n",
      "block_8_expand_BN 4 1 (39, 14, 14, 144) (14, 14, 144) (28224,)\n",
      "block_8_depthwise 1 1 (39, 14, 14, 144) (14, 14, 144) (28224,)\n",
      "block_8_depthwise_BN 4 1 (39, 14, 14, 144) (14, 14, 144) (28224,)\n",
      "block_8_project 1 1 (39, 14, 14, 24) (14, 14, 24) (4704,)\n",
      "block_8_project_BN 4 1 (39, 14, 14, 24) (14, 14, 24) (4704,)\n",
      "block_9_expand 1 1 (39, 14, 14, 144) (14, 14, 144) (28224,)\n",
      "block_9_expand_BN 4 1 (39, 14, 14, 144) (14, 14, 144) (28224,)\n",
      "block_9_depthwise 1 1 (39, 14, 14, 144) (14, 14, 144) (28224,)\n",
      "block_9_depthwise_BN 4 1 (39, 14, 14, 144) (14, 14, 144) (28224,)\n",
      "block_9_project 1 1 (39, 14, 14, 24) (14, 14, 24) (4704,)\n",
      "block_9_project_BN 4 1 (39, 14, 14, 24) (14, 14, 24) (4704,)\n",
      "block_10_expand 1 1 (39, 14, 14, 144) (14, 14, 144) (28224,)\n",
      "block_10_expand_BN 4 1 (39, 14, 14, 144) (14, 14, 144) (28224,)\n",
      "block_10_depthwise 1 1 (39, 14, 14, 144) (14, 14, 144) (28224,)\n",
      "block_10_depthwise_BN 4 1 (39, 14, 14, 144) (14, 14, 144) (28224,)\n",
      "block_10_project 1 1 (39, 14, 14, 32) (14, 14, 32) (6272,)\n",
      "block_10_project_BN 4 1 (39, 14, 14, 32) (14, 14, 32) (6272,)\n",
      "block_11_expand 1 1 (39, 14, 14, 192) (14, 14, 192) (37632,)\n",
      "block_11_expand_BN 4 1 (39, 14, 14, 192) (14, 14, 192) (37632,)\n",
      "block_11_depthwise 1 1 (39, 14, 14, 192) (14, 14, 192) (37632,)\n",
      "block_11_depthwise_BN 4 1 (39, 14, 14, 192) (14, 14, 192) (37632,)\n",
      "block_11_project 1 1 (39, 14, 14, 32) (14, 14, 32) (6272,)\n",
      "block_11_project_BN 4 1 (39, 14, 14, 32) (14, 14, 32) (6272,)\n",
      "block_12_expand 1 1 (39, 14, 14, 192) (14, 14, 192) (37632,)\n",
      "block_12_expand_BN 4 1 (39, 14, 14, 192) (14, 14, 192) (37632,)\n",
      "block_12_depthwise 1 1 (39, 14, 14, 192) (14, 14, 192) (37632,)\n",
      "block_12_depthwise_BN 4 1 (39, 14, 14, 192) (14, 14, 192) (37632,)\n",
      "block_12_project 1 1 (39, 14, 14, 32) (14, 14, 32) (6272,)\n",
      "block_12_project_BN 4 1 (39, 14, 14, 32) (14, 14, 32) (6272,)\n",
      "block_13_expand 1 1 (39, 14, 14, 192) (14, 14, 192) (37632,)\n",
      "block_13_expand_BN 4 1 (39, 14, 14, 192) (14, 14, 192) (37632,)\n",
      "block_13_depthwise 1 1 (39, 7, 7, 192) (7, 7, 192) (9408,)\n",
      "block_13_depthwise_BN 4 1 (39, 7, 7, 192) (7, 7, 192) (9408,)\n",
      "block_13_project 1 1 (39, 7, 7, 56) (7, 7, 56) (2744,)\n",
      "block_13_project_BN 4 1 (39, 7, 7, 56) (7, 7, 56) (2744,)\n",
      "block_14_expand 1 1 (39, 7, 7, 336) (7, 7, 336) (16464,)\n",
      "block_14_expand_BN 4 1 (39, 7, 7, 336) (7, 7, 336) (16464,)\n",
      "block_14_depthwise 1 1 (39, 7, 7, 336) (7, 7, 336) (16464,)\n",
      "block_14_depthwise_BN 4 1 (39, 7, 7, 336) (7, 7, 336) (16464,)\n",
      "block_14_project 1 1 (39, 7, 7, 56) (7, 7, 56) (2744,)\n",
      "block_14_project_BN 4 1 (39, 7, 7, 56) (7, 7, 56) (2744,)\n",
      "block_15_expand 1 1 (39, 7, 7, 336) (7, 7, 336) (16464,)\n",
      "block_15_expand_BN 4 1 (39, 7, 7, 336) (7, 7, 336) (16464,)\n",
      "block_15_depthwise 1 1 (39, 7, 7, 336) (7, 7, 336) (16464,)\n",
      "block_15_depthwise_BN 4 1 (39, 7, 7, 336) (7, 7, 336) (16464,)\n",
      "block_15_project 1 1 (39, 7, 7, 56) (7, 7, 56) (2744,)\n",
      "block_15_project_BN 4 1 (39, 7, 7, 56) (7, 7, 56) (2744,)\n",
      "block_16_expand 1 1 (39, 7, 7, 336) (7, 7, 336) (16464,)\n",
      "block_16_expand_BN 4 1 (39, 7, 7, 336) (7, 7, 336) (16464,)\n",
      "block_16_depthwise 1 1 (39, 7, 7, 336) (7, 7, 336) (16464,)\n",
      "block_16_depthwise_BN 4 1 (39, 7, 7, 336) (7, 7, 336) (16464,)\n",
      "block_16_project 1 1 (39, 7, 7, 112) (7, 7, 112) (5488,)\n",
      "block_16_project_BN 4 1 (39, 7, 7, 112) (7, 7, 112) (5488,)\n",
      "Conv_1 1 1 (39, 7, 7, 1280) (7, 7, 1280) (62720,)\n",
      "Conv_1_bn 4 1 (39, 7, 7, 1280) (7, 7, 1280) (62720,)\n",
      "(5725552,)\n",
      "Starting Fold:  2\n",
      "39 39 39\n",
      "36 0.07692307692307687\n",
      "Conv1 1 1 (39, 112, 112, 16) (112, 112, 16) (200704,)\n",
      "bn_Conv1 4 1 (39, 112, 112, 16) (112, 112, 16) (200704,)\n",
      "expanded_conv_depthwise 1 1 (39, 112, 112, 16) (112, 112, 16) (200704,)\n",
      "expanded_conv_depthwise_BN 4 1 (39, 112, 112, 16) (112, 112, 16) (200704,)\n",
      "expanded_conv_project 1 1 (39, 112, 112, 8) (112, 112, 8) (100352,)\n",
      "expanded_conv_project_BN 4 1 (39, 112, 112, 8) (112, 112, 8) (100352,)\n",
      "block_1_expand 1 1 (39, 112, 112, 48) (112, 112, 48) (602112,)\n",
      "block_1_expand_BN 4 1 (39, 112, 112, 48) (112, 112, 48) (602112,)\n",
      "block_1_depthwise 1 1 (39, 56, 56, 48) (56, 56, 48) (150528,)\n",
      "block_1_depthwise_BN 4 1 (39, 56, 56, 48) (56, 56, 48) (150528,)\n",
      "block_1_project 1 1 (39, 56, 56, 8) (56, 56, 8) (25088,)\n",
      "block_1_project_BN 4 1 (39, 56, 56, 8) (56, 56, 8) (25088,)\n",
      "block_2_expand 1 1 (39, 56, 56, 48) (56, 56, 48) (150528,)\n",
      "block_2_expand_BN 4 1 (39, 56, 56, 48) (56, 56, 48) (150528,)\n",
      "block_2_depthwise 1 1 (39, 56, 56, 48) (56, 56, 48) (150528,)\n",
      "block_2_depthwise_BN 4 1 (39, 56, 56, 48) (56, 56, 48) (150528,)\n",
      "block_2_project 1 1 (39, 56, 56, 8) (56, 56, 8) (25088,)\n",
      "block_2_project_BN 4 1 (39, 56, 56, 8) (56, 56, 8) (25088,)\n",
      "block_3_expand 1 1 (39, 56, 56, 48) (56, 56, 48) (150528,)\n",
      "block_3_expand_BN 4 1 (39, 56, 56, 48) (56, 56, 48) (150528,)\n",
      "block_3_depthwise 1 1 (39, 28, 28, 48) (28, 28, 48) (37632,)\n",
      "block_3_depthwise_BN 4 1 (39, 28, 28, 48) (28, 28, 48) (37632,)\n",
      "block_3_project 1 1 (39, 28, 28, 16) (28, 28, 16) (12544,)\n",
      "block_3_project_BN 4 1 (39, 28, 28, 16) (28, 28, 16) (12544,)\n",
      "block_4_expand 1 1 (39, 28, 28, 96) (28, 28, 96) (75264,)\n",
      "block_4_expand_BN 4 1 (39, 28, 28, 96) (28, 28, 96) (75264,)\n",
      "block_4_depthwise 1 1 (39, 28, 28, 96) (28, 28, 96) (75264,)\n",
      "block_4_depthwise_BN 4 1 (39, 28, 28, 96) (28, 28, 96) (75264,)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "block_4_project 1 1 (39, 28, 28, 16) (28, 28, 16) (12544,)\n",
      "block_4_project_BN 4 1 (39, 28, 28, 16) (28, 28, 16) (12544,)\n",
      "block_5_expand 1 1 (39, 28, 28, 96) (28, 28, 96) (75264,)\n",
      "block_5_expand_BN 4 1 (39, 28, 28, 96) (28, 28, 96) (75264,)\n",
      "block_5_depthwise 1 1 (39, 28, 28, 96) (28, 28, 96) (75264,)\n",
      "block_5_depthwise_BN 4 1 (39, 28, 28, 96) (28, 28, 96) (75264,)\n",
      "block_5_project 1 1 (39, 28, 28, 16) (28, 28, 16) (12544,)\n",
      "block_5_project_BN 4 1 (39, 28, 28, 16) (28, 28, 16) (12544,)\n",
      "block_6_expand 1 1 (39, 28, 28, 96) (28, 28, 96) (75264,)\n",
      "block_6_expand_BN 4 1 (39, 28, 28, 96) (28, 28, 96) (75264,)\n",
      "block_6_depthwise 1 1 (39, 14, 14, 96) (14, 14, 96) (18816,)\n",
      "block_6_depthwise_BN 4 1 (39, 14, 14, 96) (14, 14, 96) (18816,)\n",
      "block_6_project 1 1 (39, 14, 14, 24) (14, 14, 24) (4704,)\n",
      "block_6_project_BN 4 1 (39, 14, 14, 24) (14, 14, 24) (4704,)\n",
      "block_7_expand 1 1 (39, 14, 14, 144) (14, 14, 144) (28224,)\n",
      "block_7_expand_BN 4 1 (39, 14, 14, 144) (14, 14, 144) (28224,)\n",
      "block_7_depthwise 1 1 (39, 14, 14, 144) (14, 14, 144) (28224,)\n",
      "block_7_depthwise_BN 4 1 (39, 14, 14, 144) (14, 14, 144) (28224,)\n",
      "block_7_project 1 1 (39, 14, 14, 24) (14, 14, 24) (4704,)\n",
      "block_7_project_BN 4 1 (39, 14, 14, 24) (14, 14, 24) (4704,)\n",
      "block_8_expand 1 1 (39, 14, 14, 144) (14, 14, 144) (28224,)\n",
      "block_8_expand_BN 4 1 (39, 14, 14, 144) (14, 14, 144) (28224,)\n",
      "block_8_depthwise 1 1 (39, 14, 14, 144) (14, 14, 144) (28224,)\n",
      "block_8_depthwise_BN 4 1 (39, 14, 14, 144) (14, 14, 144) (28224,)\n",
      "block_8_project 1 1 (39, 14, 14, 24) (14, 14, 24) (4704,)\n",
      "block_8_project_BN 4 1 (39, 14, 14, 24) (14, 14, 24) (4704,)\n",
      "block_9_expand 1 1 (39, 14, 14, 144) (14, 14, 144) (28224,)\n",
      "block_9_expand_BN 4 1 (39, 14, 14, 144) (14, 14, 144) (28224,)\n",
      "block_9_depthwise 1 1 (39, 14, 14, 144) (14, 14, 144) (28224,)\n",
      "block_9_depthwise_BN 4 1 (39, 14, 14, 144) (14, 14, 144) (28224,)\n",
      "block_9_project 1 1 (39, 14, 14, 24) (14, 14, 24) (4704,)\n",
      "block_9_project_BN 4 1 (39, 14, 14, 24) (14, 14, 24) (4704,)\n",
      "block_10_expand 1 1 (39, 14, 14, 144) (14, 14, 144) (28224,)\n",
      "block_10_expand_BN 4 1 (39, 14, 14, 144) (14, 14, 144) (28224,)\n",
      "block_10_depthwise 1 1 (39, 14, 14, 144) (14, 14, 144) (28224,)\n",
      "block_10_depthwise_BN 4 1 (39, 14, 14, 144) (14, 14, 144) (28224,)\n",
      "block_10_project 1 1 (39, 14, 14, 32) (14, 14, 32) (6272,)\n",
      "block_10_project_BN 4 1 (39, 14, 14, 32) (14, 14, 32) (6272,)\n",
      "block_11_expand 1 1 (39, 14, 14, 192) (14, 14, 192) (37632,)\n",
      "block_11_expand_BN 4 1 (39, 14, 14, 192) (14, 14, 192) (37632,)\n",
      "block_11_depthwise 1 1 (39, 14, 14, 192) (14, 14, 192) (37632,)\n",
      "block_11_depthwise_BN 4 1 (39, 14, 14, 192) (14, 14, 192) (37632,)\n",
      "block_11_project 1 1 (39, 14, 14, 32) (14, 14, 32) (6272,)\n",
      "block_11_project_BN 4 1 (39, 14, 14, 32) (14, 14, 32) (6272,)\n",
      "block_12_expand 1 1 (39, 14, 14, 192) (14, 14, 192) (37632,)\n",
      "block_12_expand_BN 4 1 (39, 14, 14, 192) (14, 14, 192) (37632,)\n",
      "block_12_depthwise 1 1 (39, 14, 14, 192) (14, 14, 192) (37632,)\n",
      "block_12_depthwise_BN 4 1 (39, 14, 14, 192) (14, 14, 192) (37632,)\n",
      "block_12_project 1 1 (39, 14, 14, 32) (14, 14, 32) (6272,)\n",
      "block_12_project_BN 4 1 (39, 14, 14, 32) (14, 14, 32) (6272,)\n",
      "block_13_expand 1 1 (39, 14, 14, 192) (14, 14, 192) (37632,)\n",
      "block_13_expand_BN 4 1 (39, 14, 14, 192) (14, 14, 192) (37632,)\n",
      "block_13_depthwise 1 1 (39, 7, 7, 192) (7, 7, 192) (9408,)\n",
      "block_13_depthwise_BN 4 1 (39, 7, 7, 192) (7, 7, 192) (9408,)\n",
      "block_13_project 1 1 (39, 7, 7, 56) (7, 7, 56) (2744,)\n",
      "block_13_project_BN 4 1 (39, 7, 7, 56) (7, 7, 56) (2744,)\n",
      "block_14_expand 1 1 (39, 7, 7, 336) (7, 7, 336) (16464,)\n",
      "block_14_expand_BN 4 1 (39, 7, 7, 336) (7, 7, 336) (16464,)\n",
      "block_14_depthwise 1 1 (39, 7, 7, 336) (7, 7, 336) (16464,)\n",
      "block_14_depthwise_BN 4 1 (39, 7, 7, 336) (7, 7, 336) (16464,)\n",
      "block_14_project 1 1 (39, 7, 7, 56) (7, 7, 56) (2744,)\n",
      "block_14_project_BN 4 1 (39, 7, 7, 56) (7, 7, 56) (2744,)\n",
      "block_15_expand 1 1 (39, 7, 7, 336) (7, 7, 336) (16464,)\n",
      "block_15_expand_BN 4 1 (39, 7, 7, 336) (7, 7, 336) (16464,)\n",
      "block_15_depthwise 1 1 (39, 7, 7, 336) (7, 7, 336) (16464,)\n",
      "block_15_depthwise_BN 4 1 (39, 7, 7, 336) (7, 7, 336) (16464,)\n",
      "block_15_project 1 1 (39, 7, 7, 56) (7, 7, 56) (2744,)\n",
      "block_15_project_BN 4 1 (39, 7, 7, 56) (7, 7, 56) (2744,)\n",
      "block_16_expand 1 1 (39, 7, 7, 336) (7, 7, 336) (16464,)\n",
      "block_16_expand_BN 4 1 (39, 7, 7, 336) (7, 7, 336) (16464,)\n",
      "block_16_depthwise 1 1 (39, 7, 7, 336) (7, 7, 336) (16464,)\n",
      "block_16_depthwise_BN 4 1 (39, 7, 7, 336) (7, 7, 336) (16464,)\n",
      "block_16_project 1 1 (39, 7, 7, 112) (7, 7, 112) (5488,)\n",
      "block_16_project_BN 4 1 (39, 7, 7, 112) (7, 7, 112) (5488,)\n",
      "Conv_1 1 1 (39, 7, 7, 1280) (7, 7, 1280) (62720,)\n",
      "Conv_1_bn 4 1 (39, 7, 7, 1280) (7, 7, 1280) (62720,)\n",
      "(5725552,)\n",
      "Starting Fold:  3\n",
      "39 39 39\n",
      "35 0.10256410256410253\n",
      "Conv1 1 1 (39, 112, 112, 16) (112, 112, 16) (200704,)\n",
      "bn_Conv1 4 1 (39, 112, 112, 16) (112, 112, 16) (200704,)\n",
      "expanded_conv_depthwise 1 1 (39, 112, 112, 16) (112, 112, 16) (200704,)\n",
      "expanded_conv_depthwise_BN 4 1 (39, 112, 112, 16) (112, 112, 16) (200704,)\n",
      "expanded_conv_project 1 1 (39, 112, 112, 8) (112, 112, 8) (100352,)\n",
      "expanded_conv_project_BN 4 1 (39, 112, 112, 8) (112, 112, 8) (100352,)\n",
      "block_1_expand 1 1 (39, 112, 112, 48) (112, 112, 48) (602112,)\n",
      "block_1_expand_BN 4 1 (39, 112, 112, 48) (112, 112, 48) (602112,)\n",
      "block_1_depthwise 1 1 (39, 56, 56, 48) (56, 56, 48) (150528,)\n",
      "block_1_depthwise_BN 4 1 (39, 56, 56, 48) (56, 56, 48) (150528,)\n",
      "block_1_project 1 1 (39, 56, 56, 8) (56, 56, 8) (25088,)\n",
      "block_1_project_BN 4 1 (39, 56, 56, 8) (56, 56, 8) (25088,)\n",
      "block_2_expand 1 1 (39, 56, 56, 48) (56, 56, 48) (150528,)\n",
      "block_2_expand_BN 4 1 (39, 56, 56, 48) (56, 56, 48) (150528,)\n",
      "block_2_depthwise 1 1 (39, 56, 56, 48) (56, 56, 48) (150528,)\n",
      "block_2_depthwise_BN 4 1 (39, 56, 56, 48) (56, 56, 48) (150528,)\n",
      "block_2_project 1 1 (39, 56, 56, 8) (56, 56, 8) (25088,)\n",
      "block_2_project_BN 4 1 (39, 56, 56, 8) (56, 56, 8) (25088,)\n",
      "block_3_expand 1 1 (39, 56, 56, 48) (56, 56, 48) (150528,)\n",
      "block_3_expand_BN 4 1 (39, 56, 56, 48) (56, 56, 48) (150528,)\n",
      "block_3_depthwise 1 1 (39, 28, 28, 48) (28, 28, 48) (37632,)\n",
      "block_3_depthwise_BN 4 1 (39, 28, 28, 48) (28, 28, 48) (37632,)\n",
      "block_3_project 1 1 (39, 28, 28, 16) (28, 28, 16) (12544,)\n",
      "block_3_project_BN 4 1 (39, 28, 28, 16) (28, 28, 16) (12544,)\n",
      "block_4_expand 1 1 (39, 28, 28, 96) (28, 28, 96) (75264,)\n",
      "block_4_expand_BN 4 1 (39, 28, 28, 96) (28, 28, 96) (75264,)\n",
      "block_4_depthwise 1 1 (39, 28, 28, 96) (28, 28, 96) (75264,)\n",
      "block_4_depthwise_BN 4 1 (39, 28, 28, 96) (28, 28, 96) (75264,)\n",
      "block_4_project 1 1 (39, 28, 28, 16) (28, 28, 16) (12544,)\n",
      "block_4_project_BN 4 1 (39, 28, 28, 16) (28, 28, 16) (12544,)\n",
      "block_5_expand 1 1 (39, 28, 28, 96) (28, 28, 96) (75264,)\n",
      "block_5_expand_BN 4 1 (39, 28, 28, 96) (28, 28, 96) (75264,)\n",
      "block_5_depthwise 1 1 (39, 28, 28, 96) (28, 28, 96) (75264,)\n",
      "block_5_depthwise_BN 4 1 (39, 28, 28, 96) (28, 28, 96) (75264,)\n",
      "block_5_project 1 1 (39, 28, 28, 16) (28, 28, 16) (12544,)\n",
      "block_5_project_BN 4 1 (39, 28, 28, 16) (28, 28, 16) (12544,)\n",
      "block_6_expand 1 1 (39, 28, 28, 96) (28, 28, 96) (75264,)\n",
      "block_6_expand_BN 4 1 (39, 28, 28, 96) (28, 28, 96) (75264,)\n",
      "block_6_depthwise 1 1 (39, 14, 14, 96) (14, 14, 96) (18816,)\n",
      "block_6_depthwise_BN 4 1 (39, 14, 14, 96) (14, 14, 96) (18816,)\n",
      "block_6_project 1 1 (39, 14, 14, 24) (14, 14, 24) (4704,)\n",
      "block_6_project_BN 4 1 (39, 14, 14, 24) (14, 14, 24) (4704,)\n",
      "block_7_expand 1 1 (39, 14, 14, 144) (14, 14, 144) (28224,)\n",
      "block_7_expand_BN 4 1 (39, 14, 14, 144) (14, 14, 144) (28224,)\n",
      "block_7_depthwise 1 1 (39, 14, 14, 144) (14, 14, 144) (28224,)\n",
      "block_7_depthwise_BN 4 1 (39, 14, 14, 144) (14, 14, 144) (28224,)\n",
      "block_7_project 1 1 (39, 14, 14, 24) (14, 14, 24) (4704,)\n",
      "block_7_project_BN 4 1 (39, 14, 14, 24) (14, 14, 24) (4704,)\n",
      "block_8_expand 1 1 (39, 14, 14, 144) (14, 14, 144) (28224,)\n",
      "block_8_expand_BN 4 1 (39, 14, 14, 144) (14, 14, 144) (28224,)\n",
      "block_8_depthwise 1 1 (39, 14, 14, 144) (14, 14, 144) (28224,)\n",
      "block_8_depthwise_BN 4 1 (39, 14, 14, 144) (14, 14, 144) (28224,)\n",
      "block_8_project 1 1 (39, 14, 14, 24) (14, 14, 24) (4704,)\n",
      "block_8_project_BN 4 1 (39, 14, 14, 24) (14, 14, 24) (4704,)\n",
      "block_9_expand 1 1 (39, 14, 14, 144) (14, 14, 144) (28224,)\n",
      "block_9_expand_BN 4 1 (39, 14, 14, 144) (14, 14, 144) (28224,)\n",
      "block_9_depthwise 1 1 (39, 14, 14, 144) (14, 14, 144) (28224,)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "block_9_depthwise_BN 4 1 (39, 14, 14, 144) (14, 14, 144) (28224,)\n",
      "block_9_project 1 1 (39, 14, 14, 24) (14, 14, 24) (4704,)\n",
      "block_9_project_BN 4 1 (39, 14, 14, 24) (14, 14, 24) (4704,)\n",
      "block_10_expand 1 1 (39, 14, 14, 144) (14, 14, 144) (28224,)\n",
      "block_10_expand_BN 4 1 (39, 14, 14, 144) (14, 14, 144) (28224,)\n",
      "block_10_depthwise 1 1 (39, 14, 14, 144) (14, 14, 144) (28224,)\n",
      "block_10_depthwise_BN 4 1 (39, 14, 14, 144) (14, 14, 144) (28224,)\n",
      "block_10_project 1 1 (39, 14, 14, 32) (14, 14, 32) (6272,)\n",
      "block_10_project_BN 4 1 (39, 14, 14, 32) (14, 14, 32) (6272,)\n",
      "block_11_expand 1 1 (39, 14, 14, 192) (14, 14, 192) (37632,)\n",
      "block_11_expand_BN 4 1 (39, 14, 14, 192) (14, 14, 192) (37632,)\n",
      "block_11_depthwise 1 1 (39, 14, 14, 192) (14, 14, 192) (37632,)\n",
      "block_11_depthwise_BN 4 1 (39, 14, 14, 192) (14, 14, 192) (37632,)\n",
      "block_11_project 1 1 (39, 14, 14, 32) (14, 14, 32) (6272,)\n",
      "block_11_project_BN 4 1 (39, 14, 14, 32) (14, 14, 32) (6272,)\n",
      "block_12_expand 1 1 (39, 14, 14, 192) (14, 14, 192) (37632,)\n",
      "block_12_expand_BN 4 1 (39, 14, 14, 192) (14, 14, 192) (37632,)\n",
      "block_12_depthwise 1 1 (39, 14, 14, 192) (14, 14, 192) (37632,)\n",
      "block_12_depthwise_BN 4 1 (39, 14, 14, 192) (14, 14, 192) (37632,)\n",
      "block_12_project 1 1 (39, 14, 14, 32) (14, 14, 32) (6272,)\n",
      "block_12_project_BN 4 1 (39, 14, 14, 32) (14, 14, 32) (6272,)\n",
      "block_13_expand 1 1 (39, 14, 14, 192) (14, 14, 192) (37632,)\n",
      "block_13_expand_BN 4 1 (39, 14, 14, 192) (14, 14, 192) (37632,)\n",
      "block_13_depthwise 1 1 (39, 7, 7, 192) (7, 7, 192) (9408,)\n",
      "block_13_depthwise_BN 4 1 (39, 7, 7, 192) (7, 7, 192) (9408,)\n",
      "block_13_project 1 1 (39, 7, 7, 56) (7, 7, 56) (2744,)\n",
      "block_13_project_BN 4 1 (39, 7, 7, 56) (7, 7, 56) (2744,)\n",
      "block_14_expand 1 1 (39, 7, 7, 336) (7, 7, 336) (16464,)\n",
      "block_14_expand_BN 4 1 (39, 7, 7, 336) (7, 7, 336) (16464,)\n",
      "block_14_depthwise 1 1 (39, 7, 7, 336) (7, 7, 336) (16464,)\n",
      "block_14_depthwise_BN 4 1 (39, 7, 7, 336) (7, 7, 336) (16464,)\n",
      "block_14_project 1 1 (39, 7, 7, 56) (7, 7, 56) (2744,)\n",
      "block_14_project_BN 4 1 (39, 7, 7, 56) (7, 7, 56) (2744,)\n",
      "block_15_expand 1 1 (39, 7, 7, 336) (7, 7, 336) (16464,)\n",
      "block_15_expand_BN 4 1 (39, 7, 7, 336) (7, 7, 336) (16464,)\n",
      "block_15_depthwise 1 1 (39, 7, 7, 336) (7, 7, 336) (16464,)\n",
      "block_15_depthwise_BN 4 1 (39, 7, 7, 336) (7, 7, 336) (16464,)\n",
      "block_15_project 1 1 (39, 7, 7, 56) (7, 7, 56) (2744,)\n",
      "block_15_project_BN 4 1 (39, 7, 7, 56) (7, 7, 56) (2744,)\n",
      "block_16_expand 1 1 (39, 7, 7, 336) (7, 7, 336) (16464,)\n",
      "block_16_expand_BN 4 1 (39, 7, 7, 336) (7, 7, 336) (16464,)\n",
      "block_16_depthwise 1 1 (39, 7, 7, 336) (7, 7, 336) (16464,)\n",
      "block_16_depthwise_BN 4 1 (39, 7, 7, 336) (7, 7, 336) (16464,)\n",
      "block_16_project 1 1 (39, 7, 7, 112) (7, 7, 112) (5488,)\n",
      "block_16_project_BN 4 1 (39, 7, 7, 112) (7, 7, 112) (5488,)\n",
      "Conv_1 1 1 (39, 7, 7, 1280) (7, 7, 1280) (62720,)\n",
      "Conv_1_bn 4 1 (39, 7, 7, 1280) (7, 7, 1280) (62720,)\n",
      "(5725552,)\n",
      "Starting Fold:  4\n",
      "38 38 38\n",
      "34 0.10526315789473684\n",
      "Conv1 1 1 (38, 112, 112, 16) (112, 112, 16) (200704,)\n",
      "bn_Conv1 4 1 (38, 112, 112, 16) (112, 112, 16) (200704,)\n",
      "expanded_conv_depthwise 1 1 (38, 112, 112, 16) (112, 112, 16) (200704,)\n",
      "expanded_conv_depthwise_BN 4 1 (38, 112, 112, 16) (112, 112, 16) (200704,)\n",
      "expanded_conv_project 1 1 (38, 112, 112, 8) (112, 112, 8) (100352,)\n",
      "expanded_conv_project_BN 4 1 (38, 112, 112, 8) (112, 112, 8) (100352,)\n",
      "block_1_expand 1 1 (38, 112, 112, 48) (112, 112, 48) (602112,)\n",
      "block_1_expand_BN 4 1 (38, 112, 112, 48) (112, 112, 48) (602112,)\n",
      "block_1_depthwise 1 1 (38, 56, 56, 48) (56, 56, 48) (150528,)\n",
      "block_1_depthwise_BN 4 1 (38, 56, 56, 48) (56, 56, 48) (150528,)\n",
      "block_1_project 1 1 (38, 56, 56, 8) (56, 56, 8) (25088,)\n",
      "block_1_project_BN 4 1 (38, 56, 56, 8) (56, 56, 8) (25088,)\n",
      "block_2_expand 1 1 (38, 56, 56, 48) (56, 56, 48) (150528,)\n",
      "block_2_expand_BN 4 1 (38, 56, 56, 48) (56, 56, 48) (150528,)\n",
      "block_2_depthwise 1 1 (38, 56, 56, 48) (56, 56, 48) (150528,)\n",
      "block_2_depthwise_BN 4 1 (38, 56, 56, 48) (56, 56, 48) (150528,)\n",
      "block_2_project 1 1 (38, 56, 56, 8) (56, 56, 8) (25088,)\n",
      "block_2_project_BN 4 1 (38, 56, 56, 8) (56, 56, 8) (25088,)\n",
      "block_3_expand 1 1 (38, 56, 56, 48) (56, 56, 48) (150528,)\n",
      "block_3_expand_BN 4 1 (38, 56, 56, 48) (56, 56, 48) (150528,)\n",
      "block_3_depthwise 1 1 (38, 28, 28, 48) (28, 28, 48) (37632,)\n",
      "block_3_depthwise_BN 4 1 (38, 28, 28, 48) (28, 28, 48) (37632,)\n",
      "block_3_project 1 1 (38, 28, 28, 16) (28, 28, 16) (12544,)\n",
      "block_3_project_BN 4 1 (38, 28, 28, 16) (28, 28, 16) (12544,)\n",
      "block_4_expand 1 1 (38, 28, 28, 96) (28, 28, 96) (75264,)\n",
      "block_4_expand_BN 4 1 (38, 28, 28, 96) (28, 28, 96) (75264,)\n",
      "block_4_depthwise 1 1 (38, 28, 28, 96) (28, 28, 96) (75264,)\n",
      "block_4_depthwise_BN 4 1 (38, 28, 28, 96) (28, 28, 96) (75264,)\n",
      "block_4_project 1 1 (38, 28, 28, 16) (28, 28, 16) (12544,)\n",
      "block_4_project_BN 4 1 (38, 28, 28, 16) (28, 28, 16) (12544,)\n",
      "block_5_expand 1 1 (38, 28, 28, 96) (28, 28, 96) (75264,)\n",
      "block_5_expand_BN 4 1 (38, 28, 28, 96) (28, 28, 96) (75264,)\n",
      "block_5_depthwise 1 1 (38, 28, 28, 96) (28, 28, 96) (75264,)\n",
      "block_5_depthwise_BN 4 1 (38, 28, 28, 96) (28, 28, 96) (75264,)\n",
      "block_5_project 1 1 (38, 28, 28, 16) (28, 28, 16) (12544,)\n",
      "block_5_project_BN 4 1 (38, 28, 28, 16) (28, 28, 16) (12544,)\n",
      "block_6_expand 1 1 (38, 28, 28, 96) (28, 28, 96) (75264,)\n",
      "block_6_expand_BN 4 1 (38, 28, 28, 96) (28, 28, 96) (75264,)\n",
      "block_6_depthwise 1 1 (38, 14, 14, 96) (14, 14, 96) (18816,)\n",
      "block_6_depthwise_BN 4 1 (38, 14, 14, 96) (14, 14, 96) (18816,)\n",
      "block_6_project 1 1 (38, 14, 14, 24) (14, 14, 24) (4704,)\n",
      "block_6_project_BN 4 1 (38, 14, 14, 24) (14, 14, 24) (4704,)\n",
      "block_7_expand 1 1 (38, 14, 14, 144) (14, 14, 144) (28224,)\n",
      "block_7_expand_BN 4 1 (38, 14, 14, 144) (14, 14, 144) (28224,)\n",
      "block_7_depthwise 1 1 (38, 14, 14, 144) (14, 14, 144) (28224,)\n",
      "block_7_depthwise_BN 4 1 (38, 14, 14, 144) (14, 14, 144) (28224,)\n",
      "block_7_project 1 1 (38, 14, 14, 24) (14, 14, 24) (4704,)\n",
      "block_7_project_BN 4 1 (38, 14, 14, 24) (14, 14, 24) (4704,)\n",
      "block_8_expand 1 1 (38, 14, 14, 144) (14, 14, 144) (28224,)\n",
      "block_8_expand_BN 4 1 (38, 14, 14, 144) (14, 14, 144) (28224,)\n",
      "block_8_depthwise 1 1 (38, 14, 14, 144) (14, 14, 144) (28224,)\n",
      "block_8_depthwise_BN 4 1 (38, 14, 14, 144) (14, 14, 144) (28224,)\n",
      "block_8_project 1 1 (38, 14, 14, 24) (14, 14, 24) (4704,)\n",
      "block_8_project_BN 4 1 (38, 14, 14, 24) (14, 14, 24) (4704,)\n",
      "block_9_expand 1 1 (38, 14, 14, 144) (14, 14, 144) (28224,)\n",
      "block_9_expand_BN 4 1 (38, 14, 14, 144) (14, 14, 144) (28224,)\n",
      "block_9_depthwise 1 1 (38, 14, 14, 144) (14, 14, 144) (28224,)\n",
      "block_9_depthwise_BN 4 1 (38, 14, 14, 144) (14, 14, 144) (28224,)\n",
      "block_9_project 1 1 (38, 14, 14, 24) (14, 14, 24) (4704,)\n",
      "block_9_project_BN 4 1 (38, 14, 14, 24) (14, 14, 24) (4704,)\n",
      "block_10_expand 1 1 (38, 14, 14, 144) (14, 14, 144) (28224,)\n",
      "block_10_expand_BN 4 1 (38, 14, 14, 144) (14, 14, 144) (28224,)\n",
      "block_10_depthwise 1 1 (38, 14, 14, 144) (14, 14, 144) (28224,)\n",
      "block_10_depthwise_BN 4 1 (38, 14, 14, 144) (14, 14, 144) (28224,)\n",
      "block_10_project 1 1 (38, 14, 14, 32) (14, 14, 32) (6272,)\n",
      "block_10_project_BN 4 1 (38, 14, 14, 32) (14, 14, 32) (6272,)\n",
      "block_11_expand 1 1 (38, 14, 14, 192) (14, 14, 192) (37632,)\n",
      "block_11_expand_BN 4 1 (38, 14, 14, 192) (14, 14, 192) (37632,)\n",
      "block_11_depthwise 1 1 (38, 14, 14, 192) (14, 14, 192) (37632,)\n",
      "block_11_depthwise_BN 4 1 (38, 14, 14, 192) (14, 14, 192) (37632,)\n",
      "block_11_project 1 1 (38, 14, 14, 32) (14, 14, 32) (6272,)\n",
      "block_11_project_BN 4 1 (38, 14, 14, 32) (14, 14, 32) (6272,)\n",
      "block_12_expand 1 1 (38, 14, 14, 192) (14, 14, 192) (37632,)\n",
      "block_12_expand_BN 4 1 (38, 14, 14, 192) (14, 14, 192) (37632,)\n",
      "block_12_depthwise 1 1 (38, 14, 14, 192) (14, 14, 192) (37632,)\n",
      "block_12_depthwise_BN 4 1 (38, 14, 14, 192) (14, 14, 192) (37632,)\n",
      "block_12_project 1 1 (38, 14, 14, 32) (14, 14, 32) (6272,)\n",
      "block_12_project_BN 4 1 (38, 14, 14, 32) (14, 14, 32) (6272,)\n",
      "block_13_expand 1 1 (38, 14, 14, 192) (14, 14, 192) (37632,)\n",
      "block_13_expand_BN 4 1 (38, 14, 14, 192) (14, 14, 192) (37632,)\n",
      "block_13_depthwise 1 1 (38, 7, 7, 192) (7, 7, 192) (9408,)\n",
      "block_13_depthwise_BN 4 1 (38, 7, 7, 192) (7, 7, 192) (9408,)\n",
      "block_13_project 1 1 (38, 7, 7, 56) (7, 7, 56) (2744,)\n",
      "block_13_project_BN 4 1 (38, 7, 7, 56) (7, 7, 56) (2744,)\n",
      "block_14_expand 1 1 (38, 7, 7, 336) (7, 7, 336) (16464,)\n",
      "block_14_expand_BN 4 1 (38, 7, 7, 336) (7, 7, 336) (16464,)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "block_14_depthwise 1 1 (38, 7, 7, 336) (7, 7, 336) (16464,)\n",
      "block_14_depthwise_BN 4 1 (38, 7, 7, 336) (7, 7, 336) (16464,)\n",
      "block_14_project 1 1 (38, 7, 7, 56) (7, 7, 56) (2744,)\n",
      "block_14_project_BN 4 1 (38, 7, 7, 56) (7, 7, 56) (2744,)\n",
      "block_15_expand 1 1 (38, 7, 7, 336) (7, 7, 336) (16464,)\n",
      "block_15_expand_BN 4 1 (38, 7, 7, 336) (7, 7, 336) (16464,)\n",
      "block_15_depthwise 1 1 (38, 7, 7, 336) (7, 7, 336) (16464,)\n",
      "block_15_depthwise_BN 4 1 (38, 7, 7, 336) (7, 7, 336) (16464,)\n",
      "block_15_project 1 1 (38, 7, 7, 56) (7, 7, 56) (2744,)\n",
      "block_15_project_BN 4 1 (38, 7, 7, 56) (7, 7, 56) (2744,)\n",
      "block_16_expand 1 1 (38, 7, 7, 336) (7, 7, 336) (16464,)\n",
      "block_16_expand_BN 4 1 (38, 7, 7, 336) (7, 7, 336) (16464,)\n",
      "block_16_depthwise 1 1 (38, 7, 7, 336) (7, 7, 336) (16464,)\n",
      "block_16_depthwise_BN 4 1 (38, 7, 7, 336) (7, 7, 336) (16464,)\n",
      "block_16_project 1 1 (38, 7, 7, 112) (7, 7, 112) (5488,)\n",
      "block_16_project_BN 4 1 (38, 7, 7, 112) (7, 7, 112) (5488,)\n",
      "Conv_1 1 1 (38, 7, 7, 1280) (7, 7, 1280) (62720,)\n",
      "Conv_1_bn 4 1 (38, 7, 7, 1280) (7, 7, 1280) (62720,)\n",
      "(5725552,)\n"
     ]
    }
   ],
   "source": [
    "out_r = []\n",
    "\n",
    "image_list_test = '../../data/pkl_mobile/'+p+'_'+str(model_name)+'_image_list_test.txt'\n",
    "with open(image_list_test,'w+') as f:\n",
    "    for i in im_test:\n",
    "        f.write(i+'\\n')\n",
    "\n",
    "kf = KFold(n_splits= k)\n",
    "fold = 1\n",
    "fp_name = '../../data/pkl_mobile/'+str(p)+'_no_lesion_performance.txt'\n",
    "fp = open(fp_name,'a+')\n",
    "for train_index, valid_index in kf.split(im_train):\n",
    "    print(\"Starting Fold: \", fold)\n",
    "    im_valid_train = [im_train[i] for i in train_index] \n",
    "    im_valid_test = [im_train[i] for i in valid_index]\n",
    "    \n",
    "    image_list_train = '../../data/pkl_mobile/'+p+'_image_list_train_fold_'+str(fold)+'.txt'\n",
    "    with open(image_list_train,'w+') as f:\n",
    "        for i in im_valid_train:\n",
    "            f.write(i+'\\n')\n",
    "    \n",
    "    image_list_valid = '../../data/pkl_mobile/'+p+'_image_list_valid_fold_'+str(fold)+'.txt'\n",
    "    with open(image_list_valid,'w+') as f:\n",
    "        for i in im_valid_test:\n",
    "            f.write(i+'\\n')\n",
    "    \n",
    "   \n",
    "    im_temp = preprocess_image_batch(im_valid_test,img_size=(256,256), crop_size=(224,224), color_mode=\"rgb\")\n",
    "    out = model.predict(im_temp,batch_size=64)\n",
    "\n",
    "    true_valid_wids = []\n",
    "    for i in im_valid_test:\n",
    "            temp1 = i.split('/')[4]\n",
    "            temp = temp1.split('.')[0].split('_')[2]\n",
    "            true_valid_wids.append(truth[int(temp)][1])\n",
    "    \n",
    "    predicted_valid_wids = []\n",
    "    for i in range(len(im_valid_test)):\n",
    "        #print im_list[i], pprint_output(out[i]), true_wids[i]\n",
    "        predicted_valid_wids.append(pprint_output(out[i]))\n",
    "        \n",
    "    count, error  = top5accuracy(true_valid_wids, predicted_valid_wids)\n",
    "    \n",
    "    fp.write(str(p)+' '+str(fold)+' '+str(count)+' '+str(len(im_valid_test))+' '+str(error)+'\\n')\n",
    "\n",
    "    \n",
    "    print(len(true_valid_wids), len(predicted_valid_wids), len(im_valid_test))\n",
    "    print(count, error)\n",
    "    \n",
    "    \n",
    "    #}\n",
    "    # Code snippet to get the activation values and saving information{\n",
    "    data = np.array([])\n",
    "\n",
    "    i = 0\n",
    "    result ={}\n",
    "    for layer in model.layers:\n",
    "        weights = layer.get_weights()\n",
    "        if len(weights) > 0:\n",
    "            activations = get_activations(model,i,im_temp)\n",
    "            if result.get(layer.name, None) is None:\n",
    "                result[layer.name] = activations[0]\n",
    "                temp = np.mean(activations[0], axis=0).ravel()\n",
    "                if layer.name != 'Logits':\n",
    "                    print(layer.name,len(weights),len(activations), activations[0].shape, np.mean(activations[0], axis=0).shape, temp.shape)\n",
    "                    data = np.append(data, temp)\n",
    "        i += 1\n",
    "    print(data.shape)\n",
    "    out_r.append(data)\n",
    "    fold += 1\n",
    "    \n",
    "fp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "animate_fold_1_train_MobileNetV2 5725552\n",
      "animate_fold_2_train_MobileNetV2 5725552\n",
      "animate_fold_3_train_MobileNetV2 5725552\n",
      "animate_fold_4_train_MobileNetV2 5725552\n"
     ]
    }
   ],
   "source": [
    "#Saving all the data into pkl files\n",
    "for i in range(k):\n",
    "    name = p+'_fold_'+str(i+1)+'_train_'+model_name\n",
    "    out_data = out_r[i]\n",
    "    with open('../../data/pkl_mobile/'+name+'.pkl', 'wb') as f:\n",
    "        pickle.dump(out_data, f)\n",
    "    print(name, len(out_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "from keras import backend as K \n",
    "\n",
    "K.clear_session()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
