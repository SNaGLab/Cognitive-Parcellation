{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To hide warnings export PYTHONWARNINGS=\"ignore\"\n",
    "#Imports{\n",
    "\n",
    "import os\n",
    "from os.path import dirname\n",
    "from os.path import join\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' #Cha\n",
    "\n",
    "\n",
    "import sys\n",
    "import warnings\n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    \n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from imageio import imread\n",
    "from scipy.io import loadmat\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "K.set_image_data_format('channels_last')\n",
    "from keras_applications import imagenet_utils as utils\n",
    "from keras.layers import Lambda\n",
    "\n",
    "import PIL.Image\n",
    "#}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confirm TensorFlow sees the GPU\n",
    "from tensorflow.python.client import device_lib\n",
    "assert 'GPU' in str(device_lib.list_local_devices())\n",
    "\n",
    "# confirm Keras sees the GPU (for TensorFlow 1.X + Keras)\n",
    "from keras import backend\n",
    "assert len(backend.tensorflow_backend._get_available_gpus()) > 0\n",
    "\n",
    "# confirm PyTorch sees the GPU\n",
    "#from torch import cuda\n",
    "#assert cuda.is_available()\n",
    "#assert cuda.device_count() > 0\n",
    "#print(cuda.get_device_name(cuda.current_device()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code snippet needed to read activation values from each layer of the pre-trained artificial neural networks\n",
    "def get_activations(model, layer, X_batch):\n",
    "    #keras.backend.function(inputs, outputs, updates=None)\n",
    "    get_activations = keras.backend.function([model.layers[0].input, keras.backend.learning_phase()], [model.layers[layer].output,])\n",
    "    #The learning phase flag is a bool tensor (0 = test, 1 = train)\n",
    "    activations = get_activations([X_batch,0])\n",
    "    return activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to pre-process the input image to ensure uniform size and color\n",
    "def preprocess_image_batch(image_paths, img_size=None, crop_size=None, color_mode='rgb', out=None):\n",
    "    \"\"\"\n",
    "    Consistent preprocessing of images batches\n",
    "    :param image_paths: iterable: images to process\n",
    "    :param crop_size: tuple: crop images if specified\n",
    "    :param img_size: tuple: resize images if specified\n",
    "    :param color_mode: Use rgb or change to bgr mode based on type of model you want to use\n",
    "    :param out: append output to this iterable if specified\n",
    "    \"\"\"\n",
    "    img_list = []\n",
    "\n",
    "    for im_path in image_paths:\n",
    "        '''\n",
    "        img = imread(im_path,as_gray=False, pilmode=\"RGB\")\n",
    "        #print im_path\n",
    "        #print img.shape\n",
    "        if img_size:\n",
    "            img = resize(img, img_size)\n",
    "\n",
    "        img = img.astype('float32')\n",
    "        # We normalize the colors (in RGB space) with the empirical means on the training set\n",
    "        img[:, :, 0] -= 123.68\n",
    "        img[:, :, 1] -= 116.779\n",
    "        img[:, :, 2] -= 103.939\n",
    "        # We permute the colors to get them in the BGR order\n",
    "        if color_mode == 'bgr':\n",
    "            img[:, :, [0, 1, 2]] = img[:, :, [2, 1, 0]]\n",
    "        img = img.transpose((2, 0, 1))\n",
    "\n",
    "        if crop_size:\n",
    "            img = img[:, (img_size[0] - crop_size[0]) // 2:(img_size[0] + crop_size[0]) // 2\n",
    "            , (img_size[1] - crop_size[1]) // 2:(img_size[1] + crop_size[1]) // 2]\n",
    "\n",
    "        img_list.append(img)\n",
    "        '''\n",
    "        size = 224\n",
    "        ret = PIL.Image.open(im_path)\n",
    "        ret = ret.resize((size, size))\n",
    "        ret = np.asarray(ret, dtype=np.uint8).astype(np.float32)\n",
    "        if ret.ndim == 2:\n",
    "            ret.resize((size, size, 1))\n",
    "            ret = np.repeat(ret, 3, axis=-1)\n",
    "        #ret = ret.transpose((2, 0, 1))\n",
    "        #ret = np.flip(ret,0)\n",
    "        global backend\n",
    "        x = utils.preprocess_input(ret, \n",
    "            data_format=backend.image_data_format())\n",
    "        img_list.append(x)\n",
    "\n",
    "\n",
    "    try:\n",
    "        img_batch = np.stack(img_list, axis=0)\n",
    "    except:\n",
    "        print(im_path)\n",
    "        raise ValueError('when img_size and crop_size are None, images'\n",
    "                ' in image_paths must have the same shapes.')\n",
    "\n",
    "    if out is not None and hasattr(out, 'append'):\n",
    "        out.append(img_batch)\n",
    "    else:\n",
    "        return img_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to predict the top 5 accuracy\n",
    "def top5accuracy(true, predicted):\n",
    "    assert len(true) == len(predicted)\n",
    "    result = []\n",
    "    flag  = 0\n",
    "    for i in range(len(true)):\n",
    "        flag  = 0\n",
    "        temp = true[i]\n",
    "        for j in predicted[i][0:5]:\n",
    "            if j == temp:\n",
    "                flag = 1\n",
    "                break\n",
    "        if flag == 1:\n",
    "            result.append(1)\n",
    "        else:\n",
    "            result.append(0)\n",
    "    counter = 0.\n",
    "    for i in result:\n",
    "        if i == 1:\n",
    "            counter += 1.\n",
    "    error = 1.0 - counter/float(len(result))\n",
    "    #print len(np.where(np.asarray(result) == 1)[0])\n",
    "    return len(np.where(np.asarray(result) == 1)[0]), error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the details of all the 1000 classes and the function to conver the synset id to words{\n",
    "meta_clsloc_file = '../../data/meta_clsloc.mat'\n",
    "synsets = loadmat(meta_clsloc_file)['synsets'][0]\n",
    "synsets_imagenet_sorted = sorted([(int(s[0]), str(s[1][0])) for s in synsets[:1000]],key=lambda v: v[1])\n",
    "corr = {}\n",
    "for j in range(1000):\n",
    "    corr[synsets_imagenet_sorted[j][0]] = j\n",
    "\n",
    "corr_inv = {}\n",
    "for j in range(1, 1001):\n",
    "    corr_inv[corr[j]] = j\n",
    "\n",
    "def id_to_words(id_):\n",
    "    return synsets[corr_inv[id_] - 1][2][0]\n",
    "\n",
    "def pprint_output(out, n_max_synsets=10):\n",
    "    wids = []\n",
    "    best_ids = out.argsort()[::-1][:10]\n",
    "    for u in best_ids:\n",
    "        wids.append(str(synsets[corr_inv[u] - 1][1][0]))\n",
    "    #print('%.2f' % round(100 * out[u], 2) + ' : ' + id_to_words(u)+' '+ str(synsets[corr_inv[u] - 1][1][0]))\n",
    "    return wids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code snippet to load the ground truth labels to measure the performance{\n",
    "truth = {}\n",
    "with open('../../data/ILSVRC2014_clsloc_validation_ground_truth.txt') as f:\n",
    "    line_num = 1\n",
    "    for line in f.readlines():\n",
    "        ind_ = int(line)\n",
    "        temp  = None\n",
    "        for i in synsets_imagenet_sorted:\n",
    "            if i[0] == ind_:\n",
    "                temp = i\n",
    "        #print ind_,temp\n",
    "        if temp != None:\n",
    "            truth[line_num] = temp\n",
    "        else:\n",
    "            print('##########', ind_)\n",
    "            pass\n",
    "        line_num += 1\n",
    "#}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Resnet 169 models for Keras.\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "backend= keras.backend\n",
    "layers = keras.layers\n",
    "models = keras.models\n",
    "keras_utils = keras.utils\n",
    "\n",
    "\n",
    "BASE_WEIGHTS_PATH = (\n",
    "    'https://github.com/keras-team/keras-applications/'\n",
    "    'releases/download/resnet/')\n",
    "WEIGHTS_HASHES = {\n",
    "    'resnet50': ('2cb95161c43110f7111970584f804107',\n",
    "                 '4d473c1dd8becc155b73f8504c6f6626'),\n",
    "    'resnet101': ('f1aeb4b969a6efcfb50fad2f0c20cfc5',\n",
    "                  '88cf7a10940856eca736dc7b7e228a21'),\n",
    "    'resnet152': ('100835be76be38e30d865e96f2aaae62',\n",
    "                  'ee4c566cf9a93f14d82f913c2dc6dd0c'),\n",
    "    'resnet50v2': ('3ef43a0b657b3be2300d5770ece849e0',\n",
    "                   'fac2f116257151a9d068a22e544a4917'),\n",
    "    'resnet101v2': ('6343647c601c52e1368623803854d971',\n",
    "                    'c0ed64b8031c3730f411d2eb4eea35b5'),\n",
    "    'resnet152v2': ('a49b44d1979771252814e80f8ec446f9',\n",
    "                    'ed17cf2e0169df9d443503ef94b23b33'),\n",
    "    'resnext50': ('67a5b30d522ed92f75a1f16eef299d1a',\n",
    "                  '62527c363bdd9ec598bed41947b379fc'),\n",
    "    'resnext101': ('34fb605428fcc7aa4d62f44404c11509',\n",
    "                   '0f678c91647380debd923963594981b3')\n",
    "}\n",
    "\n",
    "def block1(x, filters, kernel_size=3, stride=1,\n",
    "           conv_shortcut=True, name=None, lambda_mask = None, input_size = None):\n",
    "    \"\"\"A residual block.\n",
    "\n",
    "    # Arguments\n",
    "        x: input tensor.\n",
    "        filters: integer, filters of the bottleneck layer.\n",
    "        kernel_size: default 3, kernel size of the bottleneck layer.\n",
    "        stride: default 1, stride of the first layer.\n",
    "        conv_shortcut: default True, use convolution shortcut if True,\n",
    "            otherwise identity shortcut.\n",
    "        name: string, block label.\n",
    "\n",
    "    # Returns\n",
    "        Output tensor for the residual block.\n",
    "    \"\"\"\n",
    "    global start_index, end_index, debug\n",
    "    bn_axis = 3 if backend.image_data_format() == 'channels_last' else 1\n",
    "    #print(input_size, filters)\n",
    "    if conv_shortcut is True:\n",
    "        shortcut = layers.Conv2D(4 * filters, 1, strides=stride,name=name + '_0_conv')(x)\n",
    "        #################\n",
    "        if lambda_mask is not None:\n",
    "            start_index = end_index\n",
    "            end_index = start_index + (input_size* input_size* filters * 4)\n",
    "            if debug:\n",
    "                print(name + '_0_conv',start_index,end_index)\n",
    "            shortcut_mask  = np.reshape(lambda_mask[start_index:end_index], (input_size, input_size, filters * 4))\n",
    "        else:\n",
    "            shortcut_mask = np.ones(shape=((input_size, input_size, filters * 4)))\n",
    "\n",
    "        shortcut_mask  = backend.variable(shortcut_mask)\n",
    "        shortcut_mask = Lambda(lambda z: z * shortcut_mask)(shortcut)\n",
    "        #################\n",
    "        shortcut = layers.BatchNormalization(axis=bn_axis, epsilon=1.001e-5,\n",
    "                                             name=name + '_0_bn')(shortcut)\n",
    "        #################\n",
    "        if lambda_mask is not None:\n",
    "            start_index = end_index\n",
    "            end_index = start_index + (input_size* input_size* filters * 4)\n",
    "            if debug:\n",
    "                print(name + '_0_bn',start_index,end_index)\n",
    "            shortcut_mask  = np.reshape(lambda_mask[start_index:end_index], (input_size, input_size, filters * 4))\n",
    "        else:\n",
    "            shortcut_mask = np.ones(shape=((input_size, input_size, filters * 4)))\n",
    "\n",
    "        shortcut_mask  = backend.variable(shortcut_mask)\n",
    "        shortcut_mask = Lambda(lambda z: z * shortcut_mask)(shortcut)\n",
    "        #################\n",
    "    else:\n",
    "        shortcut = x\n",
    "    \n",
    "\n",
    "    x = layers.Conv2D(filters, 1, strides=stride, name=name + '_1_conv')(x)\n",
    "    #################\n",
    "    if lambda_mask is not None:\n",
    "        start_index = end_index\n",
    "        end_index = start_index + (input_size* input_size* (filters))\n",
    "        if debug:\n",
    "            print(name + '_1_conv',start_index,end_index)\n",
    "        x_mask  = np.reshape(lambda_mask[start_index:end_index], (input_size, input_size, filters))\n",
    "    else:\n",
    "        x_mask = np.ones(shape=((input_size, input_size, filters)))\n",
    "\n",
    "    x_mask  = backend.variable(x_mask)\n",
    "    x = Lambda(lambda z: z * x_mask)(x)\n",
    "    ####################\n",
    "    \n",
    "    x = layers.BatchNormalization(axis=bn_axis, epsilon=1.001e-5, name=name + '_1_bn')(x)\n",
    "    x = layers.Activation('relu', name=name + '_1_relu')(x)\n",
    "    #################\n",
    "    if lambda_mask is not None:\n",
    "        start_index = end_index\n",
    "        end_index = start_index + (input_size* input_size* (filters))\n",
    "        if debug:\n",
    "            print(name + '_1_bn',start_index,end_index)\n",
    "        x_mask  = np.reshape(lambda_mask[start_index:end_index], (input_size, input_size, filters))\n",
    "    else:\n",
    "        x_mask = np.ones(shape=((input_size, input_size, filters)))\n",
    "\n",
    "    x_mask  = backend.variable(x_mask)\n",
    "    x = Lambda(lambda z: z * x_mask)(x)\n",
    "    ####################\n",
    "\n",
    "    x = layers.Conv2D(filters, kernel_size, padding='SAME',\n",
    "                      name=name + '_2_conv')(x)\n",
    "   #################\n",
    "    if lambda_mask is not None:\n",
    "        start_index = end_index\n",
    "        end_index = start_index + (input_size* input_size* (filters))\n",
    "        if debug:\n",
    "            print(name + '_2_conv',start_index,end_index)\n",
    "        x_mask  = np.reshape(lambda_mask[start_index:end_index], (input_size, input_size, filters))\n",
    "    else:\n",
    "        x_mask = np.ones(shape=((input_size, input_size, filters)))\n",
    "\n",
    "    x_mask  = backend.variable(x_mask)\n",
    "    x = Lambda(lambda z: z * x_mask)(x)\n",
    "    ####################\n",
    "    x = layers.BatchNormalization(axis=bn_axis, epsilon=1.001e-5,name=name + '_2_bn')(x)\n",
    "    x = layers.Activation('relu', name=name + '_2_relu')(x)\n",
    "    #################\n",
    "    if lambda_mask is not None:\n",
    "        start_index = end_index\n",
    "        end_index = start_index + (input_size* input_size* (filters))\n",
    "        if debug:\n",
    "            print(name + '_2_bn',start_index,end_index)\n",
    "        x_mask  = np.reshape(lambda_mask[start_index:end_index], (input_size, input_size, filters))\n",
    "    else:\n",
    "        x_mask = np.ones(shape=((input_size, input_size, filters)))\n",
    "\n",
    "    x_mask  = backend.variable(x_mask)\n",
    "    x = Lambda(lambda z: z * x_mask)(x)\n",
    "    ####################\n",
    "\n",
    "    x = layers.Conv2D(4 * filters, 1, name=name + '_3_conv')(x)\n",
    "    #################\n",
    "    if lambda_mask is not None:\n",
    "        start_index = end_index\n",
    "        end_index = start_index + (input_size* input_size* (filters) * 4)\n",
    "        if debug:\n",
    "            print(name + '_3_conv',start_index,end_index)\n",
    "        x_mask  = np.reshape(lambda_mask[start_index:end_index], (input_size, input_size, filters * 4))\n",
    "    else:\n",
    "        x_mask = np.ones(shape=((input_size, input_size, filters * 4)))\n",
    "\n",
    "    x_mask  = backend.variable(x_mask)\n",
    "    x = Lambda(lambda z: z * x_mask)(x)\n",
    "    ####################\n",
    "    x = layers.BatchNormalization(axis=bn_axis, epsilon=1.001e-5,name=name + '_3_bn')(x)\n",
    "\n",
    "    #################\n",
    "    if lambda_mask is not None:\n",
    "        start_index = end_index\n",
    "        end_index = start_index + (input_size* input_size* (filters) * 4)\n",
    "        if debug:\n",
    "            print(name + '_3_bn',start_index,end_index)\n",
    "        x_mask  = np.reshape(lambda_mask[start_index:end_index], (input_size, input_size, filters * 4))\n",
    "    else:\n",
    "        x_mask = np.ones(shape=((input_size, input_size, filters * 4)))\n",
    "\n",
    "    x_mask  = backend.variable(x_mask)\n",
    "    x = Lambda(lambda z: z * x_mask)(x)\n",
    "    ####################\n",
    "    x = layers.Add(name=name + '_add')([shortcut, x])\n",
    "    x = layers.Activation('relu', name=name + '_out')(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "def stack1(x, filters, blocks, stride1=2, name=None, lambda_mask=None, input_size = None):\n",
    "    \"\"\"A set of stacked residual blocks.\n",
    "\n",
    "    # Arguments\n",
    "        x: input tensor.\n",
    "        filters: integer, filters of the bottleneck layer in a block.\n",
    "        blocks: integer, blocks in the stacked blocks.\n",
    "        stride1: default 2, stride of the first layer in the first block.\n",
    "        name: string, stack label.\n",
    "\n",
    "    # Returns\n",
    "        Output tensor for the stacked blocks.\n",
    "    \"\"\"\n",
    "    x = block1(x, filters, stride=stride1, name=name + '_block1', lambda_mask=lambda_mask, input_size = input_size)\n",
    "    for i in range(2, blocks + 1):\n",
    "        x = block1(x, filters, conv_shortcut=False, name=name + '_block' + str(i), lambda_mask=lambda_mask, input_size = input_size)\n",
    "    return x\n",
    "\n",
    "\n",
    "def block2(x, filters, kernel_size=3, stride=1,\n",
    "           conv_shortcut=False, name=None):\n",
    "    \"\"\"A residual block.\n",
    "\n",
    "    # Arguments\n",
    "        x: input tensor.\n",
    "        filters: integer, filters of the bottleneck layer.\n",
    "        kernel_size: default 3, kernel size of the bottleneck layer.\n",
    "        stride: default 1, stride of the first layer.\n",
    "        conv_shortcut: default False, use convolution shortcut if True,\n",
    "            otherwise identity shortcut.\n",
    "        name: string, block label.\n",
    "\n",
    "    # Returns\n",
    "        Output tensor for the residual block.\n",
    "    \"\"\"\n",
    "    bn_axis = 3 if backend.image_data_format() == 'channels_last' else 1\n",
    "\n",
    "    preact = layers.BatchNormalization(axis=bn_axis, epsilon=1.001e-5,name=name + '_preact_bn')(x)\n",
    "    preact = layers.Activation('relu', name=name + '_preact_relu')(preact)\n",
    "\n",
    "    if conv_shortcut is True:\n",
    "        shortcut = layers.Conv2D(4 * filters, 1, strides=stride, name=name + '_0_conv')(preact)\n",
    "    else:\n",
    "        shortcut = layers.MaxPooling2D(1, strides=stride)(x) if stride > 1 else x\n",
    "\n",
    "    x = layers.Conv2D(filters, 1, strides=1, use_bias=False, name=name + '_1_conv')(preact)\n",
    "    x = layers.BatchNormalization(axis=bn_axis, epsilon=1.001e-5, name=name + '_1_bn')(x)\n",
    "    x = layers.Activation('relu', name=name + '_1_relu')(x)\n",
    "\n",
    "    x = layers.ZeroPadding2D(padding=((1, 1), (1, 1)), name=name + '_2_pad')(x)\n",
    "    x = layers.Conv2D(filters, kernel_size, strides=stride, use_bias=False, name=name + '_2_conv')(x)\n",
    "    x = layers.BatchNormalization(axis=bn_axis, epsilon=1.001e-5, name=name + '_2_bn')(x)\n",
    "    x = layers.Activation('relu', name=name + '_2_relu')(x)\n",
    "\n",
    "    x = layers.Conv2D(4 * filters, 1, name=name + '_3_conv')(x)\n",
    "    x = layers.Add(name=name + '_out')([shortcut, x])\n",
    "    return x\n",
    "\n",
    "\n",
    "def stack2(x, filters, blocks, stride1=2, name=None):\n",
    "    \"\"\"A set of stacked residual blocks.\n",
    "\n",
    "    # Arguments\n",
    "        x: input tensor.\n",
    "        filters: integer, filters of the bottleneck layer in a block.\n",
    "        blocks: integer, blocks in the stacked blocks.\n",
    "        stride1: default 2, stride of the first layer in the first block.\n",
    "        name: string, stack label.\n",
    "\n",
    "    # Returns\n",
    "        Output tensor for the stacked blocks.\n",
    "    \"\"\"\n",
    "    x = block2(x, filters, conv_shortcut=True, name=name + '_block1')\n",
    "    for i in range(2, blocks):\n",
    "        x = block2(x, filters, name=name + '_block' + str(i))\n",
    "    x = block2(x, filters, stride=stride1, name=name + '_block' + str(blocks))\n",
    "    return x\n",
    "\n",
    "\n",
    "def block3(x, filters, kernel_size=3, stride=1, groups=32,\n",
    "           conv_shortcut=True, name=None,lambda_mask=None, input_size = None):\n",
    "    \"\"\"A residual block.\n",
    "    # Arguments\n",
    "        x: input tensor.\n",
    "        filters: integer, filters of the bottleneck layer.\n",
    "        kernel_size: default 3, kernel size of the bottleneck layer.\n",
    "        stride: default 1, stride of the first layer.\n",
    "        groups: default 32, group size for grouped convolution.\n",
    "        conv_shortcut: default True, use convolution shortcut if True,\n",
    "            otherwise identity shortcut.\n",
    "        name: string, block label.\n",
    "    # Returns\n",
    "        Output tensor for the residual block.\n",
    "    \"\"\"\n",
    "    bn_axis = 3 if backend.image_data_format() == 'channels_last' else 1\n",
    "\n",
    "    if conv_shortcut is True:\n",
    "        shortcut = layers.Conv2D((64 // groups) * filters, 1, strides=stride,use_bias=False, name=name + '_0_conv')(x)\n",
    "        if lambda_mask is not None:\n",
    "            shortcut_mask  = np.reshape(lambda_mask[3211264:6422528], (input_size, input_size, filters * 2))\n",
    "        else:\n",
    "            shortcut_mask = np.ones(shape=((input_size, input_size, filters * 2)))\n",
    "\n",
    "        shortcut_mask  = backend.variable(shortcut_mask)\n",
    "        shortcut_mask = Lambda(lambda z: z * shortcut_mask)(shortcut)\n",
    "        shortcut = layers.BatchNormalization(axis=bn_axis, epsilon=1.001e-5,name=name + '_0_bn')(shortcut)\n",
    "        if lambda_mask is not None:\n",
    "            shortcut_mask  = np.reshape(lambda_mask[3211264:6422528], (filters//2, filters//2, filters * 2))\n",
    "        else:\n",
    "            shortcut_mask = np.ones(shape=((filters//2, filters//2, filters * 2)))\n",
    "\n",
    "        shortcut_mask  = backend.variable(shortcut_mask)\n",
    "    else:\n",
    "        shortcut = x\n",
    "\n",
    "    x = layers.Conv2D(filters, 1, use_bias=False, name=name + '_1_conv')(x)\n",
    "    #################\n",
    "    if lambda_mask is not None:\n",
    "        x_mask  = np.reshape(lambda_mask[3211264:6422528], (input_size, input_size, filters // 2))\n",
    "    else:\n",
    "        x_mask = np.ones(shape=((input_size, input_size, filters)))\n",
    "\n",
    "    x_mask  = backend.variable(x_mask)\n",
    "    x = Lambda(lambda z: z * x_mask)(x)\n",
    "    ####################\n",
    "    x = layers.BatchNormalization(axis=bn_axis, epsilon=1.001e-5, name=name + '_1_bn')(x)\n",
    "    x = layers.Activation('relu', name=name + '_1_relu')(x)\n",
    "    #################\n",
    "    if lambda_mask is not None:\n",
    "        x_mask  = np.reshape(lambda_mask[3211264:6422528], (input_size, input_size, filters // 2))\n",
    "    else:\n",
    "        x_mask = np.ones(shape=((input_size, input_size, filters)))\n",
    "\n",
    "    x_mask  = backend.variable(x_mask)\n",
    "    x = Lambda(lambda z: z * x_mask)(x)\n",
    "    ####################\n",
    "    c = filters // groups\n",
    "    x = layers.ZeroPadding2D(padding=((1, 1), (1, 1)), name=name + '_2_pad')(x)\n",
    "    x = layers.DepthwiseConv2D(kernel_size, strides=stride, depth_multiplier=c,use_bias=False, name=name + '_2_conv')(x)\n",
    "    #################\n",
    "    if lambda_mask is not None:\n",
    "        x_mask  = np.reshape(lambda_mask[3211264:6422528], (input_size, input_size, filters // 2))\n",
    "    else:\n",
    "        x_mask = np.ones(shape=((input_size, input_size, filters // 2)))\n",
    "\n",
    "    x_mask  = backend.variable(x_mask)\n",
    "    x = Lambda(lambda z: z * x_mask)(x)\n",
    "    ####################\n",
    "    kernel = np.zeros((1, 1, filters * c, filters), dtype=np.float32)\n",
    "    for i in range(filters):\n",
    "        start = (i // c) * c * c + i % c\n",
    "        end = start + c * c\n",
    "        kernel[:, :, start:end:c, i] = 1.\n",
    "    x = layers.Conv2D(filters, 1, use_bias=False, trainable=False,kernel_initializer={'class_name': 'Constant','config': {'value': kernel}},name=name + '_2_gconv')(x)\n",
    "    x = layers.BatchNormalization(axis=bn_axis, epsilon=1.001e-5,name=name + '_2_bn')(x)\n",
    "    x = layers.Activation('relu', name=name + '_2_relu')(x)\n",
    "\n",
    "    x = layers.Conv2D((64 // groups) * filters, 1, use_bias=False, name=name + '_3_conv')(x)\n",
    "    x = layers.BatchNormalization(axis=bn_axis, epsilon=1.001e-5, name=name + '_3_bn')(x)\n",
    "\n",
    "    x = layers.Add(name=name + '_add')([shortcut, x])\n",
    "    x = layers.Activation('relu', name=name + '_out')(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "def stack3(x, filters, blocks, stride1=2, groups=32, name=None, lambda_mask=None, input_size = None):\n",
    "    \"\"\"A set of stacked residual blocks.\n",
    "\n",
    "    # Arguments\n",
    "        x: input tensor.\n",
    "        filters: integer, filters of the bottleneck layer in a block.\n",
    "        blocks: integer, blocks in the stacked blocks.\n",
    "        stride1: default 2, stride of the first layer in the first block.\n",
    "        groups: default 32, group size for grouped convolution.\n",
    "        name: string, stack label.\n",
    "\n",
    "    # Returns\n",
    "        Output tensor for the stacked blocks.\n",
    "    \"\"\"\n",
    "    x = block3(x, filters, stride=stride1, groups=groups, name=name + '_block1', lambda_mask=lambda_mask, input_size = input_size)\n",
    "    for i in range(2, blocks + 1):\n",
    "        x = block3(x, filters, groups=groups, conv_shortcut=False,\n",
    "                   name=name + '_block' + str(i), lambda_mask=lambda_mask, input_size = input_size)\n",
    "    return x\n",
    "\n",
    "\n",
    "def ResNet(stack_fn,\n",
    "           preact,\n",
    "           use_bias,\n",
    "           model_name='resnet',\n",
    "           include_top=True,\n",
    "           weights='imagenet',\n",
    "           input_tensor=None,\n",
    "           input_shape=None,\n",
    "           pooling=None,\n",
    "           classes=1000,\n",
    "           lambda_mask = None,\n",
    "           **kwargs):\n",
    "    \"\"\"Instantiates the ResNet, ResNetV2, and ResNeXt architecture.\n",
    "\n",
    "    Optionally loads weights pre-trained on ImageNet.\n",
    "    Note that the data format convention used by the model is\n",
    "    the one specified in your Keras config at `~/.keras/keras.json`.\n",
    "\n",
    "    # Arguments\n",
    "        stack_fn: a function that returns output tensor for the\n",
    "            stacked residual blocks.\n",
    "        preact: whether to use pre-activation or not\n",
    "            (True for ResNetV2, False for ResNet and ResNeXt).\n",
    "        use_bias: whether to use biases for convolutional layers or not\n",
    "            (True for ResNet and ResNetV2, False for ResNeXt).\n",
    "        model_name: string, model name.\n",
    "        include_top: whether to include the fully-connected\n",
    "            layer at the top of the network.\n",
    "        weights: one of `None` (random initialization),\n",
    "              'imagenet' (pre-training on ImageNet),\n",
    "              or the path to the weights file to be loaded.\n",
    "        input_tensor: optional Keras tensor\n",
    "            (i.e. output of `layers.Input()`)\n",
    "            to use as image input for the model.\n",
    "        input_shape: optional shape tuple, only to be specified\n",
    "            if `include_top` is False (otherwise the input shape\n",
    "            has to be `(224, 224, 3)` (with `channels_last` data format)\n",
    "            or `(3, 224, 224)` (with `channels_first` data format).\n",
    "            It should have exactly 3 inputs channels.\n",
    "        pooling: optional pooling mode for feature extraction\n",
    "            when `include_top` is `False`.\n",
    "            - `None` means that the output of the model will be\n",
    "                the 4D tensor output of the\n",
    "                last convolutional layer.\n",
    "            - `avg` means that global average pooling\n",
    "                will be applied to the output of the\n",
    "                last convolutional layer, and thus\n",
    "                the output of the model will be a 2D tensor.\n",
    "            - `max` means that global max pooling will\n",
    "                be applied.\n",
    "        classes: optional number of classes to classify images\n",
    "            into, only to be specified if `include_top` is True, and\n",
    "            if no `weights` argument is specified.\n",
    "\n",
    "    # Returns\n",
    "        A Keras model instance.\n",
    "\n",
    "    # Raises\n",
    "        ValueError: in case of invalid argument for `weights`,\n",
    "            or invalid input shape.\n",
    "    \"\"\"\n",
    "    global backend, layers, models, keras_utils, debug\n",
    "    debug =False\n",
    "    backend= keras.backend\n",
    "    layers = keras.layers\n",
    "    models = keras.models\n",
    "    keras_utils = keras.utils\n",
    "\n",
    "\n",
    "    if not (weights in {'imagenet', None} or os.path.exists(weights)):\n",
    "        raise ValueError('The `weights` argument should be either '\n",
    "                         '`None` (random initialization), `imagenet` '\n",
    "                         '(pre-training on ImageNet), '\n",
    "                         'or the path to the weights file to be loaded.')\n",
    "\n",
    "    if weights == 'imagenet' and include_top and classes != 1000:\n",
    "        raise ValueError('If using `weights` as `\"imagenet\"` with `include_top`'\n",
    "                         ' as true, `classes` should be 1000')\n",
    "\n",
    "    # Determine proper input shape\n",
    "    input_shape = utils._obtain_input_shape(input_shape,\n",
    "                                      default_size=224,\n",
    "                                      min_size=32,\n",
    "                                      data_format=backend.image_data_format(),\n",
    "                                      require_flatten=include_top,\n",
    "                                      weights=weights)\n",
    "\n",
    "    if input_tensor is None:\n",
    "        img_input = layers.Input(shape=input_shape)\n",
    "    else:\n",
    "        if not backend.is_keras_tensor(input_tensor):\n",
    "            img_input = layers.Input(tensor=input_tensor, shape=input_shape)\n",
    "        else:\n",
    "            img_input = input_tensor\n",
    "\n",
    "    bn_axis = 3 if backend.image_data_format() == 'channels_last' else 1\n",
    "    \n",
    "    global start_index, end_index\n",
    "    start_index = 0\n",
    "    end_index = 112*112*64\n",
    "    x = layers.ZeroPadding2D(padding=((3, 3), (3, 3)), name='conv1_pad')(img_input)\n",
    "    x = layers.Conv2D(64, 7, strides=2, use_bias=use_bias, name='conv1_conv')(x)\n",
    "    \n",
    "    #################\n",
    "    if lambda_mask is not None:\n",
    "        if debug:\n",
    "            print('conv1_conv',start_index,end_index)\n",
    "        x_mask  = np.reshape(lambda_mask[start_index:end_index], (112, 112, 64))\n",
    "    else:\n",
    "        x_mask = np.ones(shape=((112, 112, 64)))\n",
    "\n",
    "    x_mask  = backend.variable(x_mask)\n",
    "    x = Lambda(lambda z: z * x_mask)(x)\n",
    "    ####################\n",
    "    if preact is False:\n",
    "        x = layers.BatchNormalization(axis=bn_axis, epsilon=1.001e-5, name='conv1_bn')(x)\n",
    "        x = layers.Activation('relu', name='conv1_relu')(x)\n",
    "        #################\n",
    "        if lambda_mask is not None:\n",
    "            start_index = end_index\n",
    "            end_index = start_index + 112*112*64 \n",
    "            if debug:\n",
    "                print('conv1_bn',start_index,end_index)\n",
    "            x_mask  = np.reshape(lambda_mask[start_index:end_index], (112, 112, 64))\n",
    "        else:\n",
    "            x_mask = np.ones(shape=((112, 112, 64)))\n",
    "\n",
    "        x_mask  = backend.variable(x_mask)\n",
    "        x = Lambda(lambda z: z * x_mask)(x)\n",
    "        ####################\n",
    "\n",
    "    x = layers.ZeroPadding2D(padding=((1, 1), (1, 1)), name='pool1_pad')(x)\n",
    "    x = layers.MaxPooling2D(3, strides=2, name='pool1_pool')(x)\n",
    "\n",
    "    x = stack_fn(x, lambda_mask=lambda_mask)\n",
    "\n",
    "    if preact is True:\n",
    "        x = layers.BatchNormalization(axis=bn_axis, epsilon=1.001e-5,name='post_bn')(x)\n",
    "        x = layers.Activation('relu', name='post_relu')(x)\n",
    "        \n",
    "\n",
    "    if include_top:\n",
    "        x = layers.GlobalAveragePooling2D(name='avg_pool')(x)\n",
    "        x = layers.Dense(classes, activation='softmax', name='probs')(x)\n",
    "    else:\n",
    "        if pooling == 'avg':\n",
    "            x = layers.GlobalAveragePooling2D(name='avg_pool')(x)\n",
    "        elif pooling == 'max':\n",
    "            x = layers.GlobalMaxPooling2D(name='max_pool')(x)\n",
    "\n",
    "    # Ensure that the model takes into account\n",
    "    # any potential predecessors of `input_tensor`.\n",
    "    if input_tensor is not None:\n",
    "        inputs = keras_utils.get_source_inputs(input_tensor)\n",
    "    else:\n",
    "        inputs = img_input\n",
    "\n",
    "    # Create model.\n",
    "    model = models.Model(inputs, x, name=model_name)\n",
    "    #for layer in model.layers:\n",
    "    #    print(layer.name,layer.output_shape)\n",
    "\n",
    "    # Load weights.\n",
    "    if (weights == 'imagenet') and (model_name in WEIGHTS_HASHES):\n",
    "        if include_top:\n",
    "            file_name = model_name + '_weights_tf_dim_ordering_tf_kernels.h5'\n",
    "            file_hash = WEIGHTS_HASHES[model_name][0]\n",
    "        else:\n",
    "            file_name = model_name + '_weights_tf_dim_ordering_tf_kernels_notop.h5'\n",
    "            file_hash = WEIGHTS_HASHES[model_name][1]\n",
    "        weights_path = keras_utils.get_file(file_name,BASE_WEIGHTS_PATH + file_name,cache_subdir='models',file_hash=file_hash)\n",
    "        by_name = True\n",
    "        #by_name = True if 'resnext' in model_name else False\n",
    "        model.load_weights(weights_path, by_name=by_name)\n",
    "    elif weights is not None:\n",
    "        model.load_weights(weights)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def ResNet50(include_top=True,weights='imagenet',input_tensor=None,input_shape=None,pooling=None,classes=1000,**kwargs):\n",
    "    def stack_fn(x, lambda_mask = None):\n",
    "        x = stack1(x, 64, 3, stride1=1, name='conv2')\n",
    "        x = stack1(x, 128, 4, name='conv3')\n",
    "        x = stack1(x, 256, 6, name='conv4')\n",
    "        x = stack1(x, 512, 3, name='conv5')\n",
    "        return x\n",
    "    return ResNet(stack_fn, False, True, 'resnet50',include_top, weights, input_tensor, input_shape,pooling, classes, **kwargs)\n",
    "\n",
    "\n",
    "def ResNet101(include_top=True, weights='imagenet',input_tensor=None,input_shape=None,pooling=None,classes=1000,lambda_mask = None,**kwargs):\n",
    "    def stack_fn(x, lambda_mask = None):\n",
    "        x = stack1(x, 64, 3, stride1=1, name='conv2',  lambda_mask=lambda_mask, input_size=56)\n",
    "        x = stack1(x, 128, 4, name='conv3',  lambda_mask=lambda_mask, input_size=28)\n",
    "        x = stack1(x, 256, 23, name='conv4',  lambda_mask=lambda_mask, input_size=14)\n",
    "        x = stack1(x, 512, 3, name='conv5',  lambda_mask=lambda_mask, input_size=7)\n",
    "        return x\n",
    "    return ResNet(stack_fn, False, True, 'resnet101', include_top, weights,input_tensor, input_shape,pooling, classes, lambda_mask,**kwargs)\n",
    "\n",
    "\n",
    "def ResNet152(include_top=True,weights='imagenet', input_tensor=None, input_shape=None,pooling=None,classes=1000, **kwargs):\n",
    "    def stack_fn(x, lambda_mask = None):\n",
    "        x = stack1(x, 64, 3, stride1=1, name='conv2')\n",
    "        x = stack1(x, 128, 8, name='conv3')\n",
    "        x = stack1(x, 256, 36, name='conv4')\n",
    "        x = stack1(x, 512, 3, name='conv5')\n",
    "        return x\n",
    "    return ResNet(stack_fn, False, True, 'resnet152',include_top, weights, input_tensor, input_shape, pooling, classes,**kwargs)\n",
    "\n",
    "\n",
    "def ResNet50V2(include_top=True, weights='imagenet',input_tensor=None, input_shape=None,pooling=None, classes=1000, **kwargs):\n",
    "    def stack_fn(x, lambda_mask = None):\n",
    "        x = stack2(x, 64, 3, name='conv2')\n",
    "        x = stack2(x, 128, 4, name='conv3')\n",
    "        x = stack2(x, 256, 6, name='conv4')\n",
    "        x = stack2(x, 512, 3, stride1=1, name='conv5')\n",
    "        return x\n",
    "    return ResNet(stack_fn, True, True, 'resnet50v2',include_top, weights, input_tensor, input_shape,pooling, classes, **kwargs)\n",
    "\n",
    "\n",
    "def ResNet101V2(include_top=True, weights='imagenet',input_tensor=None,input_shape=None, pooling=None,classes=1000, **kwargs):\n",
    "    def stack_fn(x, lambda_mask = None):\n",
    "        x = stack2(x, 64, 3, name='conv2')\n",
    "        x = stack2(x, 128, 4, name='conv3')\n",
    "        x = stack2(x, 256, 23, name='conv4')\n",
    "        x = stack2(x, 512, 3, stride1=1, name='conv5')\n",
    "        return x\n",
    "    return ResNet(stack_fn, True, True, 'resnet101v2', include_top, weights,input_tensor, input_shape, pooling, classes, **kwargs)\n",
    "\n",
    "\n",
    "def ResNet152V2(include_top=True,weights='imagenet',input_tensor=None, input_shape=None,pooling=None,classes=1000,**kwargs):\n",
    "    def stack_fn(x, lambda_mask = None):\n",
    "        x = stack2(x, 64, 3, name='conv2')\n",
    "        x = stack2(x, 128, 8, name='conv3')\n",
    "        x = stack2(x, 256, 36, name='conv4')\n",
    "        x = stack2(x, 512, 3, stride1=1, name='conv5')\n",
    "        return x\n",
    "    return ResNet(stack_fn, True, True, 'resnet152v2', include_top, weights, input_tensor, input_shape,pooling, classes, **kwargs)\n",
    "\n",
    "\n",
    "def ResNeXt50(include_top=True,weights='imagenet',input_tensor=None,input_shape=None,pooling=None,classes=1000,**kwargs):\n",
    "    def stack_fn(x, lambda_mask = None):\n",
    "        x = stack3(x, 128, 3, stride1=1, name='conv2')\n",
    "        x = stack3(x, 256, 4, name='conv3')\n",
    "        x = stack3(x, 512, 6, name='conv4')\n",
    "        x = stack3(x, 1024, 3, name='conv5')\n",
    "        return x\n",
    "    return ResNet(stack_fn, False, False, 'resnext50',include_top, weights,input_tensor, input_shape,pooling, classes, **kwargs)\n",
    "\n",
    "\n",
    "def ResNeXt101(include_top=True,weights='imagenet',input_tensor=None,input_shape=None,pooling=None,classes=1000,lambda_mask = None,**kwargs):\n",
    "    def stack_fn(x, lambda_mask = None):\n",
    "        x = stack3(x, 128, 3, stride1=1, name='conv2',  lambda_mask=lambda_mask, input_size=56)\n",
    "        x = stack3(x, 256, 4, name='conv3',  lambda_mask=lambda_mask, input_size=28)\n",
    "        x = stack3(x, 512, 23, name='conv4',  lambda_mask=lambda_mask, input_size=14)\n",
    "        x = stack3(x, 1024, 3, name='conv5',  lambda_mask=lambda_mask, input_size=7)\n",
    "        return x\n",
    "    return ResNet(stack_fn, False, False, 'resnext101',include_top, weights,input_tensor, input_shape,pooling, classes,lambda_mask,**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/abhijit/anaconda3/envs/lesion/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/abhijit/anaconda3/envs/lesion/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:1834: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/abhijit/anaconda3/envs/lesion/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 224, 224, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1_pad (ZeroPadding2D)       (None, 230, 230, 3)  0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1_conv (Conv2D)             (None, 112, 112, 64) 9472        conv1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 112, 112, 64) 0           conv1_conv[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1_bn (BatchNormalization)   (None, 112, 112, 64) 256         lambda_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1_relu (Activation)         (None, 112, 112, 64) 0           conv1_bn[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_2 (Lambda)               (None, 112, 112, 64) 0           conv1_relu[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "pool1_pad (ZeroPadding2D)       (None, 114, 114, 64) 0           lambda_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "pool1_pool (MaxPooling2D)       (None, 56, 56, 64)   0           pool1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_conv (Conv2D)    (None, 56, 56, 64)   4160        pool1_pool[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_5 (Lambda)               (None, 56, 56, 64)   0           conv2_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_bn (BatchNormali (None, 56, 56, 64)   256         lambda_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_relu (Activation (None, 56, 56, 64)   0           conv2_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "lambda_6 (Lambda)               (None, 56, 56, 64)   0           conv2_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_conv (Conv2D)    (None, 56, 56, 64)   36928       lambda_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_7 (Lambda)               (None, 56, 56, 64)   0           conv2_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_bn (BatchNormali (None, 56, 56, 64)   256         lambda_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_relu (Activation (None, 56, 56, 64)   0           conv2_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "lambda_8 (Lambda)               (None, 56, 56, 64)   0           conv2_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_3_conv (Conv2D)    (None, 56, 56, 256)  16640       lambda_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_9 (Lambda)               (None, 56, 56, 256)  0           conv2_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_0_conv (Conv2D)    (None, 56, 56, 256)  16640       pool1_pool[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_3_bn (BatchNormali (None, 56, 56, 256)  1024        lambda_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_0_bn (BatchNormali (None, 56, 56, 256)  1024        conv2_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "lambda_10 (Lambda)              (None, 56, 56, 256)  0           conv2_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_add (Add)          (None, 56, 56, 256)  0           conv2_block1_0_bn[0][0]          \n",
      "                                                                 lambda_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_out (Activation)   (None, 56, 56, 256)  0           conv2_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_conv (Conv2D)    (None, 56, 56, 64)   16448       conv2_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "lambda_11 (Lambda)              (None, 56, 56, 64)   0           conv2_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_bn (BatchNormali (None, 56, 56, 64)   256         lambda_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_relu (Activation (None, 56, 56, 64)   0           conv2_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "lambda_12 (Lambda)              (None, 56, 56, 64)   0           conv2_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_conv (Conv2D)    (None, 56, 56, 64)   36928       lambda_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_13 (Lambda)              (None, 56, 56, 64)   0           conv2_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_bn (BatchNormali (None, 56, 56, 64)   256         lambda_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_relu (Activation (None, 56, 56, 64)   0           conv2_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "lambda_14 (Lambda)              (None, 56, 56, 64)   0           conv2_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_3_conv (Conv2D)    (None, 56, 56, 256)  16640       lambda_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_15 (Lambda)              (None, 56, 56, 256)  0           conv2_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_3_bn (BatchNormali (None, 56, 56, 256)  1024        lambda_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_16 (Lambda)              (None, 56, 56, 256)  0           conv2_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_add (Add)          (None, 56, 56, 256)  0           conv2_block1_out[0][0]           \n",
      "                                                                 lambda_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_out (Activation)   (None, 56, 56, 256)  0           conv2_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_conv (Conv2D)    (None, 56, 56, 64)   16448       conv2_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "lambda_17 (Lambda)              (None, 56, 56, 64)   0           conv2_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_bn (BatchNormali (None, 56, 56, 64)   256         lambda_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_relu (Activation (None, 56, 56, 64)   0           conv2_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "lambda_18 (Lambda)              (None, 56, 56, 64)   0           conv2_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_conv (Conv2D)    (None, 56, 56, 64)   36928       lambda_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_19 (Lambda)              (None, 56, 56, 64)   0           conv2_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_bn (BatchNormali (None, 56, 56, 64)   256         lambda_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_relu (Activation (None, 56, 56, 64)   0           conv2_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "lambda_20 (Lambda)              (None, 56, 56, 64)   0           conv2_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_3_conv (Conv2D)    (None, 56, 56, 256)  16640       lambda_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_21 (Lambda)              (None, 56, 56, 256)  0           conv2_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_3_bn (BatchNormali (None, 56, 56, 256)  1024        lambda_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_22 (Lambda)              (None, 56, 56, 256)  0           conv2_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_add (Add)          (None, 56, 56, 256)  0           conv2_block2_out[0][0]           \n",
      "                                                                 lambda_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_out (Activation)   (None, 56, 56, 256)  0           conv2_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_conv (Conv2D)    (None, 28, 28, 128)  32896       conv2_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "lambda_25 (Lambda)              (None, 28, 28, 128)  0           conv3_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_bn (BatchNormali (None, 28, 28, 128)  512         lambda_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_relu (Activation (None, 28, 28, 128)  0           conv3_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "lambda_26 (Lambda)              (None, 28, 28, 128)  0           conv3_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_conv (Conv2D)    (None, 28, 28, 128)  147584      lambda_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_27 (Lambda)              (None, 28, 28, 128)  0           conv3_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_bn (BatchNormali (None, 28, 28, 128)  512         lambda_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_relu (Activation (None, 28, 28, 128)  0           conv3_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "lambda_28 (Lambda)              (None, 28, 28, 128)  0           conv3_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_3_conv (Conv2D)    (None, 28, 28, 512)  66048       lambda_28[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_29 (Lambda)              (None, 28, 28, 512)  0           conv3_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_0_conv (Conv2D)    (None, 28, 28, 512)  131584      conv2_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_3_bn (BatchNormali (None, 28, 28, 512)  2048        lambda_29[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_0_bn (BatchNormali (None, 28, 28, 512)  2048        conv3_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "lambda_30 (Lambda)              (None, 28, 28, 512)  0           conv3_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_add (Add)          (None, 28, 28, 512)  0           conv3_block1_0_bn[0][0]          \n",
      "                                                                 lambda_30[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_out (Activation)   (None, 28, 28, 512)  0           conv3_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_conv (Conv2D)    (None, 28, 28, 128)  65664       conv3_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "lambda_31 (Lambda)              (None, 28, 28, 128)  0           conv3_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_bn (BatchNormali (None, 28, 28, 128)  512         lambda_31[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_relu (Activation (None, 28, 28, 128)  0           conv3_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "lambda_32 (Lambda)              (None, 28, 28, 128)  0           conv3_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_conv (Conv2D)    (None, 28, 28, 128)  147584      lambda_32[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_33 (Lambda)              (None, 28, 28, 128)  0           conv3_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_bn (BatchNormali (None, 28, 28, 128)  512         lambda_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_relu (Activation (None, 28, 28, 128)  0           conv3_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "lambda_34 (Lambda)              (None, 28, 28, 128)  0           conv3_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_3_conv (Conv2D)    (None, 28, 28, 512)  66048       lambda_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_35 (Lambda)              (None, 28, 28, 512)  0           conv3_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_3_bn (BatchNormali (None, 28, 28, 512)  2048        lambda_35[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_36 (Lambda)              (None, 28, 28, 512)  0           conv3_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_add (Add)          (None, 28, 28, 512)  0           conv3_block1_out[0][0]           \n",
      "                                                                 lambda_36[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_out (Activation)   (None, 28, 28, 512)  0           conv3_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_conv (Conv2D)    (None, 28, 28, 128)  65664       conv3_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "lambda_37 (Lambda)              (None, 28, 28, 128)  0           conv3_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_bn (BatchNormali (None, 28, 28, 128)  512         lambda_37[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_relu (Activation (None, 28, 28, 128)  0           conv3_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "lambda_38 (Lambda)              (None, 28, 28, 128)  0           conv3_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_2_conv (Conv2D)    (None, 28, 28, 128)  147584      lambda_38[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_39 (Lambda)              (None, 28, 28, 128)  0           conv3_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_2_bn (BatchNormali (None, 28, 28, 128)  512         lambda_39[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_2_relu (Activation (None, 28, 28, 128)  0           conv3_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "lambda_40 (Lambda)              (None, 28, 28, 128)  0           conv3_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_3_conv (Conv2D)    (None, 28, 28, 512)  66048       lambda_40[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_41 (Lambda)              (None, 28, 28, 512)  0           conv3_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_3_bn (BatchNormali (None, 28, 28, 512)  2048        lambda_41[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_42 (Lambda)              (None, 28, 28, 512)  0           conv3_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_add (Add)          (None, 28, 28, 512)  0           conv3_block2_out[0][0]           \n",
      "                                                                 lambda_42[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_out (Activation)   (None, 28, 28, 512)  0           conv3_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_conv (Conv2D)    (None, 28, 28, 128)  65664       conv3_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "lambda_43 (Lambda)              (None, 28, 28, 128)  0           conv3_block4_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_bn (BatchNormali (None, 28, 28, 128)  512         lambda_43[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_relu (Activation (None, 28, 28, 128)  0           conv3_block4_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "lambda_44 (Lambda)              (None, 28, 28, 128)  0           conv3_block4_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_2_conv (Conv2D)    (None, 28, 28, 128)  147584      lambda_44[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_45 (Lambda)              (None, 28, 28, 128)  0           conv3_block4_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_2_bn (BatchNormali (None, 28, 28, 128)  512         lambda_45[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_2_relu (Activation (None, 28, 28, 128)  0           conv3_block4_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "lambda_46 (Lambda)              (None, 28, 28, 128)  0           conv3_block4_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_3_conv (Conv2D)    (None, 28, 28, 512)  66048       lambda_46[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_47 (Lambda)              (None, 28, 28, 512)  0           conv3_block4_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_3_bn (BatchNormali (None, 28, 28, 512)  2048        lambda_47[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_48 (Lambda)              (None, 28, 28, 512)  0           conv3_block4_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_add (Add)          (None, 28, 28, 512)  0           conv3_block3_out[0][0]           \n",
      "                                                                 lambda_48[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_out (Activation)   (None, 28, 28, 512)  0           conv3_block4_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_conv (Conv2D)    (None, 14, 14, 256)  131328      conv3_block4_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "lambda_51 (Lambda)              (None, 14, 14, 256)  0           conv4_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_bn (BatchNormali (None, 14, 14, 256)  1024        lambda_51[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_relu (Activation (None, 14, 14, 256)  0           conv4_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "lambda_52 (Lambda)              (None, 14, 14, 256)  0           conv4_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_conv (Conv2D)    (None, 14, 14, 256)  590080      lambda_52[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_53 (Lambda)              (None, 14, 14, 256)  0           conv4_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_bn (BatchNormali (None, 14, 14, 256)  1024        lambda_53[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_relu (Activation (None, 14, 14, 256)  0           conv4_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "lambda_54 (Lambda)              (None, 14, 14, 256)  0           conv4_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      lambda_54[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_55 (Lambda)              (None, 14, 14, 1024) 0           conv4_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_0_conv (Conv2D)    (None, 14, 14, 1024) 525312      conv3_block4_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_3_bn (BatchNormali (None, 14, 14, 1024) 4096        lambda_55[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_0_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "lambda_56 (Lambda)              (None, 14, 14, 1024) 0           conv4_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_add (Add)          (None, 14, 14, 1024) 0           conv4_block1_0_bn[0][0]          \n",
      "                                                                 lambda_56[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_out (Activation)   (None, 14, 14, 1024) 0           conv4_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_conv (Conv2D)    (None, 14, 14, 256)  262400      conv4_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "lambda_57 (Lambda)              (None, 14, 14, 256)  0           conv4_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_bn (BatchNormali (None, 14, 14, 256)  1024        lambda_57[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_relu (Activation (None, 14, 14, 256)  0           conv4_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "lambda_58 (Lambda)              (None, 14, 14, 256)  0           conv4_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_conv (Conv2D)    (None, 14, 14, 256)  590080      lambda_58[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_59 (Lambda)              (None, 14, 14, 256)  0           conv4_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_bn (BatchNormali (None, 14, 14, 256)  1024        lambda_59[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_relu (Activation (None, 14, 14, 256)  0           conv4_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "lambda_60 (Lambda)              (None, 14, 14, 256)  0           conv4_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      lambda_60[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_61 (Lambda)              (None, 14, 14, 1024) 0           conv4_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_3_bn (BatchNormali (None, 14, 14, 1024) 4096        lambda_61[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_62 (Lambda)              (None, 14, 14, 1024) 0           conv4_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_add (Add)          (None, 14, 14, 1024) 0           conv4_block1_out[0][0]           \n",
      "                                                                 lambda_62[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_out (Activation)   (None, 14, 14, 1024) 0           conv4_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_conv (Conv2D)    (None, 14, 14, 256)  262400      conv4_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "lambda_63 (Lambda)              (None, 14, 14, 256)  0           conv4_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_bn (BatchNormali (None, 14, 14, 256)  1024        lambda_63[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_relu (Activation (None, 14, 14, 256)  0           conv4_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "lambda_64 (Lambda)              (None, 14, 14, 256)  0           conv4_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_2_conv (Conv2D)    (None, 14, 14, 256)  590080      lambda_64[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_65 (Lambda)              (None, 14, 14, 256)  0           conv4_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_2_bn (BatchNormali (None, 14, 14, 256)  1024        lambda_65[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_2_relu (Activation (None, 14, 14, 256)  0           conv4_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "lambda_66 (Lambda)              (None, 14, 14, 256)  0           conv4_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      lambda_66[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_67 (Lambda)              (None, 14, 14, 1024) 0           conv4_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_3_bn (BatchNormali (None, 14, 14, 1024) 4096        lambda_67[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_68 (Lambda)              (None, 14, 14, 1024) 0           conv4_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_add (Add)          (None, 14, 14, 1024) 0           conv4_block2_out[0][0]           \n",
      "                                                                 lambda_68[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_out (Activation)   (None, 14, 14, 1024) 0           conv4_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_conv (Conv2D)    (None, 14, 14, 256)  262400      conv4_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "lambda_69 (Lambda)              (None, 14, 14, 256)  0           conv4_block4_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_bn (BatchNormali (None, 14, 14, 256)  1024        lambda_69[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_relu (Activation (None, 14, 14, 256)  0           conv4_block4_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "lambda_70 (Lambda)              (None, 14, 14, 256)  0           conv4_block4_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_2_conv (Conv2D)    (None, 14, 14, 256)  590080      lambda_70[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_71 (Lambda)              (None, 14, 14, 256)  0           conv4_block4_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_2_bn (BatchNormali (None, 14, 14, 256)  1024        lambda_71[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_2_relu (Activation (None, 14, 14, 256)  0           conv4_block4_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "lambda_72 (Lambda)              (None, 14, 14, 256)  0           conv4_block4_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      lambda_72[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_73 (Lambda)              (None, 14, 14, 1024) 0           conv4_block4_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_3_bn (BatchNormali (None, 14, 14, 1024) 4096        lambda_73[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_74 (Lambda)              (None, 14, 14, 1024) 0           conv4_block4_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_add (Add)          (None, 14, 14, 1024) 0           conv4_block3_out[0][0]           \n",
      "                                                                 lambda_74[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_out (Activation)   (None, 14, 14, 1024) 0           conv4_block4_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_conv (Conv2D)    (None, 14, 14, 256)  262400      conv4_block4_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "lambda_75 (Lambda)              (None, 14, 14, 256)  0           conv4_block5_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_bn (BatchNormali (None, 14, 14, 256)  1024        lambda_75[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_relu (Activation (None, 14, 14, 256)  0           conv4_block5_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "lambda_76 (Lambda)              (None, 14, 14, 256)  0           conv4_block5_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_2_conv (Conv2D)    (None, 14, 14, 256)  590080      lambda_76[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_77 (Lambda)              (None, 14, 14, 256)  0           conv4_block5_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_2_bn (BatchNormali (None, 14, 14, 256)  1024        lambda_77[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_2_relu (Activation (None, 14, 14, 256)  0           conv4_block5_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "lambda_78 (Lambda)              (None, 14, 14, 256)  0           conv4_block5_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      lambda_78[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_79 (Lambda)              (None, 14, 14, 1024) 0           conv4_block5_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_3_bn (BatchNormali (None, 14, 14, 1024) 4096        lambda_79[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_80 (Lambda)              (None, 14, 14, 1024) 0           conv4_block5_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_add (Add)          (None, 14, 14, 1024) 0           conv4_block4_out[0][0]           \n",
      "                                                                 lambda_80[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_out (Activation)   (None, 14, 14, 1024) 0           conv4_block5_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_conv (Conv2D)    (None, 14, 14, 256)  262400      conv4_block5_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "lambda_81 (Lambda)              (None, 14, 14, 256)  0           conv4_block6_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_bn (BatchNormali (None, 14, 14, 256)  1024        lambda_81[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_relu (Activation (None, 14, 14, 256)  0           conv4_block6_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "lambda_82 (Lambda)              (None, 14, 14, 256)  0           conv4_block6_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_2_conv (Conv2D)    (None, 14, 14, 256)  590080      lambda_82[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_83 (Lambda)              (None, 14, 14, 256)  0           conv4_block6_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_2_bn (BatchNormali (None, 14, 14, 256)  1024        lambda_83[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_2_relu (Activation (None, 14, 14, 256)  0           conv4_block6_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "lambda_84 (Lambda)              (None, 14, 14, 256)  0           conv4_block6_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      lambda_84[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_85 (Lambda)              (None, 14, 14, 1024) 0           conv4_block6_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_3_bn (BatchNormali (None, 14, 14, 1024) 4096        lambda_85[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_86 (Lambda)              (None, 14, 14, 1024) 0           conv4_block6_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_add (Add)          (None, 14, 14, 1024) 0           conv4_block5_out[0][0]           \n",
      "                                                                 lambda_86[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_out (Activation)   (None, 14, 14, 1024) 0           conv4_block6_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block7_1_conv (Conv2D)    (None, 14, 14, 256)  262400      conv4_block6_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "lambda_87 (Lambda)              (None, 14, 14, 256)  0           conv4_block7_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block7_1_bn (BatchNormali (None, 14, 14, 256)  1024        lambda_87[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block7_1_relu (Activation (None, 14, 14, 256)  0           conv4_block7_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "lambda_88 (Lambda)              (None, 14, 14, 256)  0           conv4_block7_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block7_2_conv (Conv2D)    (None, 14, 14, 256)  590080      lambda_88[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_89 (Lambda)              (None, 14, 14, 256)  0           conv4_block7_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block7_2_bn (BatchNormali (None, 14, 14, 256)  1024        lambda_89[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block7_2_relu (Activation (None, 14, 14, 256)  0           conv4_block7_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "lambda_90 (Lambda)              (None, 14, 14, 256)  0           conv4_block7_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block7_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      lambda_90[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_91 (Lambda)              (None, 14, 14, 1024) 0           conv4_block7_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block7_3_bn (BatchNormali (None, 14, 14, 1024) 4096        lambda_91[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_92 (Lambda)              (None, 14, 14, 1024) 0           conv4_block7_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block7_add (Add)          (None, 14, 14, 1024) 0           conv4_block6_out[0][0]           \n",
      "                                                                 lambda_92[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block7_out (Activation)   (None, 14, 14, 1024) 0           conv4_block7_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block8_1_conv (Conv2D)    (None, 14, 14, 256)  262400      conv4_block7_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "lambda_93 (Lambda)              (None, 14, 14, 256)  0           conv4_block8_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block8_1_bn (BatchNormali (None, 14, 14, 256)  1024        lambda_93[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block8_1_relu (Activation (None, 14, 14, 256)  0           conv4_block8_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "lambda_94 (Lambda)              (None, 14, 14, 256)  0           conv4_block8_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block8_2_conv (Conv2D)    (None, 14, 14, 256)  590080      lambda_94[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_95 (Lambda)              (None, 14, 14, 256)  0           conv4_block8_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block8_2_bn (BatchNormali (None, 14, 14, 256)  1024        lambda_95[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block8_2_relu (Activation (None, 14, 14, 256)  0           conv4_block8_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "lambda_96 (Lambda)              (None, 14, 14, 256)  0           conv4_block8_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block8_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      lambda_96[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_97 (Lambda)              (None, 14, 14, 1024) 0           conv4_block8_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block8_3_bn (BatchNormali (None, 14, 14, 1024) 4096        lambda_97[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_98 (Lambda)              (None, 14, 14, 1024) 0           conv4_block8_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block8_add (Add)          (None, 14, 14, 1024) 0           conv4_block7_out[0][0]           \n",
      "                                                                 lambda_98[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block8_out (Activation)   (None, 14, 14, 1024) 0           conv4_block8_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block9_1_conv (Conv2D)    (None, 14, 14, 256)  262400      conv4_block8_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "lambda_99 (Lambda)              (None, 14, 14, 256)  0           conv4_block9_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block9_1_bn (BatchNormali (None, 14, 14, 256)  1024        lambda_99[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block9_1_relu (Activation (None, 14, 14, 256)  0           conv4_block9_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "lambda_100 (Lambda)             (None, 14, 14, 256)  0           conv4_block9_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block9_2_conv (Conv2D)    (None, 14, 14, 256)  590080      lambda_100[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_101 (Lambda)             (None, 14, 14, 256)  0           conv4_block9_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block9_2_bn (BatchNormali (None, 14, 14, 256)  1024        lambda_101[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block9_2_relu (Activation (None, 14, 14, 256)  0           conv4_block9_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "lambda_102 (Lambda)             (None, 14, 14, 256)  0           conv4_block9_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block9_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      lambda_102[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_103 (Lambda)             (None, 14, 14, 1024) 0           conv4_block9_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block9_3_bn (BatchNormali (None, 14, 14, 1024) 4096        lambda_103[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_104 (Lambda)             (None, 14, 14, 1024) 0           conv4_block9_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block9_add (Add)          (None, 14, 14, 1024) 0           conv4_block8_out[0][0]           \n",
      "                                                                 lambda_104[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block9_out (Activation)   (None, 14, 14, 1024) 0           conv4_block9_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block10_1_conv (Conv2D)   (None, 14, 14, 256)  262400      conv4_block9_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "lambda_105 (Lambda)             (None, 14, 14, 256)  0           conv4_block10_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block10_1_bn (BatchNormal (None, 14, 14, 256)  1024        lambda_105[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block10_1_relu (Activatio (None, 14, 14, 256)  0           conv4_block10_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "lambda_106 (Lambda)             (None, 14, 14, 256)  0           conv4_block10_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block10_2_conv (Conv2D)   (None, 14, 14, 256)  590080      lambda_106[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_107 (Lambda)             (None, 14, 14, 256)  0           conv4_block10_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block10_2_bn (BatchNormal (None, 14, 14, 256)  1024        lambda_107[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block10_2_relu (Activatio (None, 14, 14, 256)  0           conv4_block10_2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "lambda_108 (Lambda)             (None, 14, 14, 256)  0           conv4_block10_2_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block10_3_conv (Conv2D)   (None, 14, 14, 1024) 263168      lambda_108[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_109 (Lambda)             (None, 14, 14, 1024) 0           conv4_block10_3_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block10_3_bn (BatchNormal (None, 14, 14, 1024) 4096        lambda_109[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_110 (Lambda)             (None, 14, 14, 1024) 0           conv4_block10_3_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block10_add (Add)         (None, 14, 14, 1024) 0           conv4_block9_out[0][0]           \n",
      "                                                                 lambda_110[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block10_out (Activation)  (None, 14, 14, 1024) 0           conv4_block10_add[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block11_1_conv (Conv2D)   (None, 14, 14, 256)  262400      conv4_block10_out[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "lambda_111 (Lambda)             (None, 14, 14, 256)  0           conv4_block11_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block11_1_bn (BatchNormal (None, 14, 14, 256)  1024        lambda_111[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block11_1_relu (Activatio (None, 14, 14, 256)  0           conv4_block11_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "lambda_112 (Lambda)             (None, 14, 14, 256)  0           conv4_block11_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block11_2_conv (Conv2D)   (None, 14, 14, 256)  590080      lambda_112[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_113 (Lambda)             (None, 14, 14, 256)  0           conv4_block11_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block11_2_bn (BatchNormal (None, 14, 14, 256)  1024        lambda_113[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block11_2_relu (Activatio (None, 14, 14, 256)  0           conv4_block11_2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "lambda_114 (Lambda)             (None, 14, 14, 256)  0           conv4_block11_2_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block11_3_conv (Conv2D)   (None, 14, 14, 1024) 263168      lambda_114[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_115 (Lambda)             (None, 14, 14, 1024) 0           conv4_block11_3_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block11_3_bn (BatchNormal (None, 14, 14, 1024) 4096        lambda_115[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_116 (Lambda)             (None, 14, 14, 1024) 0           conv4_block11_3_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block11_add (Add)         (None, 14, 14, 1024) 0           conv4_block10_out[0][0]          \n",
      "                                                                 lambda_116[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block11_out (Activation)  (None, 14, 14, 1024) 0           conv4_block11_add[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block12_1_conv (Conv2D)   (None, 14, 14, 256)  262400      conv4_block11_out[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "lambda_117 (Lambda)             (None, 14, 14, 256)  0           conv4_block12_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block12_1_bn (BatchNormal (None, 14, 14, 256)  1024        lambda_117[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block12_1_relu (Activatio (None, 14, 14, 256)  0           conv4_block12_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "lambda_118 (Lambda)             (None, 14, 14, 256)  0           conv4_block12_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block12_2_conv (Conv2D)   (None, 14, 14, 256)  590080      lambda_118[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_119 (Lambda)             (None, 14, 14, 256)  0           conv4_block12_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block12_2_bn (BatchNormal (None, 14, 14, 256)  1024        lambda_119[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block12_2_relu (Activatio (None, 14, 14, 256)  0           conv4_block12_2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "lambda_120 (Lambda)             (None, 14, 14, 256)  0           conv4_block12_2_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block12_3_conv (Conv2D)   (None, 14, 14, 1024) 263168      lambda_120[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_121 (Lambda)             (None, 14, 14, 1024) 0           conv4_block12_3_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block12_3_bn (BatchNormal (None, 14, 14, 1024) 4096        lambda_121[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_122 (Lambda)             (None, 14, 14, 1024) 0           conv4_block12_3_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block12_add (Add)         (None, 14, 14, 1024) 0           conv4_block11_out[0][0]          \n",
      "                                                                 lambda_122[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block12_out (Activation)  (None, 14, 14, 1024) 0           conv4_block12_add[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block13_1_conv (Conv2D)   (None, 14, 14, 256)  262400      conv4_block12_out[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "lambda_123 (Lambda)             (None, 14, 14, 256)  0           conv4_block13_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block13_1_bn (BatchNormal (None, 14, 14, 256)  1024        lambda_123[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block13_1_relu (Activatio (None, 14, 14, 256)  0           conv4_block13_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "lambda_124 (Lambda)             (None, 14, 14, 256)  0           conv4_block13_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block13_2_conv (Conv2D)   (None, 14, 14, 256)  590080      lambda_124[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_125 (Lambda)             (None, 14, 14, 256)  0           conv4_block13_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block13_2_bn (BatchNormal (None, 14, 14, 256)  1024        lambda_125[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block13_2_relu (Activatio (None, 14, 14, 256)  0           conv4_block13_2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "lambda_126 (Lambda)             (None, 14, 14, 256)  0           conv4_block13_2_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block13_3_conv (Conv2D)   (None, 14, 14, 1024) 263168      lambda_126[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_127 (Lambda)             (None, 14, 14, 1024) 0           conv4_block13_3_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block13_3_bn (BatchNormal (None, 14, 14, 1024) 4096        lambda_127[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_128 (Lambda)             (None, 14, 14, 1024) 0           conv4_block13_3_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block13_add (Add)         (None, 14, 14, 1024) 0           conv4_block12_out[0][0]          \n",
      "                                                                 lambda_128[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block13_out (Activation)  (None, 14, 14, 1024) 0           conv4_block13_add[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block14_1_conv (Conv2D)   (None, 14, 14, 256)  262400      conv4_block13_out[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "lambda_129 (Lambda)             (None, 14, 14, 256)  0           conv4_block14_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block14_1_bn (BatchNormal (None, 14, 14, 256)  1024        lambda_129[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block14_1_relu (Activatio (None, 14, 14, 256)  0           conv4_block14_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "lambda_130 (Lambda)             (None, 14, 14, 256)  0           conv4_block14_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block14_2_conv (Conv2D)   (None, 14, 14, 256)  590080      lambda_130[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_131 (Lambda)             (None, 14, 14, 256)  0           conv4_block14_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block14_2_bn (BatchNormal (None, 14, 14, 256)  1024        lambda_131[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block14_2_relu (Activatio (None, 14, 14, 256)  0           conv4_block14_2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "lambda_132 (Lambda)             (None, 14, 14, 256)  0           conv4_block14_2_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block14_3_conv (Conv2D)   (None, 14, 14, 1024) 263168      lambda_132[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_133 (Lambda)             (None, 14, 14, 1024) 0           conv4_block14_3_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block14_3_bn (BatchNormal (None, 14, 14, 1024) 4096        lambda_133[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_134 (Lambda)             (None, 14, 14, 1024) 0           conv4_block14_3_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block14_add (Add)         (None, 14, 14, 1024) 0           conv4_block13_out[0][0]          \n",
      "                                                                 lambda_134[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block14_out (Activation)  (None, 14, 14, 1024) 0           conv4_block14_add[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block15_1_conv (Conv2D)   (None, 14, 14, 256)  262400      conv4_block14_out[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "lambda_135 (Lambda)             (None, 14, 14, 256)  0           conv4_block15_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block15_1_bn (BatchNormal (None, 14, 14, 256)  1024        lambda_135[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block15_1_relu (Activatio (None, 14, 14, 256)  0           conv4_block15_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "lambda_136 (Lambda)             (None, 14, 14, 256)  0           conv4_block15_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block15_2_conv (Conv2D)   (None, 14, 14, 256)  590080      lambda_136[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_137 (Lambda)             (None, 14, 14, 256)  0           conv4_block15_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block15_2_bn (BatchNormal (None, 14, 14, 256)  1024        lambda_137[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block15_2_relu (Activatio (None, 14, 14, 256)  0           conv4_block15_2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "lambda_138 (Lambda)             (None, 14, 14, 256)  0           conv4_block15_2_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block15_3_conv (Conv2D)   (None, 14, 14, 1024) 263168      lambda_138[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_139 (Lambda)             (None, 14, 14, 1024) 0           conv4_block15_3_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block15_3_bn (BatchNormal (None, 14, 14, 1024) 4096        lambda_139[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_140 (Lambda)             (None, 14, 14, 1024) 0           conv4_block15_3_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block15_add (Add)         (None, 14, 14, 1024) 0           conv4_block14_out[0][0]          \n",
      "                                                                 lambda_140[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block15_out (Activation)  (None, 14, 14, 1024) 0           conv4_block15_add[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block16_1_conv (Conv2D)   (None, 14, 14, 256)  262400      conv4_block15_out[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "lambda_141 (Lambda)             (None, 14, 14, 256)  0           conv4_block16_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block16_1_bn (BatchNormal (None, 14, 14, 256)  1024        lambda_141[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block16_1_relu (Activatio (None, 14, 14, 256)  0           conv4_block16_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "lambda_142 (Lambda)             (None, 14, 14, 256)  0           conv4_block16_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block16_2_conv (Conv2D)   (None, 14, 14, 256)  590080      lambda_142[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_143 (Lambda)             (None, 14, 14, 256)  0           conv4_block16_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block16_2_bn (BatchNormal (None, 14, 14, 256)  1024        lambda_143[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block16_2_relu (Activatio (None, 14, 14, 256)  0           conv4_block16_2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "lambda_144 (Lambda)             (None, 14, 14, 256)  0           conv4_block16_2_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block16_3_conv (Conv2D)   (None, 14, 14, 1024) 263168      lambda_144[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_145 (Lambda)             (None, 14, 14, 1024) 0           conv4_block16_3_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block16_3_bn (BatchNormal (None, 14, 14, 1024) 4096        lambda_145[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_146 (Lambda)             (None, 14, 14, 1024) 0           conv4_block16_3_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block16_add (Add)         (None, 14, 14, 1024) 0           conv4_block15_out[0][0]          \n",
      "                                                                 lambda_146[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block16_out (Activation)  (None, 14, 14, 1024) 0           conv4_block16_add[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block17_1_conv (Conv2D)   (None, 14, 14, 256)  262400      conv4_block16_out[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "lambda_147 (Lambda)             (None, 14, 14, 256)  0           conv4_block17_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block17_1_bn (BatchNormal (None, 14, 14, 256)  1024        lambda_147[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block17_1_relu (Activatio (None, 14, 14, 256)  0           conv4_block17_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "lambda_148 (Lambda)             (None, 14, 14, 256)  0           conv4_block17_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block17_2_conv (Conv2D)   (None, 14, 14, 256)  590080      lambda_148[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_149 (Lambda)             (None, 14, 14, 256)  0           conv4_block17_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block17_2_bn (BatchNormal (None, 14, 14, 256)  1024        lambda_149[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block17_2_relu (Activatio (None, 14, 14, 256)  0           conv4_block17_2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "lambda_150 (Lambda)             (None, 14, 14, 256)  0           conv4_block17_2_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block17_3_conv (Conv2D)   (None, 14, 14, 1024) 263168      lambda_150[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_151 (Lambda)             (None, 14, 14, 1024) 0           conv4_block17_3_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block17_3_bn (BatchNormal (None, 14, 14, 1024) 4096        lambda_151[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_152 (Lambda)             (None, 14, 14, 1024) 0           conv4_block17_3_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block17_add (Add)         (None, 14, 14, 1024) 0           conv4_block16_out[0][0]          \n",
      "                                                                 lambda_152[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block17_out (Activation)  (None, 14, 14, 1024) 0           conv4_block17_add[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block18_1_conv (Conv2D)   (None, 14, 14, 256)  262400      conv4_block17_out[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "lambda_153 (Lambda)             (None, 14, 14, 256)  0           conv4_block18_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block18_1_bn (BatchNormal (None, 14, 14, 256)  1024        lambda_153[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block18_1_relu (Activatio (None, 14, 14, 256)  0           conv4_block18_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "lambda_154 (Lambda)             (None, 14, 14, 256)  0           conv4_block18_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block18_2_conv (Conv2D)   (None, 14, 14, 256)  590080      lambda_154[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_155 (Lambda)             (None, 14, 14, 256)  0           conv4_block18_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block18_2_bn (BatchNormal (None, 14, 14, 256)  1024        lambda_155[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block18_2_relu (Activatio (None, 14, 14, 256)  0           conv4_block18_2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "lambda_156 (Lambda)             (None, 14, 14, 256)  0           conv4_block18_2_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block18_3_conv (Conv2D)   (None, 14, 14, 1024) 263168      lambda_156[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_157 (Lambda)             (None, 14, 14, 1024) 0           conv4_block18_3_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block18_3_bn (BatchNormal (None, 14, 14, 1024) 4096        lambda_157[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_158 (Lambda)             (None, 14, 14, 1024) 0           conv4_block18_3_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block18_add (Add)         (None, 14, 14, 1024) 0           conv4_block17_out[0][0]          \n",
      "                                                                 lambda_158[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block18_out (Activation)  (None, 14, 14, 1024) 0           conv4_block18_add[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block19_1_conv (Conv2D)   (None, 14, 14, 256)  262400      conv4_block18_out[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "lambda_159 (Lambda)             (None, 14, 14, 256)  0           conv4_block19_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block19_1_bn (BatchNormal (None, 14, 14, 256)  1024        lambda_159[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block19_1_relu (Activatio (None, 14, 14, 256)  0           conv4_block19_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "lambda_160 (Lambda)             (None, 14, 14, 256)  0           conv4_block19_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block19_2_conv (Conv2D)   (None, 14, 14, 256)  590080      lambda_160[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_161 (Lambda)             (None, 14, 14, 256)  0           conv4_block19_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block19_2_bn (BatchNormal (None, 14, 14, 256)  1024        lambda_161[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block19_2_relu (Activatio (None, 14, 14, 256)  0           conv4_block19_2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "lambda_162 (Lambda)             (None, 14, 14, 256)  0           conv4_block19_2_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block19_3_conv (Conv2D)   (None, 14, 14, 1024) 263168      lambda_162[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_163 (Lambda)             (None, 14, 14, 1024) 0           conv4_block19_3_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block19_3_bn (BatchNormal (None, 14, 14, 1024) 4096        lambda_163[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_164 (Lambda)             (None, 14, 14, 1024) 0           conv4_block19_3_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block19_add (Add)         (None, 14, 14, 1024) 0           conv4_block18_out[0][0]          \n",
      "                                                                 lambda_164[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block19_out (Activation)  (None, 14, 14, 1024) 0           conv4_block19_add[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block20_1_conv (Conv2D)   (None, 14, 14, 256)  262400      conv4_block19_out[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "lambda_165 (Lambda)             (None, 14, 14, 256)  0           conv4_block20_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block20_1_bn (BatchNormal (None, 14, 14, 256)  1024        lambda_165[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block20_1_relu (Activatio (None, 14, 14, 256)  0           conv4_block20_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "lambda_166 (Lambda)             (None, 14, 14, 256)  0           conv4_block20_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block20_2_conv (Conv2D)   (None, 14, 14, 256)  590080      lambda_166[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_167 (Lambda)             (None, 14, 14, 256)  0           conv4_block20_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block20_2_bn (BatchNormal (None, 14, 14, 256)  1024        lambda_167[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block20_2_relu (Activatio (None, 14, 14, 256)  0           conv4_block20_2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "lambda_168 (Lambda)             (None, 14, 14, 256)  0           conv4_block20_2_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block20_3_conv (Conv2D)   (None, 14, 14, 1024) 263168      lambda_168[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_169 (Lambda)             (None, 14, 14, 1024) 0           conv4_block20_3_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block20_3_bn (BatchNormal (None, 14, 14, 1024) 4096        lambda_169[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_170 (Lambda)             (None, 14, 14, 1024) 0           conv4_block20_3_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block20_add (Add)         (None, 14, 14, 1024) 0           conv4_block19_out[0][0]          \n",
      "                                                                 lambda_170[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block20_out (Activation)  (None, 14, 14, 1024) 0           conv4_block20_add[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block21_1_conv (Conv2D)   (None, 14, 14, 256)  262400      conv4_block20_out[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "lambda_171 (Lambda)             (None, 14, 14, 256)  0           conv4_block21_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block21_1_bn (BatchNormal (None, 14, 14, 256)  1024        lambda_171[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block21_1_relu (Activatio (None, 14, 14, 256)  0           conv4_block21_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "lambda_172 (Lambda)             (None, 14, 14, 256)  0           conv4_block21_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block21_2_conv (Conv2D)   (None, 14, 14, 256)  590080      lambda_172[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_173 (Lambda)             (None, 14, 14, 256)  0           conv4_block21_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block21_2_bn (BatchNormal (None, 14, 14, 256)  1024        lambda_173[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block21_2_relu (Activatio (None, 14, 14, 256)  0           conv4_block21_2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "lambda_174 (Lambda)             (None, 14, 14, 256)  0           conv4_block21_2_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block21_3_conv (Conv2D)   (None, 14, 14, 1024) 263168      lambda_174[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_175 (Lambda)             (None, 14, 14, 1024) 0           conv4_block21_3_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block21_3_bn (BatchNormal (None, 14, 14, 1024) 4096        lambda_175[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_176 (Lambda)             (None, 14, 14, 1024) 0           conv4_block21_3_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block21_add (Add)         (None, 14, 14, 1024) 0           conv4_block20_out[0][0]          \n",
      "                                                                 lambda_176[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block21_out (Activation)  (None, 14, 14, 1024) 0           conv4_block21_add[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block22_1_conv (Conv2D)   (None, 14, 14, 256)  262400      conv4_block21_out[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "lambda_177 (Lambda)             (None, 14, 14, 256)  0           conv4_block22_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block22_1_bn (BatchNormal (None, 14, 14, 256)  1024        lambda_177[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block22_1_relu (Activatio (None, 14, 14, 256)  0           conv4_block22_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "lambda_178 (Lambda)             (None, 14, 14, 256)  0           conv4_block22_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block22_2_conv (Conv2D)   (None, 14, 14, 256)  590080      lambda_178[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_179 (Lambda)             (None, 14, 14, 256)  0           conv4_block22_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block22_2_bn (BatchNormal (None, 14, 14, 256)  1024        lambda_179[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block22_2_relu (Activatio (None, 14, 14, 256)  0           conv4_block22_2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "lambda_180 (Lambda)             (None, 14, 14, 256)  0           conv4_block22_2_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block22_3_conv (Conv2D)   (None, 14, 14, 1024) 263168      lambda_180[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_181 (Lambda)             (None, 14, 14, 1024) 0           conv4_block22_3_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block22_3_bn (BatchNormal (None, 14, 14, 1024) 4096        lambda_181[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_182 (Lambda)             (None, 14, 14, 1024) 0           conv4_block22_3_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block22_add (Add)         (None, 14, 14, 1024) 0           conv4_block21_out[0][0]          \n",
      "                                                                 lambda_182[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block22_out (Activation)  (None, 14, 14, 1024) 0           conv4_block22_add[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block23_1_conv (Conv2D)   (None, 14, 14, 256)  262400      conv4_block22_out[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "lambda_183 (Lambda)             (None, 14, 14, 256)  0           conv4_block23_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block23_1_bn (BatchNormal (None, 14, 14, 256)  1024        lambda_183[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block23_1_relu (Activatio (None, 14, 14, 256)  0           conv4_block23_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "lambda_184 (Lambda)             (None, 14, 14, 256)  0           conv4_block23_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block23_2_conv (Conv2D)   (None, 14, 14, 256)  590080      lambda_184[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_185 (Lambda)             (None, 14, 14, 256)  0           conv4_block23_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block23_2_bn (BatchNormal (None, 14, 14, 256)  1024        lambda_185[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block23_2_relu (Activatio (None, 14, 14, 256)  0           conv4_block23_2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "lambda_186 (Lambda)             (None, 14, 14, 256)  0           conv4_block23_2_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block23_3_conv (Conv2D)   (None, 14, 14, 1024) 263168      lambda_186[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_187 (Lambda)             (None, 14, 14, 1024) 0           conv4_block23_3_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block23_3_bn (BatchNormal (None, 14, 14, 1024) 4096        lambda_187[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_188 (Lambda)             (None, 14, 14, 1024) 0           conv4_block23_3_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block23_add (Add)         (None, 14, 14, 1024) 0           conv4_block22_out[0][0]          \n",
      "                                                                 lambda_188[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block23_out (Activation)  (None, 14, 14, 1024) 0           conv4_block23_add[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_conv (Conv2D)    (None, 7, 7, 512)    524800      conv4_block23_out[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "lambda_191 (Lambda)             (None, 7, 7, 512)    0           conv5_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_bn (BatchNormali (None, 7, 7, 512)    2048        lambda_191[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_relu (Activation (None, 7, 7, 512)    0           conv5_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "lambda_192 (Lambda)             (None, 7, 7, 512)    0           conv5_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_2_conv (Conv2D)    (None, 7, 7, 512)    2359808     lambda_192[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_193 (Lambda)             (None, 7, 7, 512)    0           conv5_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_2_bn (BatchNormali (None, 7, 7, 512)    2048        lambda_193[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_2_relu (Activation (None, 7, 7, 512)    0           conv5_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "lambda_194 (Lambda)             (None, 7, 7, 512)    0           conv5_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_3_conv (Conv2D)    (None, 7, 7, 2048)   1050624     lambda_194[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_195 (Lambda)             (None, 7, 7, 2048)   0           conv5_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_0_conv (Conv2D)    (None, 7, 7, 2048)   2099200     conv4_block23_out[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_3_bn (BatchNormali (None, 7, 7, 2048)   8192        lambda_195[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_0_bn (BatchNormali (None, 7, 7, 2048)   8192        conv5_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "lambda_196 (Lambda)             (None, 7, 7, 2048)   0           conv5_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_add (Add)          (None, 7, 7, 2048)   0           conv5_block1_0_bn[0][0]          \n",
      "                                                                 lambda_196[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_out (Activation)   (None, 7, 7, 2048)   0           conv5_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_conv (Conv2D)    (None, 7, 7, 512)    1049088     conv5_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "lambda_197 (Lambda)             (None, 7, 7, 512)    0           conv5_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_bn (BatchNormali (None, 7, 7, 512)    2048        lambda_197[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_relu (Activation (None, 7, 7, 512)    0           conv5_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "lambda_198 (Lambda)             (None, 7, 7, 512)    0           conv5_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_2_conv (Conv2D)    (None, 7, 7, 512)    2359808     lambda_198[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_199 (Lambda)             (None, 7, 7, 512)    0           conv5_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_2_bn (BatchNormali (None, 7, 7, 512)    2048        lambda_199[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_2_relu (Activation (None, 7, 7, 512)    0           conv5_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "lambda_200 (Lambda)             (None, 7, 7, 512)    0           conv5_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_3_conv (Conv2D)    (None, 7, 7, 2048)   1050624     lambda_200[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_201 (Lambda)             (None, 7, 7, 2048)   0           conv5_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_3_bn (BatchNormali (None, 7, 7, 2048)   8192        lambda_201[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_202 (Lambda)             (None, 7, 7, 2048)   0           conv5_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_add (Add)          (None, 7, 7, 2048)   0           conv5_block1_out[0][0]           \n",
      "                                                                 lambda_202[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_out (Activation)   (None, 7, 7, 2048)   0           conv5_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_conv (Conv2D)    (None, 7, 7, 512)    1049088     conv5_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "lambda_203 (Lambda)             (None, 7, 7, 512)    0           conv5_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_bn (BatchNormali (None, 7, 7, 512)    2048        lambda_203[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_relu (Activation (None, 7, 7, 512)    0           conv5_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "lambda_204 (Lambda)             (None, 7, 7, 512)    0           conv5_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_2_conv (Conv2D)    (None, 7, 7, 512)    2359808     lambda_204[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_205 (Lambda)             (None, 7, 7, 512)    0           conv5_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_2_bn (BatchNormali (None, 7, 7, 512)    2048        lambda_205[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_2_relu (Activation (None, 7, 7, 512)    0           conv5_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "lambda_206 (Lambda)             (None, 7, 7, 512)    0           conv5_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_3_conv (Conv2D)    (None, 7, 7, 2048)   1050624     lambda_206[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_207 (Lambda)             (None, 7, 7, 2048)   0           conv5_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_3_bn (BatchNormali (None, 7, 7, 2048)   8192        lambda_207[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_208 (Lambda)             (None, 7, 7, 2048)   0           conv5_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_add (Add)          (None, 7, 7, 2048)   0           conv5_block2_out[0][0]           \n",
      "                                                                 lambda_208[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_out (Activation)   (None, 7, 7, 2048)   0           conv5_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "avg_pool (GlobalAveragePooling2 (None, 2048)         0           conv5_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "probs (Dense)                   (None, 1000)         2049000     avg_pool[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 44,707,176\n",
      "Trainable params: 44,601,832\n",
      "Non-trainable params: 105,344\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_name = 'ResNet101'\n",
    "model = ResNet101( include_top=True,\n",
    "    weights=\"imagenet\",\n",
    "    input_tensor=None,\n",
    "    input_shape=None,\n",
    "    pooling=None,\n",
    "    classes=1000)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "K.image_data_format()\n",
    "tf.keras.backend.clear_session()\n",
    "gc.collect()\n",
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#Test Cell 1\n",
    "\n",
    "data_path = '../../data/pkl/'\n",
    "classes = ['animate','inanimate']\n",
    "fold = 1\n",
    "\n",
    "with open(data_path+classes[0]+'_train_'+model_name+'.pkl','rb') as f:\n",
    "        X_fold = pickle.load(f)\n",
    "with open(data_path+classes[1]+'_train_'+model_name+'.pkl','rb') as f:\n",
    "        y_fold = pickle.load(f)\n",
    "    \n",
    "X = np.column_stack((X_fold,y_fold))\n",
    "X = np.float32(X)\n",
    "\n",
    "\n",
    "kmeans = MiniBatchKMeans(n_clusters=67783, #200x reduction in points\n",
    "                         max_iter=10).fit(X)\n",
    "\n",
    "pred_kmeans = kmeans.predict(X)\n",
    "\n",
    "X_new = kmeans.cluster_centers_\n",
    "\n",
    "with open('../../data/pkl/'+model_name+'kmeans_first_train_'+model_name+'.pkl', 'wb') as handle:\n",
    "    pickle.dump([X_new,pred_kmeans,kmeans], handle, protocol=pickle.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%time\n",
    "#Version 1 - Reading pkl files from step 0 and clustering it{\n",
    "data_path = '../../data/pkl/'\n",
    "classes = ['animate','inanimate']\n",
    "\n",
    "\n",
    "result= {}\n",
    "\n",
    "k = 4 #Total Number of folds\n",
    "fold = 1\n",
    "\n",
    "for i in range(k):\n",
    "    \n",
    "    print('Perfoming Fold: ', fold)\n",
    "    clf_result = {}\n",
    "    \n",
    "    if os.path.exists('../../data/pkl/kmeans_first_'+str(fold)+'_'+model_name+'.pkl'):\n",
    "        with open('../../data/pkl/kmeans_first_'+str(fold)+'_'+model_name+'.pkl',\"rb\") as f:\n",
    "            X_new,pred_kmeans,kmeans = pickle.load(f)\n",
    "    else:   \n",
    "        with open(data_path+classes[0]+'_fold_'+str(fold)+'_train_'+model_name+'.pkl','rb') as f:\n",
    "            X_fold = pickle.load(f)\n",
    "        with open(data_path+classes[1]+'_fold_'+str(fold)+'_train_'+model_name+'.pkl','rb') as f:\n",
    "            y_fold = pickle.load(f)\n",
    "\n",
    "        X = np.column_stack((X_fold,y_fold))\n",
    "        kmeans = MiniBatchKMeans(n_clusters=67783,\n",
    "                                 max_iter=10).fit(X)\n",
    "        #print kmeans.cluster_centers_\n",
    "        pred_kmeans = kmeans.predict(X)\n",
    "        X_new = kmeans.cluster_centers_\n",
    "\n",
    "        with open('../../data/pkl/kmeans_first_'+str(fold)+'_'+model_name+'.pkl', 'wb') as handle:\n",
    "            pickle.dump([X_new,pred_kmeans,kmeans], handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    #DO CLUSTERING AND GET CLUSTERS\n",
    "    \n",
    "    from sklearn.cluster import KMeans\n",
    "\n",
    "    from sklearn.mixture import GaussianMixture\n",
    "    \n",
    "    method ='GMM'\n",
    "    print(method)\n",
    "    for j in range(1,31,1):\n",
    " \n",
    "        clf_result[j] = {}\n",
    "\n",
    "        clf = KMeans(n_clusters=j)    \n",
    "        clf = GaussianMixture(n_components=j, covariance_type='full')\n",
    "        y_pred = clf.fit_predict(X_new)\n",
    "        #print clf.cluster_centers_\n",
    "\n",
    "        for label in set(y_pred):\n",
    "            print('Cluster: ',j,'Label: ', label)\n",
    "            \n",
    "            #Lesioning and measuring performance\n",
    "            pred = y_pred.copy()\n",
    "            loc = np.where(pred==label)\n",
    "            loc_temp = kmeans.predict(X_new[loc[0]])\n",
    "            loc_new =[]\n",
    "            for entry in set(loc_temp):\n",
    "                temp = np.where(pred_kmeans==entry)[0]\n",
    "                loc_new.extend(temp)\n",
    "\n",
    "            lambda_mask = np.ones(shape=((13555712,)))\n",
    "            lambda_mask[loc_new] = 0.\n",
    "\n",
    "            #plt.scatter(X[:,0],X[:,1], c=y_pred) \n",
    "            #Change Model\n",
    "            model = VGG16(include_top=True, weights='imagenet', input_tensor=None, input_shape=None, pooling=None, classes=1000,lambda_mask=lambda_mask)\n",
    "\n",
    "            flag = 0\n",
    "            dprime = 0.\n",
    "            for p in classes:\n",
    "                im_valid_test = []\n",
    "                image_list_valid = '../../data/pkl/'+p+'_image_list_valid_fold_'+str(fold)+'.txt'\n",
    "                with open(image_list_valid,'r') as f:\n",
    "                    for line in f.readlines():\n",
    "                        im_valid_test.append(line.strip('\\n'))\n",
    "                im_temp = preprocess_image_batch(im_valid_test,img_size=(256,256), crop_size=(224,224), color_mode=\"rgb\")\n",
    "                out = model.predict(im_temp,batch_size=64)\n",
    "\n",
    "                true_valid_wids = []\n",
    "                for i in im_valid_test:\n",
    "                        temp1 = i.split('/')[4]\n",
    "                        temp = temp1.split('.')[0].split('_')[2]\n",
    "                        true_valid_wids.append(truth[int(temp)][1])\n",
    "\n",
    "                predicted_valid_wids = []\n",
    "                for i in range(len(im_valid_test)):\n",
    "                    #print im_list[i], pprint_output(out[i]), true_wids[i]\n",
    "                    predicted_valid_wids.append(pprint_output(out[i]))\n",
    "\n",
    "                count, error  = top5accuracy(true_valid_wids, predicted_valid_wids)\n",
    "\n",
    "                print(str(p)+' '+str(fold)+' '+str(count)+' '+str(len(im_valid_test))+' '+str(error))\n",
    "                \n",
    "                if flag == 0:\n",
    "                    dprime = error\n",
    "                    flag = 1\n",
    "                else:\n",
    "                    dprime -= error\n",
    "                    \n",
    "            tf.keras.backend.clear_session()\n",
    "            gc.collect()\n",
    "            del model\n",
    "            clf_result[j][label] = dprime\n",
    "    \n",
    "    with open('../../data/pkl/'+str(method)+'_30_scree_fold_'+str(fold)+'_'+model_name+'.pkl', 'wb') as handle:\n",
    "        pickle.dump(clf_result, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "    result[fold] = clf_result\n",
    "    fold += 1\n",
    "#}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading the pickle files\n",
    "method ='GMM'\n",
    "\n",
    "k = 4\n",
    "result ={}\n",
    "for i in range(1,k+1,1):\n",
    "    name = '../../data/pkl/'+str(method)+'_30_scree_fold_'+str(i)+'_VGG16.pkl'   #CHANGE\n",
    "    with open(name,\"rb\") as f:\n",
    "        result[i] = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: {0: 0.0},\n",
       " 2: {0: 0.0, 1: 0.1282051282051282},\n",
       " 3: {0: 0.0, 1: 0.0, 2: 0.15384615384615385},\n",
       " 4: {0: 0.0, 1: 0.0, 2: 0.33333333333333337, 3: -0.05128205128205121},\n",
       " 5: {0: 0.0,\n",
       "  1: 0.23076923076923073,\n",
       "  2: -0.10256410256410253,\n",
       "  3: 0.10256410256410253,\n",
       "  4: 0.0},\n",
       " 6: {0: 0.0,\n",
       "  1: -0.41025641025641024,\n",
       "  2: -0.10256410256410253,\n",
       "  3: 0.0,\n",
       "  4: 0.2564102564102564,\n",
       "  5: 0.46153846153846156},\n",
       " 7: {0: 0.0,\n",
       "  1: 0.23076923076923084,\n",
       "  2: 0.3846153846153846,\n",
       "  3: 0.0,\n",
       "  4: -0.5384615384615384,\n",
       "  5: 0.0,\n",
       "  6: -0.05128205128205121},\n",
       " 8: {0: 0.5641025641025641,\n",
       "  1: 0.0,\n",
       "  2: 0.6153846153846153,\n",
       "  3: 0.15384615384615397,\n",
       "  4: -0.05128205128205121,\n",
       "  5: 0.0,\n",
       "  6: -0.46153846153846156,\n",
       "  7: -0.10256410256410253},\n",
       " 9: {0: 0.0,\n",
       "  1: -0.46153846153846156,\n",
       "  2: 0.23076923076923084,\n",
       "  3: -0.02564102564102566,\n",
       "  4: -0.05128205128205121,\n",
       "  5: 0.7435897435897436,\n",
       "  6: 0.5897435897435898,\n",
       "  7: -0.3846153846153847,\n",
       "  8: 0.0},\n",
       " 10: {0: -0.4871794871794872,\n",
       "  1: 0.0,\n",
       "  2: 0.20512820512820507,\n",
       "  3: 0.7435897435897436,\n",
       "  4: 0.6153846153846154,\n",
       "  5: -0.07692307692307687,\n",
       "  6: -0.07692307692307687,\n",
       "  7: -0.3846153846153847,\n",
       "  8: 0.0,\n",
       "  9: -0.05128205128205121},\n",
       " 11: {0: 0.02564102564102566,\n",
       "  1: 0.6153846153846154,\n",
       "  2: 0.0,\n",
       "  3: -0.07692307692307687,\n",
       "  4: -0.5128205128205128,\n",
       "  5: -0.07692307692307687,\n",
       "  6: -0.23076923076923084,\n",
       "  7: 0.02564102564102555,\n",
       "  8: 0.6666666666666666,\n",
       "  9: -0.07692307692307687,\n",
       "  10: -0.6666666666666666},\n",
       " 12: {0: 0.0,\n",
       "  1: -0.23076923076923073,\n",
       "  2: 0.6923076923076923,\n",
       "  3: -0.07692307692307687,\n",
       "  4: -0.07692307692307687,\n",
       "  5: 0.15384615384615374,\n",
       "  6: -0.3589743589743589,\n",
       "  7: 0.05128205128205121,\n",
       "  8: -0.07692307692307687,\n",
       "  9: -0.6410256410256411,\n",
       "  10: 0.0,\n",
       "  11: 0.4358974358974359},\n",
       " 13: {0: 0.05128205128205132,\n",
       "  1: 0.0,\n",
       "  2: 0.5128205128205128,\n",
       "  3: -0.02564102564102566,\n",
       "  4: -0.05128205128205121,\n",
       "  5: -0.4871794871794872,\n",
       "  6: -0.6923076923076923,\n",
       "  7: -0.10256410256410253,\n",
       "  8: 0.02564102564102566,\n",
       "  9: 0.5384615384615384,\n",
       "  10: -0.20512820512820518,\n",
       "  11: 0.23076923076923084,\n",
       "  12: 0.15384615384615385},\n",
       " 14: {0: 0.7692307692307693,\n",
       "  1: 0.1282051282051282,\n",
       "  2: 0.0,\n",
       "  3: 0.07692307692307687,\n",
       "  4: -0.05128205128205121,\n",
       "  5: 0.1794871794871794,\n",
       "  6: 0.3076923076923077,\n",
       "  7: -0.20512820512820507,\n",
       "  8: -0.3076923076923077,\n",
       "  9: -0.07692307692307687,\n",
       "  10: 0.4871794871794871,\n",
       "  11: 0.05128205128205121,\n",
       "  12: -0.1282051282051282,\n",
       "  13: -0.4871794871794871},\n",
       " 15: {0: 0.02564102564102566,\n",
       "  1: 0.15384615384615385,\n",
       "  2: 0.0,\n",
       "  3: 0.0,\n",
       "  4: -0.05128205128205121,\n",
       "  5: -0.1794871794871794,\n",
       "  6: 0.5128205128205128,\n",
       "  7: 0.05128205128205121,\n",
       "  8: 0.0,\n",
       "  9: -0.5128205128205129,\n",
       "  10: 0.0,\n",
       "  11: 0.3076923076923076,\n",
       "  12: 0.46153846153846145,\n",
       "  13: -0.10256410256410253,\n",
       "  14: -0.6666666666666667},\n",
       " 16: {0: 0.17948717948717952,\n",
       "  1: 0.05128205128205132,\n",
       "  2: 0.0,\n",
       "  3: -0.07692307692307687,\n",
       "  4: 0.05128205128205132,\n",
       "  5: -0.6410256410256411,\n",
       "  6: 0.4871794871794871,\n",
       "  7: 0.07692307692307687,\n",
       "  8: 0.4871794871794872,\n",
       "  9: -0.10256410256410253,\n",
       "  10: -0.2564102564102564,\n",
       "  11: -0.07692307692307687,\n",
       "  12: -0.1282051282051282,\n",
       "  13: 0.07692307692307687,\n",
       "  14: -0.46153846153846156,\n",
       "  15: 0.2564102564102564},\n",
       " 17: {0: -0.10256410256410253,\n",
       "  1: 0.0,\n",
       "  2: 0.33333333333333337,\n",
       "  3: 0.0,\n",
       "  4: -0.05128205128205121,\n",
       "  5: -0.23076923076923073,\n",
       "  6: 0.02564102564102555,\n",
       "  7: 0.1282051282051282,\n",
       "  8: 0.3076923076923076,\n",
       "  9: 0.0,\n",
       "  10: 0.05128205128205132,\n",
       "  11: 0.46153846153846156,\n",
       "  12: 0.05128205128205132,\n",
       "  13: -0.46153846153846156,\n",
       "  14: -0.10256410256410253,\n",
       "  15: 0.05128205128205121,\n",
       "  16: -0.41025641025641024},\n",
       " 18: {0: 0.0,\n",
       "  1: 0.0,\n",
       "  2: -0.28205128205128205,\n",
       "  3: -0.02564102564102566,\n",
       "  4: 0.15384615384615385,\n",
       "  5: -0.07692307692307687,\n",
       "  6: 0.7435897435897436,\n",
       "  7: -0.1282051282051282,\n",
       "  8: 0.33333333333333337,\n",
       "  9: -0.6153846153846154,\n",
       "  10: -0.02564102564102566,\n",
       "  11: -0.10256410256410264,\n",
       "  12: -0.15384615384615385,\n",
       "  13: 0.1794871794871794,\n",
       "  14: 0.3846153846153847,\n",
       "  15: -0.07692307692307687,\n",
       "  16: 0.0,\n",
       "  17: -0.5128205128205128},\n",
       " 19: {0: -0.435897435897436,\n",
       "  1: 0.02564102564102566,\n",
       "  2: 0.07692307692307687,\n",
       "  3: -0.07692307692307687,\n",
       "  4: 0.07692307692307698,\n",
       "  5: -0.17948717948717952,\n",
       "  6: -0.46153846153846156,\n",
       "  7: -0.10256410256410253,\n",
       "  8: -0.23076923076923073,\n",
       "  9: 0.0,\n",
       "  10: 0.3076923076923077,\n",
       "  11: 0.28205128205128205,\n",
       "  12: -0.07692307692307687,\n",
       "  13: 0.46153846153846156,\n",
       "  14: -0.07692307692307698,\n",
       "  15: 0.1282051282051282,\n",
       "  16: -0.07692307692307687,\n",
       "  17: -0.02564102564102566,\n",
       "  18: 0.02564102564102566},\n",
       " 20: {0: 0.0,\n",
       "  1: -0.20512820512820507,\n",
       "  2: 0.3076923076923077,\n",
       "  3: -0.07692307692307687,\n",
       "  4: -0.02564102564102566,\n",
       "  5: 0.07692307692307698,\n",
       "  6: 0.1282051282051282,\n",
       "  7: -0.10256410256410253,\n",
       "  8: 0.15384615384615374,\n",
       "  9: -0.02564102564102566,\n",
       "  10: 0.28205128205128205,\n",
       "  11: -0.15384615384615385,\n",
       "  12: 0.7692307692307693,\n",
       "  13: -0.05128205128205132,\n",
       "  14: -0.46153846153846156,\n",
       "  15: 0.3076923076923076,\n",
       "  16: -0.07692307692307687,\n",
       "  17: -0.5641025641025641,\n",
       "  18: -0.20512820512820507,\n",
       "  19: -0.10256410256410253},\n",
       " 21: {0: 0.20512820512820507,\n",
       "  1: -0.6923076923076923,\n",
       "  2: -0.07692307692307687,\n",
       "  3: -0.4871794871794872,\n",
       "  4: -0.15384615384615385,\n",
       "  5: -0.07692307692307687,\n",
       "  6: 0.0,\n",
       "  7: 0.28205128205128205,\n",
       "  8: -0.02564102564102555,\n",
       "  9: -0.05128205128205121,\n",
       "  10: -0.05128205128205121,\n",
       "  11: -0.07692307692307687,\n",
       "  12: -0.23076923076923073,\n",
       "  13: -0.17948717948717952,\n",
       "  14: 0.02564102564102555,\n",
       "  15: 0.7435897435897436,\n",
       "  16: 0.07692307692307687,\n",
       "  17: 0.23076923076923073,\n",
       "  18: 0.15384615384615374,\n",
       "  19: -0.02564102564102566,\n",
       "  20: -0.15384615384615385},\n",
       " 22: {0: 0.02564102564102566,\n",
       "  1: -0.5128205128205128,\n",
       "  2: -0.1282051282051282,\n",
       "  3: 0.0,\n",
       "  4: -0.07692307692307687,\n",
       "  5: 0.02564102564102566,\n",
       "  6: 0.02564102564102566,\n",
       "  7: -0.10256410256410253,\n",
       "  8: -0.6923076923076923,\n",
       "  9: -0.15384615384615385,\n",
       "  10: -0.07692307692307698,\n",
       "  11: 0.3076923076923077,\n",
       "  12: -0.07692307692307687,\n",
       "  13: -0.02564102564102555,\n",
       "  14: 0.6923076923076923,\n",
       "  15: -0.07692307692307687,\n",
       "  16: -0.23076923076923073,\n",
       "  17: 0.0,\n",
       "  18: 0.1794871794871794,\n",
       "  19: 0.0,\n",
       "  20: 0.20512820512820507,\n",
       "  21: 0.3076923076923077},\n",
       " 23: {0: -0.6153846153846154,\n",
       "  1: 0.05128205128205132,\n",
       "  2: 0.0,\n",
       "  3: -0.07692307692307687,\n",
       "  4: -0.10256410256410253,\n",
       "  5: -0.20512820512820507,\n",
       "  6: 0.15384615384615374,\n",
       "  7: -0.02564102564102566,\n",
       "  8: -0.10256410256410253,\n",
       "  9: 0.3846153846153846,\n",
       "  10: -0.07692307692307687,\n",
       "  11: 0.15384615384615385,\n",
       "  12: 0.02564102564102555,\n",
       "  13: 0.1282051282051282,\n",
       "  14: 0.05128205128205121,\n",
       "  15: -0.1794871794871794,\n",
       "  16: -0.02564102564102555,\n",
       "  17: -0.3589743589743589,\n",
       "  18: -0.07692307692307687,\n",
       "  19: 0.23076923076923073,\n",
       "  20: -0.10256410256410253,\n",
       "  21: -0.10256410256410253,\n",
       "  22: 0.02564102564102566},\n",
       " 24: {0: 0.07692307692307698,\n",
       "  1: 0.07692307692307687,\n",
       "  2: -0.07692307692307687,\n",
       "  3: -0.07692307692307687,\n",
       "  4: -0.23076923076923073,\n",
       "  5: 0.23076923076923073,\n",
       "  6: -0.05128205128205132,\n",
       "  7: -0.15384615384615385,\n",
       "  8: 0.0,\n",
       "  9: -0.07692307692307687,\n",
       "  10: -0.10256410256410253,\n",
       "  11: 0.3076923076923077,\n",
       "  12: -0.10256410256410253,\n",
       "  13: 0.05128205128205121,\n",
       "  14: -0.05128205128205121,\n",
       "  15: -0.02564102564102566,\n",
       "  16: 0.1794871794871794,\n",
       "  17: -0.5384615384615384,\n",
       "  18: 0.28205128205128205,\n",
       "  19: -0.05128205128205132,\n",
       "  20: -0.07692307692307687,\n",
       "  21: -0.05128205128205121,\n",
       "  22: 0.33333333333333337,\n",
       "  23: -0.3076923076923076},\n",
       " 25: {0: 0.1794871794871794,\n",
       "  1: -0.02564102564102555,\n",
       "  2: 0.0,\n",
       "  3: -0.07692307692307687,\n",
       "  4: 0.17948717948717952,\n",
       "  5: 0.3076923076923076,\n",
       "  6: -0.33333333333333326,\n",
       "  7: 0.0,\n",
       "  8: 0.02564102564102555,\n",
       "  9: 0.0,\n",
       "  10: -0.20512820512820507,\n",
       "  11: -0.10256410256410253,\n",
       "  12: -0.15384615384615385,\n",
       "  13: -0.02564102564102566,\n",
       "  14: -0.07692307692307687,\n",
       "  15: -0.07692307692307687,\n",
       "  16: 0.1282051282051282,\n",
       "  17: -0.07692307692307687,\n",
       "  18: 0.1282051282051282,\n",
       "  19: 0.641025641025641,\n",
       "  20: -0.10256410256410253,\n",
       "  21: 0.02564102564102555,\n",
       "  22: -0.1282051282051282,\n",
       "  23: -0.20512820512820507,\n",
       "  24: -0.4871794871794871},\n",
       " 26: {0: -0.05128205128205132,\n",
       "  1: 0.05128205128205132,\n",
       "  2: 0.0,\n",
       "  3: 0.1282051282051282,\n",
       "  4: -0.07692307692307687,\n",
       "  5: -0.02564102564102566,\n",
       "  6: 0.1282051282051282,\n",
       "  7: -0.10256410256410253,\n",
       "  8: -0.20512820512820507,\n",
       "  9: -0.07692307692307687,\n",
       "  10: -0.23076923076923073,\n",
       "  11: -0.1282051282051282,\n",
       "  12: -0.02564102564102555,\n",
       "  13: -0.4871794871794872,\n",
       "  14: -0.1282051282051282,\n",
       "  15: 0.02564102564102566,\n",
       "  16: 0.10256410256410264,\n",
       "  17: 0.15384615384615374,\n",
       "  18: -0.10256410256410253,\n",
       "  19: 0.1282051282051282,\n",
       "  20: 0.3076923076923077,\n",
       "  21: 0.0,\n",
       "  22: 0.0,\n",
       "  23: -0.20512820512820507,\n",
       "  24: -0.05128205128205121,\n",
       "  25: -0.02564102564102566},\n",
       " 27: {0: 0.07692307692307698,\n",
       "  1: -0.23076923076923073,\n",
       "  2: -0.02564102564102555,\n",
       "  3: 0.15384615384615385,\n",
       "  4: -0.07692307692307687,\n",
       "  5: 0.0,\n",
       "  6: -0.10256410256410253,\n",
       "  7: -0.07692307692307687,\n",
       "  8: 0.2564102564102564,\n",
       "  9: -0.07692307692307687,\n",
       "  10: 0.3076923076923076,\n",
       "  11: -0.07692307692307687,\n",
       "  12: -0.07692307692307687,\n",
       "  13: -0.07692307692307698,\n",
       "  14: -0.07692307692307687,\n",
       "  15: 0.07692307692307698,\n",
       "  16: 0.15384615384615385,\n",
       "  17: -0.05128205128205121,\n",
       "  18: -0.15384615384615385,\n",
       "  19: 0.05128205128205132,\n",
       "  20: -0.4871794871794872,\n",
       "  21: -0.05128205128205132,\n",
       "  22: -0.07692307692307687,\n",
       "  23: 0.15384615384615374,\n",
       "  24: -0.10256410256410253,\n",
       "  25: -0.28205128205128205,\n",
       "  26: -0.02564102564102566},\n",
       " 28: {0: -0.1794871794871794,\n",
       "  1: 0.05128205128205121,\n",
       "  2: -0.02564102564102566,\n",
       "  3: 0.15384615384615374,\n",
       "  4: -0.07692307692307687,\n",
       "  5: 0.02564102564102566,\n",
       "  6: 0.1282051282051282,\n",
       "  7: 0.10256410256410264,\n",
       "  8: -0.10256410256410253,\n",
       "  9: -0.07692307692307687,\n",
       "  10: 0.02564102564102566,\n",
       "  11: 0.0,\n",
       "  12: -0.07692307692307687,\n",
       "  13: -0.07692307692307687,\n",
       "  14: -0.07692307692307687,\n",
       "  15: -0.4358974358974359,\n",
       "  16: -0.02564102564102566,\n",
       "  17: -0.20512820512820507,\n",
       "  18: -0.10256410256410253,\n",
       "  19: -0.02564102564102555,\n",
       "  20: -0.1794871794871794,\n",
       "  21: -0.20512820512820507,\n",
       "  22: 0.0,\n",
       "  23: 0.07692307692307698,\n",
       "  24: -0.10256410256410253,\n",
       "  25: 0.0,\n",
       "  26: 0.15384615384615385,\n",
       "  27: 0.28205128205128205},\n",
       " 29: {0: -0.3589743589743589,\n",
       "  1: 0.0,\n",
       "  2: 0.15384615384615385,\n",
       "  3: -0.07692307692307687,\n",
       "  4: 0.0,\n",
       "  5: -0.07692307692307687,\n",
       "  6: -0.1794871794871794,\n",
       "  7: 0.0,\n",
       "  8: -0.1282051282051282,\n",
       "  9: -0.07692307692307687,\n",
       "  10: -0.07692307692307698,\n",
       "  11: -0.02564102564102555,\n",
       "  12: 0.1282051282051282,\n",
       "  13: -0.02564102564102555,\n",
       "  14: -0.07692307692307687,\n",
       "  15: 0.3076923076923076,\n",
       "  16: -0.05128205128205121,\n",
       "  17: -0.20512820512820507,\n",
       "  18: 0.02564102564102566,\n",
       "  19: -0.02564102564102566,\n",
       "  20: 0.15384615384615374,\n",
       "  21: -0.10256410256410253,\n",
       "  22: 0.02564102564102566,\n",
       "  23: -0.02564102564102566,\n",
       "  24: 0.07692307692307687,\n",
       "  25: 0.05128205128205121,\n",
       "  26: -0.10256410256410253,\n",
       "  27: 0.0,\n",
       "  28: -0.20512820512820507},\n",
       " 30: {0: 0.0,\n",
       "  1: 0.0,\n",
       "  2: 0.15384615384615385,\n",
       "  3: -0.07692307692307687,\n",
       "  4: -0.10256410256410253,\n",
       "  5: 0.05128205128205132,\n",
       "  6: -0.07692307692307687,\n",
       "  7: -0.10256410256410253,\n",
       "  8: -0.05128205128205132,\n",
       "  9: 0.1282051282051282,\n",
       "  10: 0.1794871794871794,\n",
       "  11: 0.5897435897435898,\n",
       "  12: -0.02564102564102555,\n",
       "  13: 0.1282051282051282,\n",
       "  14: 0.0,\n",
       "  15: -0.07692307692307687,\n",
       "  16: 0.15384615384615385,\n",
       "  17: -0.07692307692307698,\n",
       "  18: -0.05128205128205121,\n",
       "  19: -0.10256410256410253,\n",
       "  20: -0.05128205128205121,\n",
       "  21: -0.1794871794871794,\n",
       "  22: -0.1794871794871794,\n",
       "  23: 0.0,\n",
       "  24: 0.05128205128205121,\n",
       "  25: -0.1282051282051282,\n",
       "  26: -0.07692307692307687,\n",
       "  27: -0.15384615384615385,\n",
       "  28: -0.5128205128205128,\n",
       "  29: -0.2564102564102564}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../../results/GMM_results_fold_1.eps'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-8e03750a8fe7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Performance Impact(Animate vs Inanimate) '\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgca\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxaxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_major_locator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMaxNLocator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprune\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'lower'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msavefig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../../results/'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'_results_fold_'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'.eps'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'eps'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/lesion/lib/python3.7/site-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36msavefig\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    727\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msavefig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    728\u001b[0m     \u001b[0mfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgcf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 729\u001b[0;31m     \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msavefig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    730\u001b[0m     \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw_idle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m# need this if 'transparent=True' to reset colors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    731\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/lesion/lib/python3.7/site-packages/matplotlib/figure.py\u001b[0m in \u001b[0;36msavefig\u001b[0;34m(self, fname, transparent, **kwargs)\u001b[0m\n\u001b[1;32m   2178\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_visible\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframeon\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2180\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_figure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2182\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mframeon\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/lesion/lib/python3.7/site-packages/matplotlib/backend_bases.py\u001b[0m in \u001b[0;36mprint_figure\u001b[0;34m(self, filename, dpi, facecolor, edgecolor, orientation, format, bbox_inches, **kwargs)\u001b[0m\n\u001b[1;32m   2089\u001b[0m                     \u001b[0morientation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morientation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2090\u001b[0m                     \u001b[0mbbox_inches_restore\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_bbox_inches_restore\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2091\u001b[0;31m                     **kwargs)\n\u001b[0m\u001b[1;32m   2092\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2093\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mbbox_inches\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mrestore_bbox\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/lesion/lib/python3.7/site-packages/matplotlib/backends/backend_ps.py\u001b[0m in \u001b[0;36mprint_eps\u001b[0;34m(self, outfile, *args, **kwargs)\u001b[0m\n\u001b[1;32m    840\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    841\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mprint_eps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 842\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_print_ps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'eps'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    843\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    844\u001b[0m     def _print_ps(self, outfile, format, *args,\n",
      "\u001b[0;32m~/anaconda3/envs/lesion/lib/python3.7/site-packages/matplotlib/backends/backend_ps.py\u001b[0m in \u001b[0;36m_print_ps\u001b[0;34m(self, outfile, format, papertype, dpi, facecolor, edgecolor, orientation, *args, **kwargs)\u001b[0m\n\u001b[1;32m    869\u001b[0m             self._print_figure(outfile, format, dpi, facecolor, edgecolor,\n\u001b[1;32m    870\u001b[0m                                \u001b[0morientation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0misLandscape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpapertype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 871\u001b[0;31m                                **kwargs)\n\u001b[0m\u001b[1;32m    872\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    873\u001b[0m     def _print_figure(\n",
      "\u001b[0;32m~/anaconda3/envs/lesion/lib/python3.7/site-packages/matplotlib/backends/backend_ps.py\u001b[0m in \u001b[0;36m_print_figure\u001b[0;34m(self, outfile, format, dpi, facecolor, edgecolor, orientation, isLandscape, papertype, metadata, dryrun, bbox_inches_restore, **kwargs)\u001b[0m\n\u001b[1;32m   1104\u001b[0m                 \u001b[0mprint_figure_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1105\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1106\u001b[0;31m                 \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'latin-1'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfh\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1107\u001b[0m                     \u001b[0mprint_figure_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../../results/GMM_results_fold_1.eps'"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib.ticker import MaxNLocator\n",
    "f = 1\n",
    "clf_result = result[f]\n",
    "\n",
    "\n",
    "fig = plt.figure(1)\n",
    "X = range(1,31,1)\n",
    "#X = range(2,51,1)\n",
    "for cl in X:\n",
    "    i = 0\n",
    "    for item in clf_result[cl].keys():\n",
    "        plt.plot(cl,clf_result[cl][item],'ro')\n",
    "        i += 1\n",
    "        \n",
    "plt.xticks(X)\n",
    "plt.xlabel('Number of cluster(s) k')\n",
    "plt.ylabel(\"Performance Impact\")\n",
    "plt.title('Performance Impact(Animate vs Inanimate) '+ str(f))\n",
    "plt.gca().xaxis.set_major_locator(MaxNLocator(prune='lower'))\n",
    "plt.savefig('../../results/'+str(method)+'_results_fold_'+str(f)+'.eps', format='eps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = 2\n",
    "clf_result = result[f]\n",
    "\n",
    "\n",
    "fig = plt.figure(1)\n",
    "X = range(1,51,1)\n",
    "#X = range(2,51,1)\n",
    "for cl in X:\n",
    "    i = 0\n",
    "    for item in clf_result[cl].keys():\n",
    "        plt.plot(cl,clf_result[cl][item],'ro')\n",
    "        i += 1\n",
    "        \n",
    "plt.xticks(X)\n",
    "plt.xlabel('Number of cluster(s) k')\n",
    "plt.ylabel(\"Performance Impact(Animate vs Inanimate)\")\n",
    "plt.title('Scree Plot for fold '+ str(f))\n",
    "plt.gca().xaxis.set_major_locator(MaxNLocator(prune='lower'))\n",
    "#plt.savefig('../../results/scree/'+str(method)+'_results_fold_'+str(f)+'.png', format='png', dpi=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = 3\n",
    "clf_result = result[f]\n",
    "\n",
    "\n",
    "fig = plt.figure(1)\n",
    "X = range(1,51,1)\n",
    "#X = range(2,51,1)\n",
    "for cl in X:\n",
    "    i = 0\n",
    "    for item in clf_result[cl].keys():\n",
    "        plt.plot(cl,clf_result[cl][item],'ro')\n",
    "        i += 1\n",
    "        \n",
    "plt.xticks(X)\n",
    "plt.xlabel('Number of cluster(s) k')\n",
    "plt.ylabel(\"Performance Impact(Animate vs Inanimate)\")\n",
    "plt.title('Scree Plot for fold '+ str(f))\n",
    "plt.gca().xaxis.set_major_locator(MaxNLocator(prune='lower'))\n",
    "#plt.savefig('../../results/scree/'+str(method)+'_results_fold_'+str(f)+'.png', format='png', dpi=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = 4\n",
    "clf_result = result[f]\n",
    "\n",
    "\n",
    "fig = plt.figure(1)\n",
    "X = range(1,51,1)\n",
    "#X = range(2,51,1)\n",
    "for cl in X:\n",
    "    i = 0\n",
    "    for item in clf_result[cl].keys():\n",
    "        plt.plot(cl,clf_result[cl][item],'ro')\n",
    "        i += 1\n",
    "        \n",
    "plt.xticks(X)\n",
    "plt.xlabel('Number of cluster(s) k')\n",
    "plt.ylabel(\"Performance Impact(Animate vs Inanimate)\")\n",
    "plt.title('Scree Plot for fold '+ str(f))\n",
    "plt.gca().xaxis.set_major_locator(MaxNLocator(prune='lower'))\n",
    "#plt.savefig('../../results/scree/'+str(method)+'_results_fold_'+str(f)+'.png', format='png', dpi=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find MaxAd', MaxId' and its average\n",
    "plt.figure()\n",
    "noc = 4\n",
    "for i in range(1,noc+1,1):\n",
    "    X = []\n",
    "    Y = []\n",
    "    for j in range(2,51,1):\n",
    "        X.append(j)\n",
    "        temp = []\n",
    "        for key, value in result[i][j].items():\n",
    "            temp.append(value)\n",
    "        maxa = max(temp)\n",
    "        maxi = min(temp)\n",
    "        avg = float(maxa - maxi)\n",
    "        Y.append(avg)\n",
    "    #print X,Y\n",
    "    plt.plot(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Smooth average graph\n",
    "from scipy.interpolate import spline\n",
    "noc = 4\n",
    "flag = 0\n",
    "X = range(2,51,1)\n",
    "an_fold =[]\n",
    "ian_fold = []\n",
    "Y = []\n",
    "for i in range(1,noc+1,1):\n",
    "    if i == 2:\n",
    "        flag = 1\n",
    "    for j in range(2,51,1):\n",
    "        temp = []\n",
    "        for key, value in result[i][j].items():\n",
    "            temp.append(value)\n",
    "        maxa = max(temp)\n",
    "        maxi = min(temp)\n",
    "        if flag == 0:\n",
    "            an_fold.append(maxa)\n",
    "            ian_fold.append(maxi)\n",
    "        else:\n",
    "            an_fold[j-2] += maxa\n",
    "            ian_fold[j-2] = maxi\n",
    "\n",
    "for j in range(2,51,1):\n",
    "    maxa = (an_fold[j-2]) / 4.\n",
    "    maxi = (ian_fold[j-2]) /4.\n",
    "    diff = maxa - maxi\n",
    "    Y.append(diff)\n",
    "    \n",
    "x_sm = np.array(X)\n",
    "y_sm = np.array(Y)\n",
    "\n",
    "x_smooth = np.linspace(x_sm.min(), x_sm.max(), 200)\n",
    "y_smooth = spline(X, Y, x_smooth)\n",
    "\n",
    "plt.plot(x_smooth, y_smooth, 'r', linewidth=1)\n",
    "plt.plot(Y.index(max(Y))+1,max(Y),'o')\n",
    "plt.xlabel('Number of cluster(s) k')\n",
    "plt.ylabel(\"Average Performance\")\n",
    "plt.savefig('../../results/scree/'+str(method)+'_results_fold_avg.png', format='png', dpi=200)\n",
    "print(max(Y), Y.index(max(Y)) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
