{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To hide warnings export PYTHONWARNINGS=\"ignore\"\n",
    "#Imports{\n",
    "\n",
    "import os\n",
    "from os.path import dirname\n",
    "from os.path import join\n",
    "os.environ['KERAS_BACKEND'] = 'tensorflow'\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' #Cha\n",
    "\n",
    "\n",
    "import sys\n",
    "import warnings\n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    \n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from imageio import imread\n",
    "from skimage.transform import resize\n",
    "from scipy.io import loadmat\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "import PIL.Image\n",
    "#}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/abhijit/anaconda3/envs/lesion/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/abhijit/anaconda3/envs/lesion/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/abhijit/anaconda3/envs/lesion/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:186: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/abhijit/anaconda3/envs/lesion/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# confirm TensorFlow sees the GPU\n",
    "from tensorflow.python.client import device_lib\n",
    "assert 'GPU' in str(device_lib.list_local_devices())\n",
    "\n",
    "# confirm Keras sees the GPU (for TensorFlow 1.X + Keras)\n",
    "from keras import backend\n",
    "assert len(backend.tensorflow_backend._get_available_gpus()) > 0\n",
    "\n",
    "# confirm PyTorch sees the GPU\n",
    "#from torch import cuda\n",
    "#assert cuda.is_available()\n",
    "#assert cuda.device_count() > 0\n",
    "#print(cuda.get_device_name(cuda.current_device()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model architecture\n",
    "<img src=\"../../data/misc/vgg16.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code snippet needed to read activation values from each layer of the pre-trained artificial neural networks\n",
    "def get_activations(model, layer, X_batch):\n",
    "    #keras.backend.function(inputs, outputs, updates=None)\n",
    "    get_activations = keras.backend.function([model.layers[0].input, keras.backend.learning_phase()], [model.layers[layer].output,])\n",
    "    #The learning phase flag is a bool tensor (0 = test, 1 = train)\n",
    "    activations = get_activations([X_batch,0])\n",
    "    return activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to pre-process the input image to ensure uniform size and color\n",
    "def preprocess_image_batch(image_paths, img_size=None, crop_size=None, color_mode='rgb', out=None):\n",
    "    \"\"\"\n",
    "    Consistent preprocessing of images batches\n",
    "    :param image_paths: iterable: images to process\n",
    "    :param crop_size: tuple: crop images if specified\n",
    "    :param img_size: tuple: resize images if specified\n",
    "    :param color_mode: Use rgb or change to bgr mode based on type of model you want to use\n",
    "    :param out: append output to this iterable if specified\n",
    "    \"\"\"\n",
    "    img_list = []\n",
    "\n",
    "    for im_path in image_paths:\n",
    "        '''\n",
    "        img = imread(im_path,as_gray=False, pilmode=\"RGB\")\n",
    "        #print im_path\n",
    "        #print img.shape\n",
    "        if img_size:\n",
    "            img = resize(img, img_size)\n",
    "\n",
    "        img = img.astype('float32')\n",
    "        # We normalize the colors (in RGB space) with the empirical means on the training set\n",
    "        img[:, :, 0] -= 123.68\n",
    "        img[:, :, 1] -= 116.779\n",
    "        img[:, :, 2] -= 103.939\n",
    "        # We permute the colors to get them in the BGR order\n",
    "        if color_mode == 'bgr':\n",
    "            img[:, :, [0, 1, 2]] = img[:, :, [2, 1, 0]]\n",
    "        img = img.transpose((2, 0, 1))\n",
    "\n",
    "        if crop_size:\n",
    "            img = img[:, (img_size[0] - crop_size[0]) // 2:(img_size[0] + crop_size[0]) // 2\n",
    "            , (img_size[1] - crop_size[1]) // 2:(img_size[1] + crop_size[1]) // 2]\n",
    "\n",
    "        img_list.append(img)\n",
    "        '''\n",
    "        size = 224\n",
    "        ret = PIL.Image.open(im_path)\n",
    "        ret = ret.resize((size, size))\n",
    "        ret = np.asarray(ret, dtype=np.uint8).astype(np.float32)\n",
    "        #ret[:, :, 0] -= 123.68\n",
    "        #ret[:, :, 1] -= 116.779\n",
    "        #ret[:, :, 2] -= 103.939\n",
    "        if ret.ndim == 2:\n",
    "            ret.resize((size, size, 1))\n",
    "            ret = np.repeat(ret, 3, axis=-1)\n",
    "        #ret = ret.transpose((2, 0, 1))\n",
    "        #ret = np.flip(ret,0)\n",
    "        x = preprocess_input(ret)\n",
    "        #print(x.shape)\n",
    "        img_list.append(x)\n",
    "        \n",
    "\n",
    "    try:\n",
    "        img_batch = np.stack(img_list, axis=0)\n",
    "    except:\n",
    "        print(im_path)\n",
    "        raise ValueError('when img_size and crop_size are None, images'\n",
    "                ' in image_paths must have the same shapes.')\n",
    "\n",
    "    if out is not None and hasattr(out, 'append'):\n",
    "        out.append(img_batch)\n",
    "    else:\n",
    "        return img_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to predict the top 5 accuracy\n",
    "def top5accuracy(true, predicted):\n",
    "    assert len(true) == len(predicted)\n",
    "    result = []\n",
    "    flag  = 0\n",
    "    for i in range(len(true)):\n",
    "        flag  = 0\n",
    "        temp = true[i]\n",
    "        for j in predicted[i]:\n",
    "            if j == temp:\n",
    "                flag = 1\n",
    "                break\n",
    "        if flag == 1:\n",
    "            result.append(1)\n",
    "        else:\n",
    "            result.append(0)\n",
    "    counter = 0.\n",
    "    for i in result:\n",
    "        if i == 1:\n",
    "            counter += 1.\n",
    "    error = 1.0 - counter/float(len(result))\n",
    "    #print len(np.where(np.asarray(result) == 1)[0])\n",
    "    return len(np.where(np.asarray(result) == 1)[0]), error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the details of all the 1000 classes and the function to conver the synset id to words{\n",
    "meta_clsloc_file = '../../data/meta_clsloc.mat'\n",
    "synsets = loadmat(meta_clsloc_file)['synsets'][0]\n",
    "synsets_imagenet_sorted = sorted([(int(s[0]), str(s[1][0])) for s in synsets[:1000]],key=lambda v: v[1])\n",
    "corr = {}\n",
    "for j in range(1000):\n",
    "    corr[synsets_imagenet_sorted[j][0]] = j\n",
    "\n",
    "corr_inv = {}\n",
    "for j in range(1, 1001):\n",
    "    corr_inv[corr[j]] = j\n",
    "\n",
    "def id_to_words(id_):\n",
    "    return synsets[corr_inv[id_] - 1][2][0]\n",
    "\n",
    "def pprint_output(out, n_max_synsets=10):\n",
    "    wids = []\n",
    "    best_ids = out.argsort()[::-1][:10]\n",
    "    for u in best_ids:\n",
    "        wids.append(str(synsets[corr_inv[u] - 1][1][0]))\n",
    "    #print('%.2f' % round(100 * out[u], 2) + ' : ' + id_to_words(u)+' '+ str(synsets[corr_inv[u] - 1][1][0]))\n",
    "    return wids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code snippet to load the ground truth labels to measure the performance{\n",
    "truth = {}\n",
    "with open('../../data/ILSVRC2014_clsloc_validation_ground_truth.txt') as f:\n",
    "    line_num = 1\n",
    "    for line in f.readlines():\n",
    "        ind_ = int(line)\n",
    "        temp  = None\n",
    "        for i in synsets_imagenet_sorted:\n",
    "            if i[0] == ind_:\n",
    "                temp = i\n",
    "        #print ind_,temp\n",
    "        if temp != None:\n",
    "            truth[line_num] = temp\n",
    "        else:\n",
    "            print('##########', ind_)\n",
    "            pass\n",
    "        line_num += 1\n",
    "#}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'channels_last'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "backend.image_data_format()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "from keras import layers\n",
    "from keras import models\n",
    "import keras.utils as keras_utils\n",
    "from keras.layers import Lambda\n",
    "\n",
    "from  keras.applications import imagenet_utils\n",
    "from  keras.applications.imagenet_utils import decode_predictions\n",
    "from  keras_applications.imagenet_utils import _obtain_input_shape\n",
    "\n",
    "preprocess_input = imagenet_utils.preprocess_input\n",
    "\n",
    "WEIGHTS_PATH = ('https://github.com/fchollet/deep-learning-models/'\n",
    "                'releases/download/v0.1/'\n",
    "                'vgg16_weights_tf_dim_ordering_tf_kernels.h5')\n",
    "WEIGHTS_PATH_NO_TOP = ('https://github.com/fchollet/deep-learning-models/'\n",
    "                       'releases/download/v0.1/'\n",
    "                       'vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5')\n",
    "\n",
    "\n",
    "def VGG16(include_top=True,\n",
    "          weights='imagenet',\n",
    "          input_tensor=None,\n",
    "          input_shape=None,\n",
    "          pooling=None,\n",
    "          classes=1000,\n",
    "          lambda_mask=None,\n",
    "          **kwargs):\n",
    "    \n",
    "    backend= keras.backend\n",
    "    layers = keras.layers\n",
    "    models = keras.models\n",
    "    keras_utils = tf.keras.utils\n",
    "    \n",
    "    if not (weights in {'imagenet', None} or os.path.exists(weights)):\n",
    "        raise ValueError('The `weights` argument should be either '\n",
    "                         '`None` (random initialization), `imagenet` '\n",
    "                         '(pre-training on ImageNet), '\n",
    "                         'or the path to the weights file to be loaded.')\n",
    "\n",
    "    if weights == 'imagenet' and include_top and classes != 1000:\n",
    "        raise ValueError('If using `weights` as `\"imagenet\"` with `include_top`'\n",
    "                         ' as true, `classes` should be 1000')\n",
    "    # Determine proper input shape\n",
    "    input_shape = _obtain_input_shape(input_shape,\n",
    "                                      default_size=224,\n",
    "                                      min_size=32,\n",
    "                                      data_format=backend.image_data_format(),\n",
    "                                      require_flatten=include_top,\n",
    "                                      weights=weights)\n",
    "\n",
    "    if input_tensor is None:\n",
    "        img_input = layers.Input(shape=input_shape)\n",
    "    else:\n",
    "        if not backend.is_keras_tensor(input_tensor):\n",
    "            img_input = layers.Input(tensor=input_tensor, shape=input_shape)\n",
    "        else:\n",
    "            img_input = input_tensor\n",
    "    # Block 1\n",
    "    x = layers.Conv2D(64, (3, 3),\n",
    "                      activation='relu',\n",
    "                      padding='same',\n",
    "                      name='block1_conv1')(img_input)\n",
    "    \n",
    "    if lambda_mask is not None:\n",
    "        block_1_conv_1_mask  = np.reshape(lambda_mask[0:3211264], (224, 224,64))\n",
    "    else:\n",
    "        block_1_conv_1_mask = np.ones(shape=((224, 224, 64)))\n",
    "    \n",
    "    block_1_conv_1_mask  = backend.variable(block_1_conv_1_mask)\n",
    "    block_1_conv_1_lambda = Lambda(lambda x: x * block_1_conv_1_mask)(x)\n",
    "    \n",
    "    x = layers.Conv2D(64, (3, 3),\n",
    "                      activation='relu',\n",
    "                      padding='same',\n",
    "                      name='block1_conv2')(block_1_conv_1_lambda)\n",
    "    \n",
    "    if lambda_mask is not None:\n",
    "        block_1_conv_2_mask  = np.reshape(lambda_mask[3211264:6422528], (224, 224, 64))\n",
    "    else:\n",
    "        block_1_conv_2_mask = np.ones(shape=((224, 224, 64)))\n",
    "    \n",
    "    block_1_conv_2_mask  = backend.variable(block_1_conv_2_mask)\n",
    "    block_1_conv_2_lambda = Lambda(lambda x: x * block_1_conv_2_mask)(x)\n",
    "    \n",
    "    \n",
    "    x = layers.MaxPooling2D((2, 2), strides=(2, 2), name='block1_pool')(block_1_conv_2_lambda)\n",
    "\n",
    "    # Block 2\n",
    "    x = layers.Conv2D(128, (3, 3),\n",
    "                      activation='relu',\n",
    "                      padding='same',\n",
    "                      name='block2_conv1')(x)\n",
    "    \n",
    "    if lambda_mask is not None:\n",
    "        block_2_conv_1_mask  = np.reshape(lambda_mask[6422528:8028160], (112, 112, 128))\n",
    "    else:\n",
    "        block_2_conv_1_mask = np.ones(shape=((112, 112, 128)))\n",
    "    \n",
    "    block_2_conv_1_mask  = backend.variable(block_2_conv_1_mask)\n",
    "    block_2_conv_1_lambda = Lambda(lambda x: x * block_2_conv_1_mask)(x)\n",
    "    \n",
    "    x = layers.Conv2D(128, (3, 3),\n",
    "                      activation='relu',\n",
    "                      padding='same',\n",
    "                      name='block2_conv2')(block_2_conv_1_lambda)\n",
    "    \n",
    "    \n",
    "    if lambda_mask is not None:\n",
    "        block_2_conv_2_mask  = np.reshape(lambda_mask[8028160:9633792], (112, 112, 128))\n",
    "    else:\n",
    "        block_2_conv_2_mask = np.ones(shape=((112, 112, 128)))\n",
    "    \n",
    "    block_2_conv_2_mask  = backend.variable(block_2_conv_2_mask)\n",
    "    block_2_conv_2_lambda = Lambda(lambda x: x * block_2_conv_2_mask)(x)\n",
    "    \n",
    "    x = layers.MaxPooling2D((2, 2), strides=(2, 2), name='block2_pool')(block_2_conv_2_lambda)\n",
    "\n",
    "    # Block 3\n",
    "    x = layers.Conv2D(256, (3, 3),\n",
    "                      activation='relu',\n",
    "                      padding='same',\n",
    "                      name='block3_conv1')(x)\n",
    "    \n",
    "    if lambda_mask is not None:\n",
    "        block_3_conv_1_mask  = np.reshape(lambda_mask[9633792:10436608], (56, 56, 256))\n",
    "    else:\n",
    "        block_3_conv_1_mask = np.ones(shape=((56, 56, 256)))\n",
    "    \n",
    "    block_3_conv_1_mask  = backend.variable(block_3_conv_1_mask)\n",
    "    block_3_conv_1_lambda = Lambda(lambda x: x * block_3_conv_1_mask)(x)\n",
    "    \n",
    "    \n",
    "    x = layers.Conv2D(256, (3, 3),\n",
    "                      activation='relu',\n",
    "                      padding='same',\n",
    "                      name='block3_conv2')(block_3_conv_1_lambda)\n",
    "    \n",
    "    if lambda_mask is not None:\n",
    "        block_3_conv_2_mask  = np.reshape(lambda_mask[10436608:11239424], (56, 56, 256))\n",
    "    else:\n",
    "        block_3_conv_2_mask = np.ones(shape=((56, 56, 256)))\n",
    "    \n",
    "    block_3_conv_2_mask  = backend.variable(block_3_conv_2_mask)\n",
    "    block_3_conv_2_lambda = Lambda(lambda x: x * block_3_conv_2_mask)(x)\n",
    "    \n",
    "    x = layers.Conv2D(256, (3, 3),\n",
    "                      activation='relu',\n",
    "                      padding='same',\n",
    "                      name='block3_conv3')( block_3_conv_2_lambda)\n",
    "    \n",
    "    if lambda_mask is not None:\n",
    "        block_3_conv_3_mask  = np.reshape(lambda_mask[11239424:12042240], (56, 56, 256))\n",
    "    else:\n",
    "        block_3_conv_3_mask = np.ones(shape=((56, 56, 256)))\n",
    "    \n",
    "    block_3_conv_3_mask  = backend.variable(block_3_conv_3_mask)\n",
    "    block_3_conv_3_lambda = Lambda(lambda x: x * block_3_conv_3_mask)(x)\n",
    "    \n",
    "    x = layers.MaxPooling2D((2, 2), strides=(2, 2), name='block3_pool')(block_3_conv_3_lambda)\n",
    "\n",
    "    # Block 4\n",
    "    x = layers.Conv2D(512, (3, 3),\n",
    "                      activation='relu',\n",
    "                      padding='same',\n",
    "                      name='block4_conv1')(x)\n",
    "    \n",
    "    if lambda_mask is not None:\n",
    "        block_4_conv_1_mask  = np.reshape(lambda_mask[12042240:12443648], (28, 28, 512))\n",
    "    else:\n",
    "        block_4_conv_1_mask = np.ones(shape=((28, 28, 512)))\n",
    "    \n",
    "    block_4_conv_1_mask  = backend.variable(block_4_conv_1_mask)\n",
    "    block_4_conv_1_lambda = Lambda(lambda x: x * block_4_conv_1_mask)(x)\n",
    "    \n",
    "    \n",
    "    x = layers.Conv2D(512, (3, 3),\n",
    "                      activation='relu',\n",
    "                      padding='same',\n",
    "                      name='block4_conv2')(block_4_conv_1_lambda)\n",
    "    \n",
    "    if lambda_mask is not None:\n",
    "        block_4_conv_2_mask  = np.reshape(lambda_mask[12443648:12845056], (28, 28, 512))\n",
    "    else:\n",
    "        block_4_conv_2_mask = np.ones(shape=((28, 28, 512)))\n",
    "    \n",
    "    block_4_conv_2_mask  = backend.variable(block_4_conv_2_mask)\n",
    "    block_4_conv_2_lambda = Lambda(lambda x: x * block_4_conv_2_mask)(x)\n",
    "    \n",
    "    \n",
    "    x = layers.Conv2D(512, (3, 3),\n",
    "                      activation='relu',\n",
    "                      padding='same',\n",
    "                      name='block4_conv3')(block_4_conv_2_lambda)\n",
    "    \n",
    "    \n",
    "    if lambda_mask is not None:\n",
    "        block_4_conv_3_mask  = np.reshape(lambda_mask[12845056:13246464], (28, 28, 512))\n",
    "    else:\n",
    "        block_4_conv_3_mask = np.ones(shape=((28, 28, 512)))\n",
    "    \n",
    "    block_4_conv_3_mask  = backend.variable(block_4_conv_3_mask)\n",
    "    block_4_conv_3_lambda = Lambda(lambda x: x * block_4_conv_3_mask)(x)\n",
    "    \n",
    "    \n",
    "    x = layers.MaxPooling2D((2, 2), strides=(2, 2), name='block4_pool')( block_4_conv_3_lambda)\n",
    "\n",
    "    # Block 5\n",
    "    x = layers.Conv2D(512, (3, 3),\n",
    "                      activation='relu',\n",
    "                      padding='same',\n",
    "                      name='block5_conv1')(x)\n",
    "    \n",
    "    if lambda_mask is not None:\n",
    "        block_5_conv_1_mask  = np.reshape(lambda_mask[13246464:13346816], (14, 14, 512))\n",
    "    else:\n",
    "        block_5_conv_1_mask = np.ones(shape=((14, 14, 512)))\n",
    "    \n",
    "    block_5_conv_1_mask  = backend.variable(block_5_conv_1_mask)\n",
    "    block_5_conv_1_lambda = Lambda(lambda x: x * block_5_conv_1_mask)(x)\n",
    "    \n",
    "    x = layers.Conv2D(512, (3, 3),\n",
    "                      activation='relu',\n",
    "                      padding='same',\n",
    "                      name='block5_conv2')(block_5_conv_1_lambda)\n",
    "    \n",
    "    if lambda_mask is not None:\n",
    "        block_5_conv_2_mask  = np.reshape(lambda_mask[13346816:13447168], (14, 14, 512))\n",
    "    else:\n",
    "        block_5_conv_2_mask = np.ones(shape=((14, 14, 512)))\n",
    "    \n",
    "    block_5_conv_2_mask  = backend.variable(block_5_conv_2_mask)\n",
    "    block_5_conv_2_lambda = Lambda(lambda x: x * block_5_conv_2_mask)(x)\n",
    "    \n",
    "    x = layers.Conv2D(512, (3, 3),\n",
    "                      activation='relu',\n",
    "                      padding='same',\n",
    "                      name='block5_conv3')(block_5_conv_2_lambda)\n",
    "    \n",
    "    if lambda_mask is not None:\n",
    "        block_5_conv_3_mask  = np.reshape(lambda_mask[13447168:13547520], (14, 14, 512))\n",
    "    else:\n",
    "        block_5_conv_3_mask = np.ones(shape=((14, 14, 512)))\n",
    "    \n",
    "    block_5_conv_3_mask  = backend.variable(block_5_conv_3_mask)\n",
    "    block_5_conv_3_lambda = Lambda(lambda x: x * block_5_conv_3_mask)(x)\n",
    "    \n",
    "    x = layers.MaxPooling2D((2, 2), strides=(2, 2), name='block5_pool')( block_5_conv_3_lambda)\n",
    "\n",
    "    if include_top:\n",
    "        # Classification block\n",
    "        x = layers.Flatten(name='flatten')(x)\n",
    "        x = layers.Dense(4096, activation='relu', name='fc1')(x)\n",
    "        \n",
    "        if lambda_mask is not None:\n",
    "            block_fc1_mask  = np.reshape(lambda_mask[13547520:13551616], (4096,))\n",
    "        else:\n",
    "            block_fc1_mask = np.ones(shape=((4096,)))\n",
    "        block_fc1_mask  = backend.variable(block_fc1_mask)\n",
    "        block_fc1_lambda = Lambda(lambda x: x * block_fc1_mask)(x)\n",
    "    \n",
    "        x = layers.Dense(4096, activation='relu', name='fc2')(block_fc1_lambda)\n",
    "        \n",
    "        if lambda_mask is not None:\n",
    "            block_fc2_mask  = np.reshape(lambda_mask[13551616:13555712], (4096,))\n",
    "        else:\n",
    "            block_fc2_mask = np.ones(shape=((4096,)))\n",
    "        block_fc2_mask  = backend.variable(block_fc2_mask)\n",
    "        block_fc2_lambda = Lambda(lambda x: x * block_fc2_mask)(x)\n",
    "        \n",
    "        x = layers.Dense(classes, activation='softmax', name='predictions')(block_fc2_lambda)\n",
    "    else:\n",
    "        if pooling == 'avg':\n",
    "            x = layers.GlobalAveragePooling2D()(x)\n",
    "        elif pooling == 'max':\n",
    "            x = layers.GlobalMaxPooling2D()(x)\n",
    "\n",
    "    # Ensure that the model takes into account\n",
    "    # any potential predecessors of `input_tensor`.\n",
    "    if input_tensor is not None:\n",
    "        inputs = keras_utils.get_source_inputs(input_tensor)\n",
    "    else:\n",
    "        inputs = img_input\n",
    "    # Create model.\n",
    "    model = models.Model(inputs, x, name='vgg16')\n",
    "\n",
    "    # Load weights.\n",
    "    if weights == 'imagenet':\n",
    "        if include_top:\n",
    "            weights_path = keras_utils.get_file(\n",
    "                'vgg16_weights_tf_dim_ordering_tf_kernels.h5',\n",
    "                WEIGHTS_PATH,\n",
    "                cache_subdir='models',\n",
    "                file_hash='64373286793e3c8b2b4e3219cbf3544b')\n",
    "        else:\n",
    "            weights_path = keras_utils.get_file(\n",
    "                'vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5',\n",
    "                WEIGHTS_PATH_NO_TOP,\n",
    "                cache_subdir='models',\n",
    "                file_hash='6d6bbae143d832006294945121d1f1fc')\n",
    "        model.load_weights(weights_path)\n",
    "        if backend.backend() == 'theano':\n",
    "            keras_utils.convert_all_kernels_in_model(model)\n",
    "    elif weights is not None:\n",
    "        model.load_weights(weights)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "model_name = 'VGG16'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23\n",
      "{0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22}\n",
      "Cluster:  23 Label:  0\n",
      "animate 37 39 0.05128205128205132 0.9487179487179487\n",
      "inanimate 34 39 0.1282051282051282 0.8717948717948718\n",
      "Cluster:  23 Label:  1\n",
      "animate 37 39 0.05128205128205132 0.9487179487179487\n",
      "inanimate 31 39 0.20512820512820518 0.7948717948717948\n",
      "Cluster:  23 Label:  2\n",
      "animate 37 39 0.05128205128205132 0.9487179487179487\n",
      "inanimate 33 39 0.15384615384615385 0.8461538461538461\n",
      "Cluster:  23 Label:  3\n",
      "animate 30 39 0.23076923076923073 0.7692307692307693\n",
      "inanimate 27 39 0.3076923076923077 0.6923076923076923\n",
      "Cluster:  23 Label:  4\n",
      "animate 37 39 0.05128205128205132 0.9487179487179487\n",
      "inanimate 32 39 0.17948717948717952 0.8205128205128205\n",
      "Cluster:  23 Label:  5\n",
      "animate 37 39 0.05128205128205132 0.9487179487179487\n",
      "inanimate 33 39 0.15384615384615385 0.8461538461538461\n",
      "Cluster:  23 Label:  6\n",
      "animate 37 39 0.05128205128205132 0.9487179487179487\n",
      "inanimate 33 39 0.15384615384615385 0.8461538461538461\n",
      "Cluster:  23 Label:  7\n",
      "animate 37 39 0.05128205128205132 0.9487179487179487\n",
      "inanimate 33 39 0.15384615384615385 0.8461538461538461\n",
      "Cluster:  23 Label:  8\n",
      "animate 37 39 0.05128205128205132 0.9487179487179487\n",
      "inanimate 31 39 0.20512820512820518 0.7948717948717948\n",
      "Cluster:  23 Label:  9\n",
      "animate 37 39 0.05128205128205132 0.9487179487179487\n",
      "inanimate 32 39 0.17948717948717952 0.8205128205128205\n",
      "Cluster:  23 Label:  10\n",
      "animate 37 39 0.05128205128205132 0.9487179487179487\n",
      "inanimate 33 39 0.15384615384615385 0.8461538461538461\n",
      "Cluster:  23 Label:  11\n",
      "animate 37 39 0.05128205128205132 0.9487179487179487\n",
      "inanimate 33 39 0.15384615384615385 0.8461538461538461\n",
      "Cluster:  23 Label:  12\n",
      "animate 37 39 0.05128205128205132 0.9487179487179487\n",
      "inanimate 30 39 0.23076923076923073 0.7692307692307693\n",
      "Cluster:  23 Label:  13\n",
      "animate 37 39 0.05128205128205132 0.9487179487179487\n",
      "inanimate 33 39 0.15384615384615385 0.8461538461538461\n",
      "Cluster:  23 Label:  14\n",
      "animate 37 39 0.05128205128205132 0.9487179487179487\n",
      "inanimate 32 39 0.17948717948717952 0.8205128205128205\n",
      "Cluster:  23 Label:  15\n",
      "animate 0 39 1.0 0.0\n",
      "inanimate 0 39 1.0 0.0\n",
      "Cluster:  23 Label:  16\n",
      "animate 37 39 0.05128205128205132 0.9487179487179487\n",
      "inanimate 30 39 0.23076923076923073 0.7692307692307693\n",
      "Cluster:  23 Label:  17\n",
      "animate 37 39 0.05128205128205132 0.9487179487179487\n",
      "inanimate 33 39 0.15384615384615385 0.8461538461538461\n",
      "Cluster:  23 Label:  18\n",
      "animate 37 39 0.05128205128205132 0.9487179487179487\n",
      "inanimate 33 39 0.15384615384615385 0.8461538461538461\n",
      "Cluster:  23 Label:  19\n",
      "animate 37 39 0.05128205128205132 0.9487179487179487\n",
      "inanimate 29 39 0.2564102564102564 0.7435897435897436\n",
      "Cluster:  23 Label:  20\n",
      "animate 37 39 0.05128205128205132 0.9487179487179487\n",
      "inanimate 33 39 0.15384615384615385 0.8461538461538461\n",
      "Cluster:  23 Label:  21\n",
      "animate 37 39 0.05128205128205132 0.9487179487179487\n",
      "inanimate 33 39 0.15384615384615385 0.8461538461538461\n",
      "Cluster:  23 Label:  22\n",
      "animate 38 39 0.02564102564102566 0.9743589743589743\n",
      "inanimate 31 39 0.20512820512820518 0.7948717948717948\n"
     ]
    }
   ],
   "source": [
    "#%%time\n",
    "#Testing on test data{\n",
    "data_path = '../../data/pkl/'\n",
    "classes = ['animate','inanimate']\n",
    "\n",
    "result = {}\n",
    "\n",
    "with open(data_path+classes[0]+'_test_'+model_name+'.pkl','rb') as f:\n",
    "        X_fold = pickle.load(f)\n",
    "with open(data_path+classes[1]+'_test_'+model_name+'.pkl','rb') as f:\n",
    "        y_fold = pickle.load(f)\n",
    "\n",
    "X = np.column_stack((X_fold,y_fold))  \n",
    "if os.path.exists('../../data/pkl/kmeans_first_test_'+model_name+'.pkl'):\n",
    "    with open('../../data/pkl/kmeans_first_test_'+model_name+'.pkl',\"rb\") as f:\n",
    "        X_new,pred_kmeans,kmeans = pickle.load(f)\n",
    "else:   \n",
    "   \n",
    "    kmeans = MiniBatchKMeans(n_clusters=65827,\n",
    "                             random_state=0,\n",
    "                             batch_size=6,\n",
    "                             max_iter=10).fit(X)\n",
    "    #print kmeans.cluster_centers_\n",
    "    pred_kmeans = kmeans.predict(X)\n",
    "    X_new = kmeans.cluster_centers_\n",
    "    with open('../../data/pkl/kmeans_first_test_'+model_name+'.pkl', 'wb') as handle:\n",
    "        pickle.dump([X_new,pred_kmeans,kmeans], handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "#DO CLUSTERING AND GET CLUSTERS\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import SpectralClustering\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.cluster import Birch\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.mixture import GaussianMixture\n",
    "#import genieclust\n",
    "#import hdbscan\n",
    "#import smm\n",
    "\n",
    "j = 23  #Set this value from scree plot!\n",
    "method = 'GMM'\n",
    "print(j)\n",
    "#clf = hdbscan.HDBSCAN(min_cluster_size=j, gen_min_span_tree=True)\n",
    "#clf = DBSCAN(eps=5.443)\n",
    "#clf = KMeans(n_clusters=j,random_state=143)\n",
    "#clf= SpectralClustering(n_clusters=j,random_state=143)\n",
    "#clf =  AgglomerativeClustering(n_clusters=j, linkage='ward')\n",
    "#clf = Birch(branching_factor=50, n_clusters=j, threshold=0.5,compute_labels=True)\n",
    "clf = GaussianMixture(n_components=j, covariance_type='full',random_state=143)\n",
    "#clf= genieclust.genie.Genie(n_clusters=j)\n",
    "#clf= smm.SMM(n_components=j, covariance_type='full', random_state=143, tol=1e-12,min_covar=1e-6, n_iter=1000, n_init=1, params='wmcd', init_params='wmcd')\n",
    "temp = clf.fit(X_new)\n",
    "y_pred = clf.predict(X_new)\n",
    "#y_pred = clf.fit_predict(X_new)\n",
    "print(set(y_pred))\n",
    "#Z = clf.predict(X)\n",
    "\n",
    "for label in set(y_pred):\n",
    "    print('Cluster: ',j,'Label: ', label)\n",
    "\n",
    "    #Lesioning and measuring performance\n",
    "    pred = y_pred.copy()\n",
    "    loc = np.where(pred==label)\n",
    "    loc_temp = kmeans.predict(X_new[loc[0]])\n",
    "    loc_new =[]\n",
    "    for entry in set(loc_temp):\n",
    "        temp = np.where(pred_kmeans==entry)[0]\n",
    "        loc_new.extend(temp)\n",
    "\n",
    "    lambda_mask = np.ones(shape=((13555712,)))\n",
    "    lambda_mask[loc_new] = 0.\n",
    "\n",
    "    #plt.scatter(X[:,0],X[:,1], c=y_pred) \n",
    "    model = VGG16(include_top=True, weights='imagenet', input_tensor=None, input_shape=None, pooling=None, classes=1000,lambda_mask=lambda_mask)\n",
    "    \n",
    "    flag = 0\n",
    "    dprime = 0.\n",
    "    for p in classes:\n",
    "        im_valid_test = []\n",
    "        image_list_valid = '../../data/pkl/'+p+'_image_list_test.txt'\n",
    "        with open(image_list_valid,'r') as f:\n",
    "            for line in f.readlines():\n",
    "                im_valid_test.append(line.strip('\\n'))\n",
    "        im_temp = preprocess_image_batch(im_valid_test,img_size=(256,256), crop_size=(224,224), color_mode=\"rgb\")\n",
    "\n",
    "        out = model.predict(im_temp,batch_size=32)\n",
    "       \n",
    "        true_valid_wids = []\n",
    "        for i in im_valid_test:\n",
    "                temp1 = i.split('/')[4]\n",
    "                temp = temp1.split('.')[0].split('_')[2]\n",
    "                true_valid_wids.append(truth[int(temp)][1])\n",
    "\n",
    "        predicted_valid_wids = []\n",
    "        for i in range(len(im_valid_test)):\n",
    "            #print im_list[i], pprint_output(out[i]), true_wids[i]\n",
    "            predicted_valid_wids.append(pprint_output(out[i]))\n",
    "\n",
    "        count, error  = top5accuracy(true_valid_wids, predicted_valid_wids)\n",
    "\n",
    "        print(str(p)+' '+str(count)+' '+str(len(im_valid_test))+' '+str(error)+' '+str(1-error))\n",
    "\n",
    "        if flag == 0:\n",
    "            dprime = error\n",
    "            flag = 1\n",
    "        else:\n",
    "            dprime -= error\n",
    "    keras.backend.clear_session()\n",
    "    gc.collect()\n",
    "    del model\n",
    "\n",
    "    result[label] = dprime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([-0.07692307692307687,\n",
       "  -0.15384615384615385,\n",
       "  -0.10256410256410253,\n",
       "  -0.07692307692307698,\n",
       "  -0.1282051282051282,\n",
       "  -0.10256410256410253,\n",
       "  -0.10256410256410253,\n",
       "  -0.10256410256410253,\n",
       "  -0.15384615384615385,\n",
       "  -0.1282051282051282,\n",
       "  -0.10256410256410253,\n",
       "  -0.10256410256410253,\n",
       "  -0.1794871794871794,\n",
       "  -0.10256410256410253,\n",
       "  -0.1282051282051282,\n",
       "  0.0,\n",
       "  -0.1794871794871794,\n",
       "  -0.10256410256410253,\n",
       "  -0.10256410256410253,\n",
       "  -0.20512820512820507,\n",
       "  -0.10256410256410253,\n",
       "  -0.10256410256410253,\n",
       "  -0.17948717948717952],\n",
       " (13555712, 2))"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(result.values()),X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67783 67783\n"
     ]
    }
   ],
   "source": [
    "z_temp = []\n",
    "for item in y_pred:\n",
    "    z_temp.append(result[item])\n",
    "print(len(z_temp),len(X_new))\n",
    "loc_z = kmeans.predict(X_new)\n",
    "z = np.ones(shape=((13555712,)))\n",
    "for i in range(len(loc_z)):\n",
    "    temp = np.where(pred_kmeans==loc_z[i])[0]\n",
    "    z[temp] = z_temp[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13555712, 2)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = X[:,0]\n",
    "y = X[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Density Plot for Animate/Inanimate\n",
    "\n",
    "print(x.shape,y.shape,z.shape)\n",
    "fig, ax = plt.subplots()\n",
    "cs = ax.scatter(x, y, c=z, s=10,cmap='coolwarm')\n",
    "cbar = fig.colorbar(cs)\n",
    "lims = [\n",
    "    np.min([ax.get_xlim(), ax.get_ylim()]),  # min of both axes\n",
    "    np.max([ax.get_xlim(), ax.get_ylim()]),  # max of both axes\n",
    "]\n",
    "ax.plot(lims, lims, 'k-', alpha=0.75)\n",
    "plt.xlabel('Animate')\n",
    "plt.ylabel('Inanimate')\n",
    "plt.title('Performance Impact - Density plot')\n",
    "plt.savefig('../../results/'+str(method)+'_results_density.png', format='png',dpi=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15 19\n",
      "0.0 0.20512820512820507\n"
     ]
    }
   ],
   "source": [
    "print(list(result.values()).index(max(result.values())), list(result.values()).index(min(result.values())))\n",
    "ana = int(list(result.values()).index(max(result.values())))\n",
    "ina = int(list(result.values()).index(min(result.values())))\n",
    "print(result[ana], -1*(result[ina]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_info = {}\n",
    "#Comparing Layer lesions\n",
    "classes = ['animate','inanimate']\n",
    "%time\n",
    "for label in [ana,ina]:\n",
    "    layer_info[label] = {}\n",
    "    for layer in [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15]:\n",
    "        if layer == 1:\n",
    "            start = 0\n",
    "            end = 3211264\n",
    "        elif layer == 2:\n",
    "            start = 3211264\n",
    "            end = 6422528\n",
    "        elif layer == 3:\n",
    "            start = 6422528\n",
    "            end = 8028160\n",
    "        elif layer == 4:\n",
    "            start = 8028160\n",
    "            end = 9633792\n",
    "        elif layer == 5:\n",
    "            start = 9633792\n",
    "            end = 10436608\n",
    "        elif layer == 6:\n",
    "            start = 10436608\n",
    "            end = 11239424\n",
    "        elif layer == 7:\n",
    "            start = 11239424\n",
    "            end = 12042240\n",
    "        elif layer == 8:\n",
    "            start = 12042240\n",
    "            end = 12443648\n",
    "        elif layer == 9:\n",
    "            start = 12443648\n",
    "            end = 12845056\n",
    "        elif layer == 10:\n",
    "            start = 12845056\n",
    "            end = 13246464\n",
    "        elif layer == 11:\n",
    "            start = 13246464\n",
    "            end = 13346816\n",
    "        elif layer == 12:\n",
    "            start = 13346816\n",
    "            end = 13447168\n",
    "        elif layer == 13:\n",
    "            start = 13447168\n",
    "            end = 13547520\n",
    "        elif layer == 14:\n",
    "            start = 13547520\n",
    "            end = 13551616\n",
    "        elif layer == 15:\n",
    "            start = 13551616\n",
    "            end = 13555712\n",
    "\n",
    "        layer_info[label][layer] = {}\n",
    "    \n",
    "        #No lesion\n",
    "        #print('No-lesioning')\n",
    "        #print('Label:',label)\n",
    "        #print('Layer:',layer)\n",
    "\n",
    "        pred = clf.predict(X_new)\n",
    "        lambda_mask = np.ones(shape=((13555712,)))\n",
    "       \n",
    "\n",
    "        model = VGG16(include_top=True, weights='imagenet', input_tensor=None, input_shape=None, pooling=None, classes=1000,lambda_mask=lambda_mask)\n",
    "\n",
    "        flag = 0\n",
    "        dprime = 0.\n",
    "        for p in classes:\n",
    "            im_valid_test = []\n",
    "            image_list_valid = '../../data/pkl/'+p+'_image_list_test.txt'\n",
    "            with open(image_list_valid,'r') as f:\n",
    "                for line in f.readlines():\n",
    "                    im_valid_test.append(line.strip('\\n'))\n",
    "            im_temp = preprocess_image_batch(im_valid_test,img_size=(256,256), crop_size=(224,224), color_mode=\"rgb\")\n",
    "            out = model.predict(im_temp,batch_size=64)\n",
    "\n",
    "            true_valid_wids = []\n",
    "            for i in im_valid_test:\n",
    "                    temp1 = i.split('/')[4]\n",
    "                    temp = temp1.split('.')[0].split('_')[2]\n",
    "                    true_valid_wids.append(truth[int(temp)][1])\n",
    "\n",
    "            predicted_valid_wids = []\n",
    "            for i in range(len(im_valid_test)):\n",
    "                #print im_list[i], pprint_output(out[i]), true_wids[i]\n",
    "                predicted_valid_wids.append(pprint_output(out[i]))\n",
    "\n",
    "            count, error  = top5accuracy(true_valid_wids, predicted_valid_wids)\n",
    "\n",
    "            print(str(p)+' '+str(count)+' '+str(len(im_valid_test))+' '+str(error)+' '+str(1-error))\n",
    "              \n",
    "            if flag == 0:\n",
    "                dprime = error\n",
    "                flag = 1\n",
    "            else:\n",
    "                dprime -= error\n",
    "        print('Layer: ',layer,'Label: ', label)\n",
    "        print('No lesion: ',dprime)\n",
    "        layer_info[label][layer]['no'] = dprime\n",
    "        \n",
    "        tf.keras.backend.clear_session()\n",
    "        gc.collect()\n",
    "        del model\n",
    "        #Before lesion\n",
    "        #print('Pre-layer-lesioning')\n",
    "        #print('Label:',label)\n",
    "        #print('Layer:',layer)\n",
    "\n",
    "        pred = clf.predict(X_new)\n",
    "        loc = np.where(pred==label)[0]\n",
    "        loc_new =[]\n",
    "        for i in range(len(loc)):\n",
    "            temp = np.where(pred_kmeans==loc[i])[0]\n",
    "            loc_new.extend(temp)\n",
    "\n",
    "        lambda_mask = np.ones(shape=((13555712,)))\n",
    "        lambda_mask[loc_new] = 0.\n",
    "        print('pre-loc', len(loc_new))\n",
    "        model = VGG16(include_top=True, weights='imagenet', input_tensor=None, input_shape=None, pooling=None, classes=1000,lambda_mask=lambda_mask)\n",
    "\n",
    "        flag = 0\n",
    "        dprime = 0.\n",
    "        for p in classes:\n",
    "            im_valid_test = []\n",
    "            image_list_valid = '../../data/pkl/'+p+'_image_list_test.txt'\n",
    "            with open(image_list_valid,'r') as f:\n",
    "                for line in f.readlines():\n",
    "                    im_valid_test.append(line.strip('\\n'))\n",
    "            im_temp = preprocess_image_batch(im_valid_test,img_size=(256,256), crop_size=(224,224), color_mode=\"rgb\")\n",
    "            out = model.predict(im_temp,batch_size=64)\n",
    "\n",
    "            true_valid_wids = []\n",
    "            for i in im_valid_test:\n",
    "                    temp1 = i.split('/')[4]\n",
    "                    temp = temp1.split('.')[0].split('_')[2]\n",
    "                    true_valid_wids.append(truth[int(temp)][1])\n",
    "\n",
    "            predicted_valid_wids = []\n",
    "            for i in range(len(im_valid_test)):\n",
    "                #print im_list[i], pprint_output(out[i]), true_wids[i]\n",
    "                predicted_valid_wids.append(pprint_output(out[i]))\n",
    "\n",
    "            count, error  = top5accuracy(true_valid_wids, predicted_valid_wids)\n",
    "\n",
    "            print(str(p)+' '+str(count)+' '+str(len(im_valid_test))+' '+str(error)+' '+str(1-error))\n",
    "\n",
    "            if flag == 0:\n",
    "                dprime = error\n",
    "                flag = 1\n",
    "            else:\n",
    "                dprime -= error\n",
    "        print('Cluster Only: ',dprime)\n",
    "        layer_info[label][layer]['pre'] = dprime   \n",
    "        tf.keras.backend.clear_session()\n",
    "        gc.collect()\n",
    "        del model\n",
    "             \n",
    "            \n",
    "        #After Lesion\n",
    "       # print('Post-layer-lesioning')\n",
    "        #print('Label:',label)\n",
    "        #print('Layer:',layer)\n",
    "        pred = clf.predict(X_new)\n",
    "        loc = np.where(pred==label)[0]\n",
    "        loc_new =[]\n",
    "        for i in range(len(loc)):\n",
    "            temp = np.where(pred_kmeans==loc[i])[0]\n",
    "            temp2 = temp[np.asarray(np.where((temp >end) | (temp <=start))[0])]\n",
    "            #print(len(temp), len(temp2))\n",
    "            loc_new.extend(temp2)\n",
    "\n",
    "\n",
    "        lambda_mask = np.ones(shape=((13555712,)))\n",
    "        lambda_mask[loc_new] = 0.\n",
    "        print('post-loc', len(loc_new))\n",
    "        model = VGG16(include_top=True, weights='imagenet', input_tensor=None, input_shape=None, pooling=None, classes=1000,lambda_mask=lambda_mask)\n",
    "\n",
    "        flag = 0\n",
    "        dprime = 0.\n",
    "        for p in classes:\n",
    "            im_valid_test = []\n",
    "            image_list_valid = '../../data/pkl/'+p+'_image_list_test.txt'\n",
    "            with open(image_list_valid,'r') as f:\n",
    "                for line in f.readlines():\n",
    "                    im_valid_test.append(line.strip('\\n'))\n",
    "            im_temp = preprocess_image_batch(im_valid_test,img_size=(256,256), crop_size=(224,224), color_mode=\"rgb\")\n",
    "            out = model.predict(im_temp,batch_size=64)\n",
    "\n",
    "            true_valid_wids = []\n",
    "            for i in im_valid_test:\n",
    "                    temp1 = i.split('/')[4]\n",
    "                    temp = temp1.split('.')[0].split('_')[2]\n",
    "                    true_valid_wids.append(truth[int(temp)][1])\n",
    "\n",
    "            predicted_valid_wids = []\n",
    "            for i in range(len(im_valid_test)):\n",
    "                #print im_list[i], pprint_output(out[i]), true_wids[i]\n",
    "                predicted_valid_wids.append(pprint_output(out[i]))\n",
    "\n",
    "            count, error  = top5accuracy(true_valid_wids, predicted_valid_wids)\n",
    "\n",
    "            print(str(p)+' '+str(count)+' '+str(len(im_valid_test))+' '+str(error)+' '+str(1-error))\n",
    "\n",
    "            if flag == 0:\n",
    "                dprime = error\n",
    "                flag = 1\n",
    "            else:\n",
    "                dprime -= error\n",
    "        print('Cluster - layer: ',dprime)\n",
    "        layer_info[label][layer]['post'] = dprime\n",
    "        \n",
    "        \n",
    "         #Random Lesion\n",
    "       # print('Post-layer-lesioning')\n",
    "        #print('Label:',label)\n",
    "        #print('Layer:',layer)\n",
    "        pred = clf.predict(X_new)\n",
    "        loc = np.where(pred==label)[0]\n",
    "        loc_new =[]\n",
    "        for i in range(len(loc)):\n",
    "            temp = np.where(pred_kmeans==loc[i])[0]\n",
    "            temp2 = temp[np.asarray(np.where((temp >end) | (temp <=start))[0])]\n",
    "            #print(len(temp), len(temp2))\n",
    "            loc_new.extend(temp2)\n",
    "\n",
    "        loc_new2 = np.random.randint(start,end,len(loc_new))\n",
    "        lambda_mask = np.ones(shape=((13555712,)))\n",
    "        lambda_mask[loc_new2] = 0.\n",
    "        print('post-rand-loc', len(loc_new))\n",
    "        model = VGG16(include_top=True, weights='imagenet', input_tensor=None, input_shape=None, pooling=None, classes=1000,lambda_mask=lambda_mask)\n",
    "\n",
    "        flag = 0\n",
    "        dprime = 0.\n",
    "        for p in classes:\n",
    "            im_valid_test = []\n",
    "            image_list_valid = '../../data/pkl/'+p+'_image_list_test.txt'\n",
    "            with open(image_list_valid,'r') as f:\n",
    "                for line in f.readlines():\n",
    "                    im_valid_test.append(line.strip('\\n'))\n",
    "            im_temp = preprocess_image_batch(im_valid_test,img_size=(256,256), crop_size=(224,224), color_mode=\"rgb\")\n",
    "            out = model.predict(im_temp,batch_size=64)\n",
    "\n",
    "            true_valid_wids = []\n",
    "            for i in im_valid_test:\n",
    "                    temp1 = i.split('/')[4]\n",
    "                    temp = temp1.split('.')[0].split('_')[2]\n",
    "                    true_valid_wids.append(truth[int(temp)][1])\n",
    "\n",
    "            predicted_valid_wids = []\n",
    "            for i in range(len(im_valid_test)):\n",
    "                #print im_list[i], pprint_output(out[i]), true_wids[i]\n",
    "                predicted_valid_wids.append(pprint_output(out[i]))\n",
    "\n",
    "            count, error  = top5accuracy(true_valid_wids, predicted_valid_wids)\n",
    "\n",
    "            print(str(p)+' '+str(count)+' '+str(len(im_valid_test))+' '+str(error)+' '+str(1-error))\n",
    "\n",
    "            if flag == 0:\n",
    "                dprime = error\n",
    "                flag = 1\n",
    "            else:\n",
    "                dprime -= error\n",
    "        print('Random: ',dprime)\n",
    "        layer_info[label][layer]['rand'] = dprime\n",
    "        tf.keras.backend.clear_session()\n",
    "        gc.collect()\n",
    "        del model\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.ticker import MaxNLocator\n",
    "X = np.arange(1,16)\n",
    "Y = []\n",
    "Z =[]\n",
    "tmp = 0.\n",
    "for item in X:\n",
    "    print(item,layer_info[ana][item]['pre'],layer_info[ana][item]['post'],layer_info[ana][item]['rand'])\n",
    "    tmp += layer_info[ana][item]['pre']-layer_info[ana][item]['post']\n",
    "    Y.append(layer_info[ana][item]['pre']-layer_info[ana][item]['post'])\n",
    "    if layer_info[ana][item]['rand'] == 0.:\n",
    "        Z.append(0.)\n",
    "    else:\n",
    "        Z.append(layer_info[ana][item]['rand']-layer_info[ana][item]['no'])\n",
    "print(tmp)\n",
    "width = 0.35  # the width of the bars\n",
    "\n",
    "rects1 = plt.bar(X- width/2, Y, width, label='Layer-specific cluster lesions')\n",
    "rects2 = plt.bar(X+width/2, Z, width, label='Random layer lesions')\n",
    "\n",
    "plt.ylabel('Relative change in performance impact')\n",
    "plt.xlabel('Different layers of Alexnet')\n",
    "plt.xticks(X, ('conv_1_1', 'conv_1_2', 'conv_2_1','conv_2_2','conv_3_1','conv_3_2','conv_3_3','conv_4_1','conv_4_2','conv_4_3','conv_5_1','conv_5_2', 'conv_5_3','fc1','fc2'))\n",
    "plt.title('Change in Performance by layer for Animate')\n",
    "plt.legend()\n",
    "plt.gca().xaxis.set_major_locator(MaxNLocator(prune='lower'))\n",
    "plt.savefig('../../results/scree/animate_by_layer.png', format='png')\n",
    "#plt.ylim([0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.arange(1,16)\n",
    "Y = []\n",
    "Z = []\n",
    "tmp = 0.\n",
    "for item in X:\n",
    "    print(item,layer_info[ina][item]['pre'],layer_info[ina][item]['post'],layer_info[ana][item]['rand'])\n",
    "    Y.append((-1*layer_info[ina][item]['pre'])-(-1*layer_info[ina][item]['post']))\n",
    "    if layer_info[ana][item]['rand'] == 0.:\n",
    "        Z.append(0.)\n",
    "    else:\n",
    "        Z.append(layer_info[ana][item]['rand']-layer_info[ana][item]['no'])\n",
    "    tmp += (-1*layer_info[ina][item]['pre'])-(-1*layer_info[ina][item]['post'])\n",
    "print(tmp)\n",
    "width = 0.35  # the width of the bars\n",
    "\n",
    "rects1 = plt.bar(X- width/2, Y, width, label='Layer-specific cluster lesions')\n",
    "rects2 = plt.bar(X+width/2, Z, width, label='Random layer lesions')\n",
    "\n",
    "plt.ylabel('Relative change in performance impact')\n",
    "plt.xticks(X, ('conv_1_1', 'conv_1_2', 'conv_2_1','conv_2_2','conv_3_1','conv_3_2','conv_3_3','conv_4_1','conv_4_2','conv_4_3','conv_5_1','conv_5_2', 'conv_5_3','fc1','fc2'))\n",
    "plt.xlabel('Different layers of Alexnet')\n",
    "plt.title('Change in Performance by layer for Inanimate')\n",
    "plt.legend()\n",
    "plt.gca().xaxhttp://localhost:8888/notebooks/code/experiments/keras_models_step_3.ipynb#is.set_major_locator(MaxNLocator(prune='lower'))\n",
    "plt.savefig('../../results/scree/inaniamte_by_layer.png', format='png')\n",
    "#plt.ylim([0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#D' histogram for animate and inanimate double bars for different clustering techniques\n",
    "\n",
    "labels = ['KMeans','GMM','SMM','Birch','Single','Ward','DBSCAN','HDBSCAN','Genie']\n",
    "animate_means = [64.10,66.67,69.23, 25.64,0,58.97,0,17.59,12.82]\n",
    "inanimate_means = [79.49,79.49,82.05,71.79,23.07,82.07,23.07,23.07,38.46]\n",
    "\n",
    "x = np.arange(len(labels))  # the label locations\n",
    "width = 0.35  # the width of the bars\n",
    "\n",
    "ax = plt.subplot(1,2,1)\n",
    "ax.barh(x, animate_means, label='Animate')\n",
    "# Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "#ax.set_xlabel('Performance Impact')\n",
    "#ax.set_title('Performance by Class')\n",
    "#ax.set_yticks(x)\n",
    "#ax.set_yticklabels(labels,rotation=0)\n",
    "ax.legend()\n",
    "plt.xlim([0,100])\n",
    "ax.invert_xaxis()\n",
    "\n",
    "ax = plt.subplot(1,2,2)\n",
    "ax.barh(x, inanimate_means, label='Inanimate', color='orange')\n",
    "# Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "#ax.set_xlabel('Performance Impact')\n",
    "#ax.set_title('Performance by Class')\n",
    "ax.set_yticks(x)\n",
    "ax.set_yticklabels(labels,rotation=0)\n",
    "ax.legend()\n",
    "plt.xlim([0,100])\n",
    "plt.tight_layout()\n",
    "def autolabel(rects, xpos='center'):\n",
    "    xpos = xpos.lower()  # normalize the case of the parameter\n",
    "    ha = {'center': 'center', 'right': 'left', 'left': 'right'}\n",
    "    offset = {'center': 0.5, 'right': 0.57, 'left': 0.43}  # x_txt = x + w*off\n",
    "\n",
    "    for rect in rects:\n",
    "        height = rect.get_height()\n",
    "        ax.text(rect.get_x() + rect.get_width()*offset[xpos], 1.01*height,\n",
    "                '{}'.format(height), ha=ha[xpos], va='bottom',size=8)\n",
    "        \n",
    "#autolabel(rects1)\n",
    "#autolabel(rects2)\n",
    "plt.savefig('../../results/clustering_results.eps', format='eps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "lims = [\n",
    "    np.min([ax.get_xlim(), ax.get_ylim()]),  # min of both axes\n",
    "    np.max([ax.get_xlim(), ax.get_ylim()]),  # max of both axes\n",
    "]\n",
    "ax.plot(lims, lims, 'k-', alpha=0.75)\n",
    "hb =  ax.hexbin(X[:,0],X[:,1],bins='log') \n",
    "fig.colorbar(hb)\n",
    "plt.xlim([-2.2,8000])\n",
    "plt.ylim([-2.2,8000])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.scatter(X[:,0],X[:,1])\n",
    "lims = [\n",
    "    np.min([ax.get_xlim(), ax.get_ylim()]),  # min of both axes\n",
    "    np.max([ax.get_xlim(), ax.get_ylim()]),  # max of both axes\n",
    "]\n",
    "ax.plot(lims, lims, 'k-', alpha=0.75, zorder=1)\n",
    "plt.xlim([-2.2,8000])\n",
    "plt.ylim([-2.2,8000])\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ana,ina, set(pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "label_loc = np.where(pred==ana)[0]\n",
    "#print len(label_loc)\n",
    "Zx = []\n",
    "Zy = []\n",
    "for i in range(len(label_loc)):\n",
    "    temp = np.where(pred_kmeans==label_loc[i])[0]\n",
    "    for i in temp:\n",
    "        Zx.append(X[i][0])\n",
    "        Zy.append(X[i][1])\n",
    "#print len(np.where(Z!=0)[0])\n",
    "print(len(Zx))\n",
    "sc = plt.scatter(Zx,Zy) \n",
    "plt.xlim([-2.2,300])\n",
    "plt.ylim([-2.2,300])\n",
    "lims = [\n",
    "    np.min([ax.get_xlim(), ax.get_ylim()]),  # min of both axes\n",
    "    np.max([ax.get_xlim(), ax.get_ylim()]),  # max of both axes\n",
    "]\n",
    "\n",
    "# now plot both limits against eachother\n",
    "ax.plot(lims, lims, 'k-', alpha=0.75, zorder=1)\n",
    "plt.xlabel('Animate')\n",
    "plt.ylabel('Inanimate')\n",
    "plt.title('Most selective cluster for animate class')\n",
    "plt.savefig('../../results/scree/'+str(method)+'_results_ana.png', format='png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "label_loc = np.where(pred==ina)[0]\n",
    "print(len(label_loc))\n",
    "Zx = []\n",
    "Zy = []\n",
    "for i in range(len(label_loc)):\n",
    "    temp = np.where(pred_kmeans==label_loc[i])[0]\n",
    "    for i in temp:\n",
    "        Zx.append(X[i][0])\n",
    "        Zy.append(X[i][1])\n",
    "sc = plt.scatter(Zx,Zy) \n",
    "plt.xlim([-2.2,300])\n",
    "plt.ylim([-2.2,300])\n",
    "lims = [\n",
    "    np.min([ax.get_xlim(), ax.get_ylim()]),  # min of both axes\n",
    "    np.max([ax.get_xlim(), ax.get_ylim()]),  # max of both axes\n",
    "]\n",
    "\n",
    "# now plot both limits against eachother\n",
    "ax.plot(lims, lims, 'k-', alpha=0.75, zorder=1)\n",
    "plt.xlabel('Animate')\n",
    "plt.ylabel('Inanimate')\n",
    "plt.title('Most selective cluster for inanimate class')\n",
    "plt.savefig('../../results/scree/'+str(method)+'_results_ina.png', format='png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_loc = np.where(pred==ana)[0]\n",
    "Z = []\n",
    "for i in range(len(label_loc)):\n",
    "    temp = np.where(pred_kmeans==label_loc[i])[0]\n",
    "    for i in temp:\n",
    "        Z.append(i)\n",
    "print(max(Z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_loc = np.where(pred==ana)[0]\n",
    "Z = []\n",
    "for i in range(len(label_loc)):\n",
    "    temp = np.where(pred_kmeans==label_loc[i])[0]\n",
    "    for i in temp:# Create a ClassificationModel\n",
    "        Z.append(i)\n",
    "X = np.arange(15)\n",
    "Y = np.zeros((15,))\n",
    "\n",
    "for i in Z:\n",
    "    if i in range(0,3211264): \n",
    "        Y[0] += 1\n",
    "    elif i in range(3211264,6422528):\n",
    "        Y[1] += 1\n",
    "    elif i in range(6422528,8028160):\n",
    "        Y[2] += 1\n",
    "    elif i in range(8028160,9633792):\n",
    "        Y[3] += 1\n",
    "    elif i in range(9633792,10436608):\n",
    "        Y[4] += 1\n",
    "    elif i in range(10436608,11239424):\n",
    "        Y[5] += 1\n",
    "    elif i in range(11239424,12042240):\n",
    "        Y[6] += 1\n",
    "    elif i in range(12042240,12443648):\n",
    "        Y[7] += 1\n",
    "    elif i in range(12443648,12845056):\n",
    "        Y[8] += 1\n",
    "    elif i in range(12845056,13246464):\n",
    "        Y[9] += 1\n",
    "    elif i in range(13246464,13346816):\n",
    "        Y[10] += 1\n",
    "    elif i in range(13346816,13447168):\n",
    "        Y[11] += 1\n",
    "    elif i in range(13447168,13547520):\n",
    "        Y[12] += 1\n",
    "    elif i in range(13547520,13551616):\n",
    "        Y[13] += 1\n",
    "    elif i in range(13551616,13555712):\n",
    "        Y[14] += 1\n",
    "    else:\n",
    "        print(i)\n",
    "\n",
    "Y[0] = float(Y[0]) /3211264\n",
    "Y[1] = float(Y[0]) / 3211264\n",
    "Y[2] = float(Y[2]) / 1605632\n",
    "Y[3] = float(Y[3]) / 1605632\n",
    "Y[4] = float(Y[4]) / 802816\n",
    "Y[5] = float(Y[5]) /802816\n",
    "Y[6] = float(Y[6]) /802816\n",
    "Y[7] = float(Y[7]) /401408\n",
    "Y[8] = float(Y[8]) /401408\n",
    "Y[9] = float(Y[9]) /401408\n",
    "Y[10] = float(Y[10]) /100352\n",
    "Y[11] = float(Y[11]) /100352\n",
    "Y[12] = float(Y[12]) /100352\n",
    "Y[13] = float(Y[13]) /4096\n",
    "Y[14] = float(Y[14]) /4096\n",
    "\n",
    "plt.ylim([0,1.])\n",
    "rect = plt.bar(X,Y)\n",
    "plt.xticks(X, ('conv_1_1', 'conv_1_2', 'conv_2_1','conv_2_2','conv_3_1','conv_3_2','conv_3_3','conv_4_1','conv_4_2','conv_4_3','conv_5_1','conv_5_2', 'conv_5_3','fc1','fc2'))\n",
    "plt.ylabel('Relative count of neurons')\n",
    "plt.title('Neurons from the animate cluster')\n",
    "plt.gca().xaxis.set_major_locator(MaxNLocator(prune='lower'))\n",
    "#autolabel(rect)\n",
    "plt.savefig('../../results/scree/'+str(method)+'_results_ana_hist_alt.png', format='png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_loc = np.where(pred==ina)[0]\n",
    "Z = []# Create a ClassificationModel\n",
    "for i in range(len(label_loc)):\n",
    "    temp = np.where(pred_kmeans==label_loc[i])[0]\n",
    "    for i in temp:\n",
    "        Z.append(i)\n",
    "X = np.arange(15)\n",
    "Y = np.zeros((15,))\n",
    "\n",
    "for i in Z:\n",
    "    if i in range(0,3211264): \n",
    "        Y[0] += 1\n",
    "    elif i in range(3211264,6422528):\n",
    "        Y[1] += 1\n",
    "    elif i in range(6422528,8028160):\n",
    "        Y[2] += 1\n",
    "    elif i in range(8028160,9633792):\n",
    "        Y[3] += 1\n",
    "    elif i in range(9633792,10436608):\n",
    "        Y[4] += 1\n",
    "    elif i in range(10436608,11239424):\n",
    "        Y[5] += 1\n",
    "    elif i in range(11239424,12042240):\n",
    "        Y[6] += 1\n",
    "    elif i in range(12042240,12443648):\n",
    "        Y[7] += 1\n",
    "    elif i in range(12443648,12845056):\n",
    "        Y[8] += 1\n",
    "    elif i in range(12845056,13246464):\n",
    "        Y[9] += 1\n",
    "    elif i in range(13246464,13346816):\n",
    "        Y[10] += 1\n",
    "    elif i in range(13346816,13447168):\n",
    "        Y[11] += 1\n",
    "    elif i in range(13447168,13547520):\n",
    "        Y[12] += 1\n",
    "    elif i in range(13547520,13551616):\n",
    "        Y[13] += 1\n",
    "    elif i in range(13551616,13555712):\n",
    "        Y[14] += 1\n",
    "    else:\n",
    "        print(i)\n",
    "\n",
    "Y[0] = float(Y[0]) /3211264\n",
    "Y[1] = float(Y[0]) / 3211264\n",
    "Y[2] = float(Y[2]) / 1605632\n",
    "Y[3] = float(Y[3]) / 1605632\n",
    "Y[4] = float(Y[4]) / 802816\n",
    "Y[5] = float(Y[5]) /802816\n",
    "Y[6] = float(Y[6]) /802816\n",
    "Y[7] = float(Y[7]) /401408\n",
    "Y[8] = float(Y[8]) /401408\n",
    "Y[9] = float(Y[9]) /401408\n",
    "Y[10] = float(Y[10]) /100352\n",
    "Y[11] = float(Y[11]) /100352\n",
    "Y[12] = float(Y[12]) /100352\n",
    "Y[13] = float(Y[13]) /4096\n",
    "Y[14] = float(Y[14]) /4096\n",
    "\n",
    "plt.ylim([0,1.])\n",
    "rect = plt.bar(X,Y)\n",
    "plt.ylabel('Relative count of neurons')\n",
    "plt.title('Neurons from the inanimate cluster')\n",
    "plt.xticks(X, ('conv_1_1', 'conv_1_2', 'conv_2_1','conv_2_2','conv_3_1','conv_3_2','conv_3_3','conv_4_1','conv_4_2','conv_4_3','conv_5_1','conv_5_2', 'conv_5_3','fc1','fc2'))\n",
    "plt.gca().xaxis.set_major_locator(MaxNLocator(prune='lower'))\n",
    "#autolabel(rect)\n",
    "plt.savefig('../../results/scree/'+str(method)+'_results_ina_hist_alt.png', format='png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_loc = np.where(pred==ina)[0]\n",
    "Z = []\n",
    "for i in range(len(label_loc)):\n",
    "    temp = np.where(pred_kmeans==label_loc[i])[0]\n",
    "    for i in temp:\n",
    "        Z.append(i)\n",
    "X = np.arange(7)\n",
    "Y = np.zeros((7,))\n",
    "\n",
    "print(len(Z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
